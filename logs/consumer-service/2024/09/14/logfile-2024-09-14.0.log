2024-09-14 23:41:34 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - Starting KafkaListenerIT using Java 17.0.8 with PID 424853 (started by anduser in /home/anduser/dev/assessment/assessment-m2/consumer)
2024-09-14 23:41:34 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-14 23:41:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-14 23:41:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 73 ms. Found 2 MongoDB repository interfaces.
2024-09-14 23:41:36 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5cdf221a, com.mongodb.Jep395RecordCodecProvider@7e27f603, com.mongodb.KotlinCodecProvider@6136e1fc]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33595], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-14 23:41:36 [cluster-ClusterId{value='66e5e6f0a9e2f33f5d8f42ea', description='null'}-localhost:33595] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33595, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=46473134, setName='docker-rs', canonicalAddress=53806a46acf9:27017, hosts=[53806a46acf9:27017], passives=[], arbiters=[], primary='53806a46acf9:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e5e6d9f1bc197c67c37fcc, counter=6}, lastWriteDate=Sat Sep 14 23:41:35 GET 2024, lastUpdateTimeNanos=32623635559185}
2024-09-14 23:41:48 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33596]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:41:48 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726342909363
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Subscribed to topic(s): DeleteCustomerEventTopic
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33596]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726342909417
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Subscribed to topic(s): UpdateCustomerAddressEventTopic
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33596]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726342909447
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Subscribed to topic(s): CreateCustomerEventTopic
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33596]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726342909468
2024-09-14 23:41:49 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Subscribed to topic(s): UpdateCustomerNameEventTopic
2024-09-14 23:41:49 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - Started KafkaListenerIT in 14.93 seconds (process running for 42.379)
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {CreateCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {UpdateCustomerNameEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Cluster ID: M6M11FzxTjmiudyv3ya9qA
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Cluster ID: M6M11FzxTjmiudyv3ya9qA
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Cluster ID: M6M11FzxTjmiudyv3ya9qA
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Cluster ID: M6M11FzxTjmiudyv3ya9qA
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Discovered group coordinator localhost:33596 (id: 2147483646 rack: null)
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Discovered group coordinator localhost:33596 (id: 2147483646 rack: null)
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Discovered group coordinator localhost:33596 (id: 2147483646 rack: null)
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Discovered group coordinator localhost:33596 (id: 2147483646 rack: null)
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-4-7c874ba2-a9b8-454d-ac07-5339ff876b0a
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-1-ad76f869-521a-4994-bec1-f9a2e054cfd9
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-3-d866c3dc-7d0d-464f-8f03-e4ef81d1cc89
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-2-cd3d2324-5939-465d-b577-a32e038a1440
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-1-ad76f869-521a-4994-bec1-f9a2e054cfd9', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-3-d866c3dc-7d0d-464f-8f03-e4ef81d1cc89', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-2-cd3d2324-5939-465d-b577-a32e038a1440', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-4-7c874ba2-a9b8-454d-ac07-5339ff876b0a', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error while fetching metadata with correlation id 8 : {UpdateCustomerNameEventTopic=LEADER_NOT_AVAILABLE, CreateCustomerEventTopic=LEADER_NOT_AVAILABLE, UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE, DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Finished assignment for group at generation 1: {consumer-testConsumer-3-d866c3dc-7d0d-464f-8f03-e4ef81d1cc89=Assignment(partitions=[]), consumer-testConsumer-1-ad76f869-521a-4994-bec1-f9a2e054cfd9=Assignment(partitions=[]), consumer-testConsumer-2-cd3d2324-5939-465d-b577-a32e038a1440=Assignment(partitions=[]), consumer-testConsumer-4-7c874ba2-a9b8-454d-ac07-5339ff876b0a=Assignment(partitions=[])}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-3-d866c3dc-7d0d-464f-8f03-e4ef81d1cc89', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-1-ad76f869-521a-4994-bec1-f9a2e054cfd9', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-2-cd3d2324-5939-465d-b577-a32e038a1440', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-4-7c874ba2-a9b8-454d-ac07-5339ff876b0a', protocol='range'}
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:41:49 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {CreateCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: cached metadata has changed from (version3: {UpdateCustomerNameEventTopic=[], CreateCustomerEventTopic=[], UpdateCustomerAddressEventTopic=[], DeleteCustomerEventTopic=[]}) at the beginning of the rebalance to (version4: {UpdateCustomerNameEventTopic=[NO_RACKS], CreateCustomerEventTopic=[NO_RACKS], UpdateCustomerAddressEventTopic=[NO_RACKS], DeleteCustomerEventTopic=[NO_RACKS]})
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:41:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:50 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33596]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-14 23:41:50 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:41:50 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33598]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:50 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=consumer-service-producer-1] Instantiated an idempotent producer.
2024-09-14 23:41:50 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-14 23:41:50 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:41:50 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:41:50 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726342910709
2024-09-14 23:41:50 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=consumer-service-producer-1] Cluster ID: M6M11FzxTjmiudyv3ya9qA
2024-09-14 23:41:50 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=consumer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-14 23:41:51 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CreateCustomerEventTopic
2024-09-14 23:41:51 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CreateCustomerEventTopic and value {"id": "65c99567-ac3a-41c0-94be-d69727eef5f9", "name": "Alex", "address": "New York", "timestamp": "2024-09-14T19:41:50.646598006Z", "eventType": "CreateCustomerViewEvent"}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-3-d866c3dc-7d0d-464f-8f03-e4ef81d1cc89', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-2-cd3d2324-5939-465d-b577-a32e038a1440', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-4-7c874ba2-a9b8-454d-ac07-5339ff876b0a', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-1-ad76f869-521a-4994-bec1-f9a2e054cfd9', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Finished assignment for group at generation 2: {consumer-testConsumer-3-d866c3dc-7d0d-464f-8f03-e4ef81d1cc89=Assignment(partitions=[CreateCustomerEventTopic-0]), consumer-testConsumer-1-ad76f869-521a-4994-bec1-f9a2e054cfd9=Assignment(partitions=[DeleteCustomerEventTopic-0]), consumer-testConsumer-2-cd3d2324-5939-465d-b577-a32e038a1440=Assignment(partitions=[UpdateCustomerAddressEventTopic-0]), consumer-testConsumer-4-7c874ba2-a9b8-454d-ac07-5339ff876b0a=Assignment(partitions=[UpdateCustomerNameEventTopic-0])}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-3-d866c3dc-7d0d-464f-8f03-e4ef81d1cc89', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-1-ad76f869-521a-4994-bec1-f9a2e054cfd9', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-4-7c874ba2-a9b8-454d-ac07-5339ff876b0a', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[DeleteCustomerEventTopic-0])
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-2-cd3d2324-5939-465d-b577-a32e038a1440', protocol='range'}
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[UpdateCustomerNameEventTopic-0])
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[CreateCustomerEventTopic-0])
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[UpdateCustomerAddressEventTopic-0])
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Adding newly assigned partitions: UpdateCustomerAddressEventTopic-0
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Adding newly assigned partitions: UpdateCustomerNameEventTopic-0
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Adding newly assigned partitions: DeleteCustomerEventTopic-0
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Adding newly assigned partitions: CreateCustomerEventTopic-0
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Found no committed offset for partition UpdateCustomerAddressEventTopic-0
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Found no committed offset for partition UpdateCustomerNameEventTopic-0
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Found no committed offset for partition DeleteCustomerEventTopic-0
2024-09-14 23:41:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Found no committed offset for partition CreateCustomerEventTopic-0
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting offset for partition CreateCustomerEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33596 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting offset for partition DeleteCustomerEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33596 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting offset for partition UpdateCustomerNameEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33596 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting offset for partition UpdateCustomerAddressEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33596 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [CreateCustomerEventTopic-0]
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [UpdateCustomerAddressEventTopic-0]
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [UpdateCustomerNameEventTopic-0]
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [DeleteCustomerEventTopic-0]
2024-09-14 23:41:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.listener.DefaultErrorHandler - Backoff FixedBackOff{interval=0, currentAttempts=1, maxAttempts=0} exhausted for CreateCustomerEventTopic-0@0
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener failed
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2873)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.checkDeser(KafkaMessageListenerContainer.java:2921)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2773)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.support.serializer.DeserializationException: failed to deserialize
	at org.springframework.kafka.support.serializer.SerializationUtils.deserializationException(SerializationUtils.java:158)
	at org.springframework.kafka.support.serializer.ErrorHandlingDeserializer.deserialize(ErrorHandlingDeserializer.java:218)
	at org.apache.kafka.common.serialization.Deserializer.deserialize(Deserializer.java:73)
	at org.apache.kafka.clients.consumer.internals.CompletedFetch.parseRecord(CompletedFetch.java:321)
	at org.apache.kafka.clients.consumer.internals.CompletedFetch.fetchRecords(CompletedFetch.java:283)
	at org.apache.kafka.clients.consumer.internals.FetchCollector.fetchRecords(FetchCollector.java:168)
	at org.apache.kafka.clients.consumer.internals.FetchCollector.collectFetch(FetchCollector.java:134)
	at org.apache.kafka.clients.consumer.internals.Fetcher.collectFetch(Fetcher.java:145)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:693)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:617)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:590)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1625)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1600)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1405)
	... 4 common frames omitted
Caused by: org.apache.kafka.common.errors.SerializationException: Error retrieving Avro value schema for id 1
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:410)
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer.deserialize(AbstractKafkaAvroDeserializer.java:191)
	at io.confluent.kafka.serializers.KafkaAvroDeserializer.deserialize(KafkaAvroDeserializer.java:107)
	at org.springframework.kafka.support.serializer.ErrorHandlingDeserializer.deserialize(ErrorHandlingDeserializer.java:215)
	... 17 common frames omitted
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)
	at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:281)
	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)
	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1309)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1242)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1128)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1057)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1665)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1589)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:529)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:317)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:413)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:952)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:936)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:916)
	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaByIdFromRegistry(CachedSchemaRegistryClient.java:333)
	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaBySubjectAndId(CachedSchemaRegistryClient.java:464)
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:401)
	... 20 common frames omitted
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33596]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.kafka.serializers.KafkaAvroDeserializer

2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33598]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = true
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33598]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = true
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, use.latest.version, spring.deserializer.value.delegate.class, specific.avro.reader, spring.deserializer.key.delegate.class]' were supplied but are not used yet.
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726342915423
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-5, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-5, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:41:55 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-5 unregistered
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:55 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node 1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 21 due to node 1 being disconnected (elapsed time since creation: 410ms, elapsed time since send: 410ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 21 due to node 1 being disconnected (elapsed time since creation: 412ms, elapsed time since send: 412ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 21 due to node 1 being disconnected (elapsed time since creation: 411ms, elapsed time since send: 411ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 23 due to node 1 being disconnected (elapsed time since creation: 371ms, elapsed time since send: 371ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Cancelled in-flight HEARTBEAT request with correlation id 22 due to node 2147483646 being disconnected (elapsed time since creation: 11ms, elapsed time since send: 11ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error sending fetch request (sessionId=1124513063, epoch=6) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:41:55 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node -1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error sending fetch request (sessionId=1732297590, epoch=5) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error sending fetch request (sessionId=736712938, epoch=5) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error sending fetch request (sessionId=1699433122, epoch=5) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Group coordinator localhost:33596 (id: 2147483646 rack: null) is unavailable or invalid due to cause: null. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:41:55 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Cancelled in-flight HEARTBEAT request with correlation id 22 due to node 2147483646 being disconnected (elapsed time since creation: 17ms, elapsed time since send: 17ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Group coordinator localhost:33596 (id: 2147483646 rack: null) is unavailable or invalid due to cause: null. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Cancelled in-flight HEARTBEAT request with correlation id 22 due to node 2147483646 being disconnected (elapsed time since creation: 18ms, elapsed time since send: 18ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Group coordinator localhost:33596 (id: 2147483646 rack: null) is unavailable or invalid due to cause: null. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Cancelled in-flight HEARTBEAT request with correlation id 24 due to node 2147483646 being disconnected (elapsed time since creation: 19ms, elapsed time since send: 19ms, request timeout: 30000ms)
2024-09-14 23:41:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Group coordinator localhost:33596 (id: 2147483646 rack: null) is unavailable or invalid due to cause: null. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node 1 disconnected.
2024-09-14 23:41:56 [kafka-producer-network-thread | consumer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node 1 disconnected.
2024-09-14 23:41:56 [kafka-producer-network-thread | consumer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [cluster-ClusterId{value='66e5e6f0a9e2f33f5d8f42ea', description='null'}-localhost:33595] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33595
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-14 23:41:56 [cluster-ClusterId{value='66e5e6f0a9e2f33f5d8f42ea', description='null'}-localhost:33595] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33595
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-14 23:41:56 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node 1 disconnected.
2024-09-14 23:41:56 [kafka-producer-network-thread | consumer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Revoke previously assigned partitions DeleteCustomerEventTopic-0
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Revoke previously assigned partitions UpdateCustomerNameEventTopic-0
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Revoke previously assigned partitions CreateCustomerEventTopic-0
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Revoke previously assigned partitions UpdateCustomerAddressEventTopic-0
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [DeleteCustomerEventTopic-0]
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [UpdateCustomerNameEventTopic-0]
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [CreateCustomerEventTopic-0]
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [UpdateCustomerAddressEventTopic-0]
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33596) could not be established. Node may not be available.
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-1 unregistered
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-4 unregistered
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-2 unregistered
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-3 unregistered
2024-09-14 23:41:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:41:56 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=consumer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-14 23:41:56 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:41:56 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:41:56 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:41:56 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:41:56 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for consumer-service-producer-1 unregistered
2024-09-14 23:44:02 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-14 23:44:02 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-14 23:44:02 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.consumer.integration.listener.KafkaListenerIT]: KafkaListenerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-14 23:44:02 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.consumer.ConsumerApplication for test class com.zhigalko.consumer.integration.listener.KafkaListenerIT
2024-09-14 23:44:03 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-14 23:44:03 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-14 23:44:03 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-14 23:44:03 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-14 23:44:03 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-14 23:44:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 7aefe892c7bf88dc6315b996270d0115bbfae4411d6571d2233692229e6779f4
2024-09-14 23:44:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.381311389S
2024-09-14 23:44:04 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-14 23:44:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-14 23:44:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-14 23:44:05 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-14 23:44:05 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 45f136189bb409bdf12da60dc3edb7238f0ad4cd4bfeca77908466d7ab7ae879
2024-09-14 23:44:06 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.261159144S
2024-09-14 23:44:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-14 23:44:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: a63719374cd5cd6957d404077dbf6f1b09d01bde07890fee926c1f2d7b2c369b
2024-09-14 23:44:17 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-14 23:44:17 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-14 23:44:17 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.consumer.integration.listener.KafkaListenerIT]: KafkaListenerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-14 23:44:17 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.consumer.ConsumerApplication for test class com.zhigalko.consumer.integration.listener.KafkaListenerIT
2024-09-14 23:44:18 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-14 23:44:18 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-14 23:44:18 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-14 23:44:18 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-14 23:44:18 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-14 23:44:19 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 47d4ff6cc9b970a1da39343ca7fff8cee6a1521a9500ab1a687fc735f0d567eb
2024-09-14 23:44:20 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.347190604S
2024-09-14 23:44:20 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-14 23:44:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-14 23:44:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-14 23:44:20 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-14 23:44:20 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 10e517ca00b6d6e1c529b3b0e194cb2a9ede776056c07b3c77ed02b7d1a8a1d7
2024-09-14 23:44:21 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.052802074S
2024-09-14 23:44:22 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-14 23:44:22 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: a32f21e0042eb5d5c77ebbef89a83223c95d690638a294dfb3be7db191a97663
2024-09-14 23:44:29 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT6.900704388S
2024-09-14 23:44:29 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-14 23:44:29 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 1a8dbd9dc0af626e046e268260f0977e48aca671615bd8f008fc871df43b26ba
2024-09-14 23:44:30 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /xenodochial_dewdney: Waiting for 60 seconds for URL: http://localhost:33607/subjects (where port 33607 maps to container port 8081)
2024-09-14 23:44:42 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT12.882436177S
2024-09-14 23:44:42 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - Starting KafkaListenerIT using Java 17.0.8 with PID 427692 (started by anduser in /home/anduser/dev/assessment/assessment-m2/consumer)
2024-09-14 23:44:42 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-14 23:44:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-14 23:44:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 91 ms. Found 2 MongoDB repository interfaces.
2024-09-14 23:44:44 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5cdf221a, com.mongodb.Jep395RecordCodecProvider@7e27f603, com.mongodb.KotlinCodecProvider@6136e1fc]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33604], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-14 23:44:44 [cluster-ClusterId{value='66e5e7ac5ff0ea62cb9ec686', description='null'}-localhost:33604] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33604, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=45335284, setName='docker-rs', canonicalAddress=10e517ca00b6:27017, hosts=[10e517ca00b6:27017], passives=[], arbiters=[], primary='10e517ca00b6:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e5e794bb246267538a76a2, counter=6}, lastWriteDate=Sat Sep 14 23:44:42 GET 2024, lastUpdateTimeNanos=32811448858998}
2024-09-14 23:45:00 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33605]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:45:00 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:45:01 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:01 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:01 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:45:01 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:45:01 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:45:01 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343101767
2024-09-14 23:45:01 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Subscribed to topic(s): UpdateCustomerAddressEventTopic
2024-09-14 23:45:01 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33605]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:45:01 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:45:03 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:03 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:03 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:45:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:45:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:45:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343103738
2024-09-14 23:45:03 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Subscribed to topic(s): DeleteCustomerEventTopic
2024-09-14 23:45:03 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33605]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:45:03 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:45:05 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:05 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:05 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:45:05 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:45:05 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:45:05 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343105595
2024-09-14 23:45:05 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Subscribed to topic(s): CreateCustomerEventTopic
2024-09-14 23:45:05 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33605]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:45:05 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:45:06 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:06 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:06 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:45:06 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:45:06 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:45:06 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343106767
2024-09-14 23:45:06 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Subscribed to topic(s): UpdateCustomerNameEventTopic
2024-09-14 23:45:06 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - Started KafkaListenerIT in 24.317 seconds (process running for 50.711)
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {UpdateCustomerNameEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {CreateCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Cluster ID: CUssC4bHSJuF-fbdQnuR_w
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Cluster ID: CUssC4bHSJuF-fbdQnuR_w
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Cluster ID: CUssC4bHSJuF-fbdQnuR_w
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Cluster ID: CUssC4bHSJuF-fbdQnuR_w
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Discovered group coordinator localhost:33605 (id: 2147483646 rack: null)
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Discovered group coordinator localhost:33605 (id: 2147483646 rack: null)
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Discovered group coordinator localhost:33605 (id: 2147483646 rack: null)
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Discovered group coordinator localhost:33605 (id: 2147483646 rack: null)
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error while fetching metadata with correlation id 8 : {UpdateCustomerNameEventTopic=LEADER_NOT_AVAILABLE, CreateCustomerEventTopic=LEADER_NOT_AVAILABLE, UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE, DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {UpdateCustomerNameEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {CreateCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Finished assignment for group at generation 1: {consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb=Assignment(partitions=[]), consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646=Assignment(partitions=[]), consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db=Assignment(partitions=[]), consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48=Assignment(partitions=[])}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb', protocol='range'}
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: cached metadata has changed from (version3: {UpdateCustomerNameEventTopic=[], CreateCustomerEventTopic=[], UpdateCustomerAddressEventTopic=[], DeleteCustomerEventTopic=[]}) at the beginning of the rebalance to (version4: {UpdateCustomerNameEventTopic=[NO_RACKS], CreateCustomerEventTopic=[NO_RACKS], UpdateCustomerAddressEventTopic=[NO_RACKS], DeleteCustomerEventTopic=[NO_RACKS]})
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:45:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:08 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33605]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-14 23:45:08 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:45:08 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33607]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:08 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=consumer-service-producer-1] Instantiated an idempotent producer.
2024-09-14 23:45:08 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-14 23:45:08 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:45:08 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:45:08 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343108157
2024-09-14 23:45:08 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=consumer-service-producer-1] Cluster ID: CUssC4bHSJuF-fbdQnuR_w
2024-09-14 23:45:08 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=consumer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-14 23:45:08 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CreateCustomerEventTopic
2024-09-14 23:45:08 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CreateCustomerEventTopic and value {"id": "ef04cdeb-2bfa-4017-a513-b9a63a2c6120", "name": "Alex", "address": "New York", "timestamp": "2024-09-14T19:45:08.090748148Z", "eventType": "CreateCustomerViewEvent"}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Finished assignment for group at generation 2: {consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb=Assignment(partitions=[CreateCustomerEventTopic-0]), consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646=Assignment(partitions=[DeleteCustomerEventTopic-0]), consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db=Assignment(partitions=[UpdateCustomerNameEventTopic-0]), consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48=Assignment(partitions=[UpdateCustomerAddressEventTopic-0])}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48', protocol='range'}
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[DeleteCustomerEventTopic-0])
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[CreateCustomerEventTopic-0])
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[UpdateCustomerAddressEventTopic-0])
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[UpdateCustomerNameEventTopic-0])
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Adding newly assigned partitions: UpdateCustomerAddressEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Adding newly assigned partitions: DeleteCustomerEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Adding newly assigned partitions: CreateCustomerEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Adding newly assigned partitions: UpdateCustomerNameEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Found no committed offset for partition DeleteCustomerEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Found no committed offset for partition UpdateCustomerNameEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Found no committed offset for partition UpdateCustomerAddressEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Found no committed offset for partition CreateCustomerEventTopic-0
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting offset for partition UpdateCustomerAddressEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33605 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting offset for partition CreateCustomerEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33605 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting offset for partition DeleteCustomerEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33605 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting offset for partition UpdateCustomerNameEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33605 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [DeleteCustomerEventTopic-0]
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [UpdateCustomerNameEventTopic-0]
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [UpdateCustomerAddressEventTopic-0]
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [CreateCustomerEventTopic-0]
2024-09-14 23:45:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.listener.DefaultErrorHandler - Backoff FixedBackOff{interval=0, currentAttempts=1, maxAttempts=0} exhausted for CreateCustomerEventTopic-0@0
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener failed
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2873)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.checkDeser(KafkaMessageListenerContainer.java:2921)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2773)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.support.serializer.DeserializationException: failed to deserialize
	at org.springframework.kafka.support.serializer.SerializationUtils.deserializationException(SerializationUtils.java:158)
	at org.springframework.kafka.support.serializer.ErrorHandlingDeserializer.deserialize(ErrorHandlingDeserializer.java:218)
	at org.apache.kafka.common.serialization.Deserializer.deserialize(Deserializer.java:73)
	at org.apache.kafka.clients.consumer.internals.CompletedFetch.parseRecord(CompletedFetch.java:321)
	at org.apache.kafka.clients.consumer.internals.CompletedFetch.fetchRecords(CompletedFetch.java:283)
	at org.apache.kafka.clients.consumer.internals.FetchCollector.fetchRecords(FetchCollector.java:168)
	at org.apache.kafka.clients.consumer.internals.FetchCollector.collectFetch(FetchCollector.java:134)
	at org.apache.kafka.clients.consumer.internals.Fetcher.collectFetch(Fetcher.java:145)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:693)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:617)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:590)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1625)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1600)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1405)
	... 4 common frames omitted
Caused by: org.apache.kafka.common.errors.SerializationException: Error retrieving Avro value schema for id 1
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:410)
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer.deserialize(AbstractKafkaAvroDeserializer.java:191)
	at io.confluent.kafka.serializers.KafkaAvroDeserializer.deserialize(KafkaAvroDeserializer.java:107)
	at org.springframework.kafka.support.serializer.ErrorHandlingDeserializer.deserialize(ErrorHandlingDeserializer.java:215)
	... 17 common frames omitted
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)
	at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:281)
	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)
	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1309)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1242)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1128)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1057)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1665)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1589)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:529)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:317)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:413)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:952)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:936)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:916)
	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaByIdFromRegistry(CachedSchemaRegistryClient.java:333)
	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaBySubjectAndId(CachedSchemaRegistryClient.java:464)
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:401)
	... 20 common frames omitted
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Revoke previously assigned partitions UpdateCustomerAddressEventTopic-0
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Revoke previously assigned partitions CreateCustomerEventTopic-0
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Revoke previously assigned partitions DeleteCustomerEventTopic-0
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Revoke previously assigned partitions UpdateCustomerNameEventTopic-0
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [UpdateCustomerAddressEventTopic-0]
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [CreateCustomerEventTopic-0]
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [DeleteCustomerEventTopic-0]
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [UpdateCustomerNameEventTopic-0]
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Member consumer-testConsumer-1-572af8a8-1236-44b2-be7b-fb65784efb48 sending LeaveGroup request to coordinator localhost:33605 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Member consumer-testConsumer-3-abdc8b5e-b8f0-432a-bdab-52bb197bb5bb sending LeaveGroup request to coordinator localhost:33605 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Member consumer-testConsumer-2-0020408c-ec9e-41f4-8e37-ec6ec0243646 sending LeaveGroup request to coordinator localhost:33605 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Member consumer-testConsumer-4-c88031d4-caa6-4bc5-be53-7fd5128e71db sending LeaveGroup request to coordinator localhost:33605 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:12 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:45:17 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-14 23:45:17 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-14 23:45:17 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.consumer.integration.listener.KafkaListenerIT]: KafkaListenerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-14 23:45:18 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.consumer.ConsumerApplication for test class com.zhigalko.consumer.integration.listener.KafkaListenerIT
2024-09-14 23:45:18 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-14 23:45:19 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-14 23:45:19 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-14 23:45:19 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-14 23:45:19 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-14 23:45:20 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 0155da66c0541751c0ae894dedf4c0b8f58312efe7a2928f2de0e49360c8cece
2024-09-14 23:45:20 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.409736418S
2024-09-14 23:45:20 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-14 23:45:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-14 23:45:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-14 23:45:20 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-14 23:45:20 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 16cf50177b6ab826423f5424d1f6de7f711d991369275e2b60c631500fed2c60
2024-09-14 23:45:21 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.273871414S
2024-09-14 23:45:23 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-14 23:45:23 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: f258e6bae127328a42701ed497ed2443bd58b53a397261822fd43637c075ee5e
2024-09-14 23:45:29 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT6.492844099S
2024-09-14 23:45:29 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-14 23:45:29 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 5ec9def9a7cf4f31cdaa2baad4fe268f3c3ed3996b5e483a65edeb3aaccfed9a
2024-09-14 23:45:30 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /loving_vaughan: Waiting for 60 seconds for URL: http://localhost:33612/subjects (where port 33612 maps to container port 8081)
2024-09-14 23:45:40 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT10.851352353S
2024-09-14 23:45:41 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - Starting KafkaListenerIT using Java 17.0.8 with PID 429284 (started by anduser in /home/anduser/dev/assessment/assessment-m2/consumer)
2024-09-14 23:45:41 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-14 23:45:42 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-14 23:45:42 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 83 ms. Found 2 MongoDB repository interfaces.
2024-09-14 23:45:42 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@4522d793, com.mongodb.Jep395RecordCodecProvider@64dfb31d, com.mongodb.KotlinCodecProvider@4438b862]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33609], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-14 23:45:42 [cluster-ClusterId{value='66e5e7e65763510e32a6546c', description='null'}-localhost:33609] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33609, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=38726286, setName='docker-rs', canonicalAddress=16cf50177b6a:27017, hosts=[16cf50177b6a:27017], passives=[], arbiters=[], primary='16cf50177b6a:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e5e7d1513e8f0ec024a1f8, counter=6}, lastWriteDate=Sat Sep 14 23:45:42 GET 2024, lastUpdateTimeNanos=32870053838969}
2024-09-14 23:45:52 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33610]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:45:53 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:45:56 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:56 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:45:56 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:45:56 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:45:56 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:45:56 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343156826
2024-09-14 23:45:56 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Subscribed to topic(s): UpdateCustomerAddressEventTopic
2024-09-14 23:45:56 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33610]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:45:56 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:46:06 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:32 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:32 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:46:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:46:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:46:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343192565
2024-09-14 23:46:32 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Subscribed to topic(s): DeleteCustomerEventTopic
2024-09-14 23:46:32 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33610]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:46:32 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:46:33 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:33 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343193150
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Subscribed to topic(s): CreateCustomerEventTopic
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33610]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:46:33 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:33 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343193769
2024-09-14 23:46:33 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Subscribed to topic(s): UpdateCustomerNameEventTopic
2024-09-14 23:46:33 [main] [34mINFO [0;39m c.z.c.integration.KafkaListenerIT - Started KafkaListenerIT in 53.06 seconds (process running for 77.737)
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {CreateCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {UpdateCustomerNameEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error while fetching metadata with correlation id 2 : {DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Cluster ID: V7Hte218RUiN2C_qzr9D-w
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Cluster ID: V7Hte218RUiN2C_qzr9D-w
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Cluster ID: V7Hte218RUiN2C_qzr9D-w
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Cluster ID: V7Hte218RUiN2C_qzr9D-w
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Discovered group coordinator localhost:33610 (id: 2147483646 rack: null)
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Discovered group coordinator localhost:33610 (id: 2147483646 rack: null)
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Discovered group coordinator localhost:33610 (id: 2147483646 rack: null)
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Discovered group coordinator localhost:33610 (id: 2147483646 rack: null)
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-4-dd32e153-4646-4d6e-8241-59c0d28f4126
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-1-d634599b-a6ee-458d-9ef4-7580f8a9956d
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-3-817e63c1-ae96-4e98-a20e-689d9e532192
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: need to re-join with the given member-id: consumer-testConsumer-2-048c1c41-f0c1-4636-84aa-38b6f63e9674
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-1-d634599b-a6ee-458d-9ef4-7580f8a9956d', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-4-dd32e153-4646-4d6e-8241-59c0d28f4126', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-2-048c1c41-f0c1-4636-84aa-38b6f63e9674', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testConsumer-3-817e63c1-ae96-4e98-a20e-689d9e532192', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error while fetching metadata with correlation id 8 : {UpdateCustomerNameEventTopic=LEADER_NOT_AVAILABLE, CreateCustomerEventTopic=LEADER_NOT_AVAILABLE, UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE, DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Finished assignment for group at generation 1: {consumer-testConsumer-1-d634599b-a6ee-458d-9ef4-7580f8a9956d=Assignment(partitions=[]), consumer-testConsumer-4-dd32e153-4646-4d6e-8241-59c0d28f4126=Assignment(partitions=[]), consumer-testConsumer-2-048c1c41-f0c1-4636-84aa-38b6f63e9674=Assignment(partitions=[]), consumer-testConsumer-3-817e63c1-ae96-4e98-a20e-689d9e532192=Assignment(partitions=[])}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-3-817e63c1-ae96-4e98-a20e-689d9e532192', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-1-d634599b-a6ee-458d-9ef4-7580f8a9956d', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-2-048c1c41-f0c1-4636-84aa-38b6f63e9674', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testConsumer-4-dd32e153-4646-4d6e-8241-59c0d28f4126', protocol='range'}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[])
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Adding newly assigned partitions: 
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: []
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {CreateCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {DeleteCustomerEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error while fetching metadata with correlation id 9 : {UpdateCustomerAddressEventTopic=LEADER_NOT_AVAILABLE}
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: cached metadata has changed from (version3: {UpdateCustomerNameEventTopic=[], CreateCustomerEventTopic=[], UpdateCustomerAddressEventTopic=[], DeleteCustomerEventTopic=[]}) at the beginning of the rebalance to (version4: {UpdateCustomerNameEventTopic=[NO_RACKS], CreateCustomerEventTopic=[NO_RACKS], UpdateCustomerAddressEventTopic=[NO_RACKS], DeleteCustomerEventTopic=[NO_RACKS]})
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:35 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33610]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-14 23:46:35 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:46:35 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33612]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:35 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=consumer-service-producer-1] Instantiated an idempotent producer.
2024-09-14 23:46:35 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-14 23:46:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:46:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:46:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343195176
2024-09-14 23:46:35 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=consumer-service-producer-1] Cluster ID: V7Hte218RUiN2C_qzr9D-w
2024-09-14 23:46:35 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=consumer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-14 23:46:35 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CreateCustomerEventTopic
2024-09-14 23:46:35 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CreateCustomerEventTopic and value {"id": "2fb11a13-990a-42e4-a46f-53d637ff84f2", "name": "Alex", "address": "New York", "timestamp": "2024-09-14T19:46:35.105496595Z", "eventType": "CreateCustomerViewEvent"}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: group is already rebalancing
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Revoke previously assigned partitions 
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: []
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] (Re-)joining group
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-1-d634599b-a6ee-458d-9ef4-7580f8a9956d', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-4-dd32e153-4646-4d6e-8241-59c0d28f4126', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-2-048c1c41-f0c1-4636-84aa-38b6f63e9674', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully joined group with generation Generation{generationId=2, memberId='consumer-testConsumer-3-817e63c1-ae96-4e98-a20e-689d9e532192', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Finished assignment for group at generation 2: {consumer-testConsumer-1-d634599b-a6ee-458d-9ef4-7580f8a9956d=Assignment(partitions=[UpdateCustomerAddressEventTopic-0]), consumer-testConsumer-4-dd32e153-4646-4d6e-8241-59c0d28f4126=Assignment(partitions=[UpdateCustomerNameEventTopic-0]), consumer-testConsumer-2-048c1c41-f0c1-4636-84aa-38b6f63e9674=Assignment(partitions=[DeleteCustomerEventTopic-0]), consumer-testConsumer-3-817e63c1-ae96-4e98-a20e-689d9e532192=Assignment(partitions=[CreateCustomerEventTopic-0])}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-1-d634599b-a6ee-458d-9ef4-7580f8a9956d', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-2-048c1c41-f0c1-4636-84aa-38b6f63e9674', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-3-817e63c1-ae96-4e98-a20e-689d9e532192', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[DeleteCustomerEventTopic-0])
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[CreateCustomerEventTopic-0])
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[UpdateCustomerAddressEventTopic-0])
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Successfully synced group in generation Generation{generationId=2, memberId='consumer-testConsumer-4-dd32e153-4646-4d6e-8241-59c0d28f4126', protocol='range'}
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Notifying assignor about the new Assignment(partitions=[UpdateCustomerNameEventTopic-0])
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Adding newly assigned partitions: UpdateCustomerAddressEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Adding newly assigned partitions: CreateCustomerEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Adding newly assigned partitions: UpdateCustomerNameEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Adding newly assigned partitions: DeleteCustomerEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Found no committed offset for partition UpdateCustomerAddressEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Found no committed offset for partition DeleteCustomerEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Found no committed offset for partition CreateCustomerEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Found no committed offset for partition UpdateCustomerNameEventTopic-0
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting offset for partition UpdateCustomerNameEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33610 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting offset for partition UpdateCustomerAddressEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33610 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting offset for partition CreateCustomerEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33610 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting offset for partition DeleteCustomerEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33610 (id: 1 rack: null)], epoch=0}}.
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [CreateCustomerEventTopic-0]
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [UpdateCustomerAddressEventTopic-0]
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [DeleteCustomerEventTopic-0]
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions assigned: [UpdateCustomerNameEventTopic-0]
2024-09-14 23:46:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.listener.DefaultErrorHandler - Backoff FixedBackOff{interval=0, currentAttempts=1, maxAttempts=0} exhausted for CreateCustomerEventTopic-0@0
org.springframework.kafka.listener.ListenerExecutionFailedException: Listener failed
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2873)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.checkDeser(KafkaMessageListenerContainer.java:2921)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2773)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.support.serializer.DeserializationException: failed to deserialize
	at org.springframework.kafka.support.serializer.SerializationUtils.deserializationException(SerializationUtils.java:158)
	at org.springframework.kafka.support.serializer.ErrorHandlingDeserializer.deserialize(ErrorHandlingDeserializer.java:218)
	at org.apache.kafka.common.serialization.Deserializer.deserialize(Deserializer.java:73)
	at org.apache.kafka.clients.consumer.internals.CompletedFetch.parseRecord(CompletedFetch.java:321)
	at org.apache.kafka.clients.consumer.internals.CompletedFetch.fetchRecords(CompletedFetch.java:283)
	at org.apache.kafka.clients.consumer.internals.FetchCollector.fetchRecords(FetchCollector.java:168)
	at org.apache.kafka.clients.consumer.internals.FetchCollector.collectFetch(FetchCollector.java:134)
	at org.apache.kafka.clients.consumer.internals.Fetcher.collectFetch(Fetcher.java:145)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:693)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:617)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:590)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollConsumer(KafkaMessageListenerContainer.java:1625)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1600)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1405)
	... 4 common frames omitted
Caused by: org.apache.kafka.common.errors.SerializationException: Error retrieving Avro value schema for id 1
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:410)
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer.deserialize(AbstractKafkaAvroDeserializer.java:191)
	at io.confluent.kafka.serializers.KafkaAvroDeserializer.deserialize(KafkaAvroDeserializer.java:107)
	at org.springframework.kafka.support.serializer.ErrorHandlingDeserializer.deserialize(ErrorHandlingDeserializer.java:215)
	... 17 common frames omitted
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)
	at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:281)
	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)
	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1309)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1242)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1128)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1057)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1665)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1589)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:529)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:317)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:413)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:952)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:936)
	at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:916)
	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaByIdFromRegistry(CachedSchemaRegistryClient.java:333)
	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaBySubjectAndId(CachedSchemaRegistryClient.java:464)
	at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:401)
	... 20 common frames omitted
2024-09-14 23:46:39 [awaitility-thread] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33610]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testConsumer-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.kafka.serializers.KafkaAvroDeserializer

2024-09-14 23:46:39 [awaitility-thread] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33612]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = true
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33612]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = true
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, use.latest.version, spring.deserializer.value.delegate.class, specific.avro.reader, spring.deserializer.key.delegate.class]' were supplied but are not used yet.
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726343203815
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-5, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-5, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:46:43 [awaitility-thread] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-5 unregistered
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 24 due to node 1 being disconnected (elapsed time since creation: 449ms, elapsed time since send: 449ms, request timeout: 30000ms)
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 23 due to node 1 being disconnected (elapsed time since creation: 448ms, elapsed time since send: 448ms, request timeout: 30000ms)
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 23 due to node 1 being disconnected (elapsed time since creation: 448ms, elapsed time since send: 448ms, request timeout: 30000ms)
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Cancelled in-flight FETCH request with correlation id 23 due to node 1 being disconnected (elapsed time since creation: 448ms, elapsed time since send: 448ms, request timeout: 30000ms)
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Error sending fetch request (sessionId=1231853420, epoch=6) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:46:44 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node -1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Error sending fetch request (sessionId=974683785, epoch=6) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Error sending fetch request (sessionId=127114263, epoch=6) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Group coordinator localhost:33610 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Group coordinator localhost:33610 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Group coordinator localhost:33610 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node -1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 2147483646 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Error sending fetch request (sessionId=2034254116, epoch=6) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-14 23:46:44 [kafka-coordinator-heartbeat-thread | testConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Group coordinator localhost:33610 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [kafka-producer-network-thread | consumer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [cluster-ClusterId{value='66e5e7e65763510e32a6546c', description='null'}-localhost:33609] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33609
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [cluster-ClusterId{value='66e5e7e65763510e32a6546c', description='null'}-localhost:33609] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33609
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-14 23:46:44 [kafka-producer-network-thread | consumer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Node 1 disconnected.
2024-09-14 23:46:44 [kafka-producer-network-thread | consumer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=consumer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Revoke previously assigned partitions DeleteCustomerEventTopic-0
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Revoke previously assigned partitions UpdateCustomerNameEventTopic-0
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [DeleteCustomerEventTopic-0]
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Revoke previously assigned partitions CreateCustomerEventTopic-0
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Revoke previously assigned partitions UpdateCustomerAddressEventTopic-0
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [UpdateCustomerNameEventTopic-0]
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [CreateCustomerEventTopic-0]
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: partitions revoked: [UpdateCustomerAddressEventTopic-0]
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-1, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-4, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-testConsumer-3, groupId=testConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Node 1 disconnected.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testConsumer-2, groupId=testConsumer] Connection to node 1 (localhost/127.0.0.1:33610) could not be established. Node may not be available.
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-1 unregistered
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-3 unregistered
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-4 unregistered
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-testConsumer-2 unregistered
2024-09-14 23:46:44 [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - testConsumer: Consumer stopped
2024-09-14 23:46:44 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=consumer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-14 23:46:44 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-14 23:46:44 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-14 23:46:44 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-14 23:46:44 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-14 23:46:44 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for consumer-service-producer-1 unregistered
