2024-09-15 11:02:23 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 11:02:23 [main] [34mINFO [0;39m c.z.producer.ProducerApplication - Starting ProducerApplication using Java 17.0.8 with PID 9832 (/home/anduser/dev/assessment/assessment-m2/producer/target/classes started by anduser in /home/anduser/dev/assessment/assessment-m2)
2024-09-15 11:02:23 [main] [34mINFO [0;39m c.z.producer.ProducerApplication - No active profile set, falling back to 1 default profile: "default"
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 87 ms. Found 1 MongoDB repository interface.
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 9001 (http)
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-9001"]
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.a.catalina.core.StandardService - Starting service [Tomcat]
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.28]
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2024-09-15 11:02:24 [main] [34mINFO [0;39m o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1151 ms
2024-09-15 11:02:25 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@7b95bdb0, com.mongodb.Jep395RecordCodecProvider@3f0ce0d1, com.mongodb.KotlinCodecProvider@664db2ca]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 11:02:25 [cluster-ClusterId{value='66e68681fb28046701b0483b', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=16923313}
2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-9001"]
2024-09-15 11:02:26 [main] [34mINFO [0;39m o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port 9001 (http) with context path '/'
2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 11:02:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 11:02:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726383746555
2024-09-15 11:02:26 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 11:02:26 [main] [34mINFO [0;39m c.z.producer.ProducerApplication - Started ProducerApplication in 3.095 seconds (process running for 4.773)
2024-09-15 11:02:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 11:02:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight API_VERSIONS request with correlation id 3 due to node -2 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2024-09-15 11:02:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 11:02:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: FqcP9J1MQ5a2psFQkE3dfA
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-945ca091-acd1-4811-b86c-bde03b7405a7
2024-09-15 11:02:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=3, memberId='consumer-customerViewConsumer-1-945ca091-acd1-4811-b86c-bde03b7405a7', protocol='range'}
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 3: {consumer-customerViewConsumer-1-945ca091-acd1-4811-b86c-bde03b7405a7=Assignment(partitions=[CustomerViewEventTopic-0, CustomerViewEventTopic-1])}
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=3, memberId='consumer-customerViewConsumer-1-945ca091-acd1-4811-b86c-bde03b7405a7', protocol='range'}
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0, CustomerViewEventTopic-1])
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0, CustomerViewEventTopic-1
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition CustomerViewEventTopic-1 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 2 rack: null)], epoch=20}}
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19092 (id: 1 rack: null)], epoch=12}}.
2024-09-15 11:02:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0, CustomerViewEventTopic-1]
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m c.z.p.s.impl.CustomerServiceImpl - Received command: com.zhigalko.producer.command.CreateCustomerCommand@40cb38bf
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m c.z.p.h.i.CreateCustomerCommandHandler - Avro event created with event type - CreateCustomerEvent
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726383765438
2024-09-15 11:02:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: FqcP9J1MQ5a2psFQkE3dfA
2024-09-15 11:02:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 14000 with epoch 0
2024-09-15 11:02:45 [http-nio-9001-exec-2] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CreateCustomerEventTopic
2024-09-15 11:02:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CreateCustomerEventTopic and value {"id": "c0d652f7-0700-4b75-822a-571708ce0c65", "name": "Anna", "address": "New York", "timestamp": "2024-09-15T07:02:45.413791742Z", "eventType": "CreateCustomerEvent"}
2024-09-15 11:02:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 1, leaderEpoch = 20, offset = 1, CreateTime = 1726383766112, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "afedeae6-8557-4ed8-a89b-15b8e77e92ef", "name": "Anna", "address": "New York", "timestamp": "2024-09-15T07:02:46.090669726Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 2, "version": 1})
2024-09-15 11:02:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 2
2024-09-15 11:02:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 2, name - Anna
2024-09-15 11:02:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=2, name=Anna, address=New York]
2024-09-15 11:03:13 [http-nio-9001-exec-3] [34mINFO [0;39m c.z.p.s.impl.CustomerServiceImpl - Received command: com.zhigalko.producer.command.CreateCustomerCommand@37ecbab4
2024-09-15 11:03:13 [http-nio-9001-exec-3] [34mINFO [0;39m c.z.p.h.i.CreateCustomerCommandHandler - Avro event created with event type - CreateCustomerEvent
2024-09-15 11:03:13 [http-nio-9001-exec-3] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CreateCustomerEventTopic
2024-09-15 11:03:13 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CreateCustomerEventTopic and value {"id": "148c145b-185f-4e14-a877-fbf7d188132c", "name": "Anna", "address": "New York", "timestamp": "2024-09-15T07:03:13.294064870Z", "eventType": "CreateCustomerEvent"}
2024-09-15 11:04:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 1, leaderEpoch = 20, offset = 2, CreateTime = 1726383870457, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "5f367f68-72f6-4018-a492-9570c143071d", "name": "Anna", "address": "New York", "timestamp": "2024-09-15T07:04:30.457247519Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 3, "version": 1})
2024-09-15 11:04:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 3
2024-09-15 11:04:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 3, name - Anna
2024-09-15 11:04:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=3, name=Anna, address=New York]
2024-09-15 11:04:55 [cluster-ClusterId{value='66e68681fb28046701b0483b', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 11600 (InterruptedAtShutdown): 'interrupted at shutdown' on server localhost:27017. The full response is {"ok": 0.0, "errmsg": "interrupted at shutdown", "code": 11600, "codeName": "InterruptedAtShutdown"}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:251)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:201)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:431)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 11:04:55 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:6379
2024-09-15 11:04:55 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:6379]: Connection refused: localhost/127.0.0.1:6379
2024-09-15 11:04:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:04:58 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 11:04:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 672 due to node 1 being disconnected (elapsed time since creation: 159ms, elapsed time since send: 159ms, request timeout: 30000ms)
2024-09-15 11:04:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=288) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:04:58 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight API_VERSIONS request with correlation id 677 due to node 1 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=INITIAL) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight API_VERSIONS request with correlation id 680 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=INITIAL) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight API_VERSIONS request with correlation id 683 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
2024-09-15 11:04:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=INITIAL) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:05:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:05:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 11:05:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=INITIAL) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:05:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:05:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 11:05:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=INITIAL) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:05:02 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:05:02 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 11:05:02 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=INITIAL) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=39626649, epoch=INITIAL) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0, CustomerViewEventTopic-1
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0, CustomerViewEventTopic-1]
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Member consumer-customerViewConsumer-1-945ca091-acd1-4811-b86c-bde03b7405a7 sending LeaveGroup request to coordinator localhost:29092 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 11:05:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 11:05:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 11:05:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 11:05:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 11:05:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 11:05:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 11:05:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 11:05:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 11:05:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 11:05:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 11:05:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 11:05:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 11:05:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 12:58:27 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 12:58:27 [main] [34mINFO [0;39m c.z.producer.ProducerApplication - Starting ProducerApplication using Java 17.0.8 with PID 138192 (/home/anduser/dev/assessment/assessment-m2/producer/target/classes started by anduser in /home/anduser/dev/assessment/assessment-m2)
2024-09-15 12:58:27 [main] [34mINFO [0;39m c.z.producer.ProducerApplication - No active profile set, falling back to 1 default profile: "default"
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 119 ms. Found 1 MongoDB repository interface.
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 10 ms. Found 0 Redis repository interfaces.
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 9001 (http)
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-9001"]
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.a.catalina.core.StandardService - Starting service [Tomcat]
2024-09-15 12:58:28 [main] [34mINFO [0;39m o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.28]
2024-09-15 12:58:29 [main] [34mINFO [0;39m o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2024-09-15 12:58:29 [main] [34mINFO [0;39m o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1253 ms
2024-09-15 12:58:29 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@591f6f83, com.mongodb.Jep395RecordCodecProvider@2b44605c, com.mongodb.KotlinCodecProvider@55421b8d]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 12:58:29 [cluster-ClusterId{value='66e6a1b5131a9b6a5f756558', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=14839510}
2024-09-15 12:58:29 [main] [34mINFO [0;39m o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-9001"]
2024-09-15 12:58:29 [main] [34mINFO [0;39m o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port 9001 (http) with context path '/'
2024-09-15 12:58:29 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 12:58:30 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 12:58:30 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 12:58:30 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 12:58:30 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 12:58:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 12:58:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 12:58:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726390710140
2024-09-15 12:58:30 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 12:58:30 [main] [34mINFO [0;39m c.z.producer.ProducerApplication - Started ProducerApplication in 2.755 seconds (process running for 3.749)
2024-09-15 12:58:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 12:58:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight API_VERSIONS request with correlation id 1 due to node -2 being disconnected (elapsed time since creation: 12ms, elapsed time since send: 12ms, request timeout: 30000ms)
2024-09-15 12:58:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 12:58:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 4 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 5 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 6 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 7 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 8 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 9 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 10 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 12 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 13 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 14 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 15 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 16 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 18 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 19 : {CustomerViewEventTopic=INVALID_REPLICATION_FACTOR}
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: FqcP9J1MQ5a2psFQkE3dfA
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException)
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-59d2249a-c986-42c5-9014-3c8e4736dcd8
2024-09-15 12:58:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=5, memberId='consumer-customerViewConsumer-1-59d2249a-c986-42c5-9014-3c8e4736dcd8', protocol='range'}
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 5: {consumer-customerViewConsumer-1-59d2249a-c986-42c5-9014-3c8e4736dcd8=Assignment(partitions=[CustomerViewEventTopic-0, CustomerViewEventTopic-1])}
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=5, memberId='consumer-customerViewConsumer-1-59d2249a-c986-42c5-9014-3c8e4736dcd8', protocol='range'}
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0, CustomerViewEventTopic-1])
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0, CustomerViewEventTopic-1
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition CustomerViewEventTopic-1 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 2 rack: null)], epoch=22}}
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19092 (id: 1 rack: null)], epoch=14}}.
2024-09-15 12:58:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0, CustomerViewEventTopic-1]
2024-09-15 12:58:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Attempt to heartbeat failed since coordinator localhost:29092 (id: 2147483645 rack: null) is either not started or not valid
2024-09-15 12:58:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:29092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 12:58:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:29092 (id: 2147483645 rack: null)
2024-09-15 12:58:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483645
2024-09-15 12:58:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:19092 (id: 2147483646 rack: null)
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.s.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.s.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m c.z.p.s.impl.CustomerServiceImpl - Received command: com.zhigalko.producer.command.CreateCustomerCommand@72ea3c57
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m c.z.p.h.i.CreateCustomerCommandHandler - Avro event created with event type - CreateCustomerEvent
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://127.0.0.1:8081]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 12:58:59 [http-nio-9001-exec-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726390739933
2024-09-15 12:58:59 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: FqcP9J1MQ5a2psFQkE3dfA
2024-09-15 12:59:00 [http-nio-9001-exec-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CreateCustomerEventTopic
2024-09-15 12:59:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 16000 with epoch 0
2024-09-15 12:59:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CreateCustomerEventTopic and value {"id": "70dc79cf-35dd-4e5f-81ed-39e508ca542c", "name": "Anna", "address": "New York", "timestamp": "2024-09-15T08:58:59.912428319Z", "eventType": "CreateCustomerEvent"}
2024-09-15 12:59:03 [http-nio-9001-exec-3] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 12:59:03 [http-nio-9001-exec-3] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Anna, address=New York]
2024-09-15 12:59:28 [http-nio-9001-exec-4] [34mINFO [0;39m c.z.p.s.impl.CustomerServiceImpl - Received command: com.zhigalko.producer.command.UpdateCustomerNameCommand@24613bc
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Disconnecting from node 2 due to request timeout.
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 234 due to node 2 being disconnected (elapsed time since creation: 66832ms, elapsed time since send: 66831ms, request timeout: 30000ms)
2024-09-15 13:00:34 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Disconnecting from node 1 due to request timeout.
2024-09-15 13:00:34 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight METADATA request with correlation id 236 due to node 1 being disconnected (elapsed time since creation: 66734ms, elapsed time since send: 66734ms, request timeout: 30000ms)
2024-09-15 13:00:34 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:19092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 13:00:34 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:19092 (id: 2147483646 rack: null)
2024-09-15 13:00:34 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483646
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=443301757, epoch=82) to node 2:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 13:00:34 [http-nio-9001-exec-4] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic UpdateCustomerNameEventTopic
2024-09-15 13:00:34 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic UpdateCustomerNameEventTopic and value {"id": "8dbf66f6-18d6-4b79-afee-f527f558c100", "name": "Evgeniy", "timestamp": "2024-09-15T08:59:28.206322806Z", "eventType": "UpdateCustomerNameEvent", "aggregateId": 2}
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:19092 (id: 2147483646 rack: null)
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Attempt to heartbeat with Generation{generationId=5, memberId='consumer-customerViewConsumer-1-59d2249a-c986-42c5-9014-3c8e4736dcd8', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Giving away all assigned partitions as lost since generation/memberID has been reset,indicating that consumer is in old state or no longer part of the group
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Lost previously assigned partitions CustomerViewEventTopic-0, CustomerViewEventTopic-1
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions lost: [CustomerViewEventTopic-0, CustomerViewEventTopic-1]
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0, CustomerViewEventTopic-1]
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-59006630-fac5-4578-903e-7803ee2fddac
2024-09-15 13:00:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=7, memberId='consumer-customerViewConsumer-1-59006630-fac5-4578-903e-7803ee2fddac', protocol='range'}
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 7: {consumer-customerViewConsumer-1-59006630-fac5-4578-903e-7803ee2fddac=Assignment(partitions=[CustomerViewEventTopic-0, CustomerViewEventTopic-1])}
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=7, memberId='consumer-customerViewConsumer-1-59006630-fac5-4578-903e-7803ee2fddac', protocol='range'}
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0, CustomerViewEventTopic-1])
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0, CustomerViewEventTopic-1
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition CustomerViewEventTopic-1 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 2 rack: null)], epoch=22}}
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19092 (id: 1 rack: null)], epoch=14}}.
2024-09-15 13:00:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0, CustomerViewEventTopic-1]
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 14, offset = 0, CreateTime = 1726390821545, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "391f8181-53b0-4251-9d8e-68970ea8969c", "name": "Anna", "address": "New York", "timestamp": "2024-09-15T09:00:20.040532287Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 4, "version": 1})
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 4
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 4, name - Anna
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=4, name=Anna, address=New York]
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 14, offset = 1, CreateTime = 1726390823667, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "60c5f540-f5c5-490c-a827-e2186a29ec4d", "name": "Anna", "address": "New York", "timestamp": "2024-09-15T09:00:22.808309799Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 5, "version": 1})
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 5
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 5, name - Anna
2024-09-15 13:01:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=5, name=Anna, address=New York]
2024-09-15 13:07:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 13:08:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 13:08:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -2 disconnected.
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0, CustomerViewEventTopic-1
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0, CustomerViewEventTopic-1]
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Member consumer-customerViewConsumer-1-59006630-fac5-4578-903e-7803ee2fddac sending LeaveGroup request to coordinator localhost:19092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 sent an invalid full fetch response with extraIds=(_NHcEnsJT2W0Lt-SLaYFKQ), response=()
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 13:08:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 13:08:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 13:08:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 13:08:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 13:08:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 13:08:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 13:08:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 17:53:13 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.ProducerApplicationTests]: ProducerApplicationTests does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 17:53:13 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.ProducerApplicationTests
2024-09-15 17:53:14 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 17:53:14 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Starting ProducerApplicationTests using Java 17.0.8 with PID 415083 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 17:53:14 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - No active profile set, falling back to 1 default profile: "default"
2024-09-15 17:53:14 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:53:14 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 17:53:14 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 85 ms. Found 1 MongoDB repository interface.
2024-09-15 17:53:14 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:53:14 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 17:53:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 17:53:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 8 ms. Found 0 Redis repository interfaces.
2024-09-15 17:53:15 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@52aa7742, com.mongodb.Jep395RecordCodecProvider@3d0d120b, com.mongodb.KotlinCodecProvider@6b5c134e]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 17:53:15 [cluster-ClusterId{value='66e6e6cb02971f5ff6887492', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 17:53:16 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 17:53:16 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 17:53:16 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:53:16 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:53:16 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 17:53:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 17:53:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 17:53:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726408396949
2024-09-15 17:53:16 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 17:53:16 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Started ProducerApplicationTests in 2.915 seconds (process running for 3.976)
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 17:53:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 17:53:42 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.ProducerApplicationTests]: ProducerApplicationTests does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 17:53:42 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.ProducerApplicationTests
2024-09-15 17:53:42 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 17:53:42 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Starting ProducerApplicationTests using Java 17.0.8 with PID 417273 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 17:53:42 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - No active profile set, falling back to 1 default profile: "default"
2024-09-15 17:53:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:53:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 17:53:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 75 ms. Found 1 MongoDB repository interface.
2024-09-15 17:53:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:53:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 17:53:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 17:53:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 6 ms. Found 0 Redis repository interfaces.
2024-09-15 17:53:43 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@ddffa6c, com.mongodb.Jep395RecordCodecProvider@42c54bad, com.mongodb.KotlinCodecProvider@6bb4cc0e]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 17:53:43 [cluster-ClusterId{value='66e6e6e7e740da74ba6b2b50', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 17:53:46 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 17:53:46 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 17:53:46 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:53:46 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:53:46 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 17:53:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 17:53:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 17:53:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726408426864
2024-09-15 17:53:46 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 17:53:46 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Started ProducerApplicationTests in 4.473 seconds (process running for 5.439)
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 17:53:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 17:57:02 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.ProducerApplicationTests]: ProducerApplicationTests does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 17:57:02 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.ProducerApplicationTests
2024-09-15 17:57:03 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 17:57:03 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Starting ProducerApplicationTests using Java 17.0.8 with PID 426915 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 17:57:03 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - No active profile set, falling back to 1 default profile: "default"
2024-09-15 17:57:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:57:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 17:57:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 62 ms. Found 1 MongoDB repository interface.
2024-09-15 17:57:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:57:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 17:57:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 17:57:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 17:57:04 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6537ac, com.mongodb.Jep395RecordCodecProvider@3b218c74, com.mongodb.KotlinCodecProvider@ddffa6c]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 17:57:04 [cluster-ClusterId{value='66e6e7b0d1164876a8f1f267', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 17:57:05 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 17:57:05 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 17:57:05 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:57:05 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:57:05 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 17:57:05 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 17:57:05 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 17:57:05 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726408625446
2024-09-15 17:57:05 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 17:57:05 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Started ProducerApplicationTests in 2.455 seconds (process running for 3.269)
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 17:57:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 17:57:38 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.ProducerApplicationTests]: ProducerApplicationTests does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 17:57:38 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.ProducerApplicationTests
2024-09-15 17:57:38 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 17:57:38 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Starting ProducerApplicationTests using Java 17.0.8 with PID 427151 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 17:57:38 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - No active profile set, falling back to 1 default profile: "default"
2024-09-15 17:57:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:57:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 17:57:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 75 ms. Found 1 MongoDB repository interface.
2024-09-15 17:57:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 17:57:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 17:57:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 17:57:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 17:57:39 [cluster-ClusterId{value='66e6e7d3b438a82c25f6fe14', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 17:57:39 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@ddffa6c, com.mongodb.Jep395RecordCodecProvider@42c54bad, com.mongodb.KotlinCodecProvider@6bb4cc0e]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 17:57:40 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 17:57:40 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 17:57:40 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:57:40 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 17:57:41 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 17:57:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 17:57:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 17:57:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726408661063
2024-09-15 17:57:41 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 17:57:41 [main] [34mINFO [0;39m c.z.p.ProducerApplicationTests - Started ProducerApplicationTests in 2.381 seconds (process running for 3.142)
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 17:57:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 18:02:31 [main] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = create-customer-topic, partition = 2, leaderEpoch = null, offset = 1, NoTimestampType = -1, serialized key size = -1, serialized value size = -1, headers = RecordHeaders(headers = [], isReadOnly = false), key = feb997f3-fe55-48ca-9b4f-73880855f897, value = {"id": "e622cf2a-a01e-449d-b403-4e79343860fc", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:02:31.582796671Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:13:12 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 18:13:12 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 18:13:12 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 18:13:13 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 18:13:13 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 18:13:13 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 18:13:13 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 18:13:13 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 18:13:13 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 18:13:13 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 19e8f03b53ae2d9d208b59eadbcb2637fc518dad0a61bd2d3af7b4e5289e8729
2024-09-15 18:13:14 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.811812546S
2024-09-15 18:13:14 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 18:13:14 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 18:13:14 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 18:13:14 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 18:13:14 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: d79b3f6d1b01dee489595d9896b4608d92a6a064286146f2b557e1b4a77c9ba9
2024-09-15 18:13:14 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.759226881S
2024-09-15 18:13:15 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 18:13:15 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 1de4b32bca47e63fc0ee8163ee6beeda4a2e2a8079260511079049846921dcf1
2024-09-15 18:13:18 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.201648211S
2024-09-15 18:13:18 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 18:13:19 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 7b0daeb5edc6fbbc7a287f3437f554630272ec14c7f9b501f8787561b1d921c9
2024-09-15 18:13:19 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /blissful_wilbur: Waiting for 60 seconds for URL: http://localhost:33651/subjects (where port 33651 maps to container port 8081)
2024-09-15 18:13:24 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.548722438S
2024-09-15 18:13:24 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 18:13:24 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 429253 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 18:13:24 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 18:13:25 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:13:25 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 18:13:25 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 66 ms. Found 1 MongoDB repository interface.
2024-09-15 18:13:25 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:13:25 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 18:13:25 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 18:13:25 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 18:13:25 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5366575d, com.mongodb.Jep395RecordCodecProvider@1b6cad77, com.mongodb.KotlinCodecProvider@1fca53a7]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33648], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 18:13:25 [cluster-ClusterId{value='66e6eb85da86ba786b20833a', description='null'}-localhost:33648] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33648, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=27539826, setName='docker-rs', canonicalAddress=d79b3f6d1b01:27017, hosts=[d79b3f6d1b01:27017], passives=[], arbiters=[], primary='d79b3f6d1b01:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e6eb7adb72bebc3f5f1ea9, counter=6}, lastWriteDate=Sun Sep 15 18:13:15 GET 2024, lastUpdateTimeNanos=25775877276907}
2024-09-15 18:13:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33649]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 18:13:26 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:13:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33651]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:13:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33651]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:13:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 18:13:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:13:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:13:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726409606624
2024-09-15 18:13:26 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 18:13:26 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.103 seconds (process running for 14.504)
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: oaOOfBx2RQGKAayUVSsIPQ
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33649 (id: 2147483646 rack: null)
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-de8f3019-3e23-407f-9ef1-c3341f973427
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-de8f3019-3e23-407f-9ef1-c3341f973427', protocol='range'}
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-de8f3019-3e23-407f-9ef1-c3341f973427=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-de8f3019-3e23-407f-9ef1-c3341f973427', protocol='range'}
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:13:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:13:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:13:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33649 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:13:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:13:35 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33649]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 18:13:35 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:13:35 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33651]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:13:35 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 18:13:35 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 18:13:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:13:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:13:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726409615993
2024-09-15 18:13:36 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: oaOOfBx2RQGKAayUVSsIPQ
2024-09-15 18:13:36 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 18:13:36 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 18:13:49 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "1e586eb7-b926-48d5-ad88-811996e36cb0", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:13:27.154806236Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 18:13:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409616116, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "1e586eb7-b926-48d5-ad88-811996e36cb0", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:13:27.154806236Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:13:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:13:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:13:54 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33649 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:55 [cluster-ClusterId{value='66e6eb85da86ba786b20833a', description='null'}-localhost:33648] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33648
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:13:55 [cluster-ClusterId{value='66e6eb85da86ba786b20833a', description='null'}-localhost:33648] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33648
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:13:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:56 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:13:56 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:56 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:13:56 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:57 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:13:57 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:57 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:13:57 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:13:58 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:13:58 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:14:01 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:14:01 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:14:01 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:14:01 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33649) could not be established. Node may not be available.
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 11 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 13 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 44 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 18:14:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 18:14:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 18:14:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:14:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:14:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:14:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:14:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 18:14:06 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 18:14:06 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 18:14:07 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 18:14:07 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 18:14:07 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 18:14:07 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 18:14:07 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 18:14:07 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 18:14:07 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 18:14:08 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: cb04e67880e13486229f406e3173bfdd8a02f499a790122ee713a942e6b82372
2024-09-15 18:14:08 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.841621806S
2024-09-15 18:14:08 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 18:14:08 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 18:14:08 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 18:14:08 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 18:14:08 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 0fe68510cbacdcf6ddd0ff479246a2bb98e858fd8b0524bce77a8b00648c2333
2024-09-15 18:14:09 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.799875522S
2024-09-15 18:14:10 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 18:14:10 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: a21fb8a5c250bdc9d5816814c2ecd8b8e195ee9cb26a1a8f9699ef09be12e093
2024-09-15 18:14:13 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.095825972S
2024-09-15 18:14:13 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 18:14:13 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: c2d50b2eddd7e72baf530d37e7c1c0056162feb5eccf3e1977dab838f0b302cc
2024-09-15 18:14:13 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /charming_saha: Waiting for 60 seconds for URL: http://localhost:33656/subjects (where port 33656 maps to container port 8081)
2024-09-15 18:14:18 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.487725208S
2024-09-15 18:14:18 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 18:14:18 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 430770 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 18:14:18 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 18:14:19 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:14:19 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 18:14:19 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 61 ms. Found 1 MongoDB repository interface.
2024-09-15 18:14:19 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:14:19 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 18:14:19 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 18:14:19 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 18:14:19 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@35cbeb54, com.mongodb.Jep395RecordCodecProvider@a7ae340, com.mongodb.KotlinCodecProvider@5e8bd498]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33653], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 18:14:19 [cluster-ClusterId{value='66e6ebbb66d8b031c4ecc6a6', description='null'}-localhost:33653] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33653, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=18221444, setName='docker-rs', canonicalAddress=0fe68510cbac:27017, hosts=[0fe68510cbac:27017], passives=[], arbiters=[], primary='0fe68510cbac:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e6ebb0652917d3a899c2dc, counter=6}, lastWriteDate=Sun Sep 15 18:14:09 GET 2024, lastUpdateTimeNanos=25829778031030}
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33654]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:14:20 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33656]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:14:20 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33656]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726409660390
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 18:14:20 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 1.779 seconds (process running for 13.976)
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: tpquqmHlTrW73b7eqUVmIQ
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33654 (id: 2147483646 rack: null)
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-4a8c5fb4-2db5-4f4c-8ff2-b9d168ae4d4d
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-4a8c5fb4-2db5-4f4c-8ff2-b9d168ae4d4d', protocol='range'}
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-4a8c5fb4-2db5-4f4c-8ff2-b9d168ae4d4d=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-4a8c5fb4-2db5-4f4c-8ff2-b9d168ae4d4d', protocol='range'}
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33654 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:14:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33654]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:14:20 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33656]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:14:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726409660827
2024-09-15 18:14:20 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: tpquqmHlTrW73b7eqUVmIQ
2024-09-15 18:14:20 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 18:14:21 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 18:14:21 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409660832, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409660832, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409660832, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409660832, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409660832, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409660832, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:24 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409660832, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "2680e0ee-c836-4308-9e77-d0557f095e7d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:14:20.802186898Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:24 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 26 due to node 1 being disconnected (elapsed time since creation: 237ms, elapsed time since send: 237ms, request timeout: 30000ms)
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 18:14:24 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=895097711, epoch=13) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 18:14:24 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33654 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 18:14:24 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:14:24 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33654) could not be established. Node may not be available.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33654) could not be established. Node may not be available.
2024-09-15 18:14:24 [cluster-ClusterId{value='66e6ebbb66d8b031c4ecc6a6', description='null'}-localhost:33653] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33653
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:14:24 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:14:24 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33654) could not be established. Node may not be available.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33654) could not be established. Node may not be available.
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 18:14:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 18:14:25 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 18:14:25 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:14:25 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:14:25 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:14:25 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:14:25 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 18:19:04 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 18:19:04 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 18:19:05 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 18:19:05 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 18:19:06 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 18:19:06 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 18:19:06 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 18:19:06 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 18:19:06 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 18:19:07 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 71ec86f0f4fafa30b56ec90d543120b57a0640ef15c039f3c0e0067f8ba55b4a
2024-09-15 18:19:08 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.859754282S
2024-09-15 18:19:08 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 18:19:08 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 18:19:08 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 18:19:08 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 18:19:08 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 3b55d45b0c07543822b56ea260bc092262dfb774c5409bd712d2db8e6731ed19
2024-09-15 18:19:09 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.166401183S
2024-09-15 18:19:11 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 18:19:11 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 3a893e3505af06427dda7b679c47319603178d0f7e25d6881e65bc9dfe265624
2024-09-15 18:19:11 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.518685349S
2024-09-15 18:19:11 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 18:19:11 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: c63a82edeacdcd64782c5441249a5c39d596a705c52acaf12067e25fdadd7d96
2024-09-15 18:19:18 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT6.783317301S
2024-09-15 18:19:18 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 18:19:18 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 13c5185f74ea727fb7a6b09228d1e7cf5dee97ccc37f5e054c000e338481902c
2024-09-15 18:19:18 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /agitated_brattain: Waiting for 60 seconds for URL: http://localhost:33662/subjects (where port 33662 maps to container port 8081)
2024-09-15 18:19:30 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT11.860064268S
2024-09-15 18:19:30 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 18:19:30 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 433975 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 18:19:30 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 18:19:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:19:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 18:19:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 163 ms. Found 1 MongoDB repository interface.
2024-09-15 18:19:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:19:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 18:19:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 18:19:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 14 ms. Found 0 Redis repository interfaces.
2024-09-15 18:19:33 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@299786b1, com.mongodb.Jep395RecordCodecProvider@75f8d9b0, com.mongodb.KotlinCodecProvider@4f7ae05]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33658], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 18:19:33 [cluster-ClusterId{value='66e6ecf486484e7cf735ad2c', description='null'}-localhost:33658] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33658, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=32848230, setName='docker-rs', canonicalAddress=3b55d45b0c07:27017, hosts=[3b55d45b0c07:27017], passives=[], arbiters=[], primary='3b55d45b0c07:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e6ecddeacdb29b5378da1e, counter=6}, lastWriteDate=Sun Sep 15 18:19:30 GET 2024, lastUpdateTimeNanos=26143224407793}
2024-09-15 18:19:34 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33660]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 18:19:34 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:19:35 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33662]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:19:35 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33662]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:19:35 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 18:19:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:19:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:19:35 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726409975440
2024-09-15 18:19:35 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 18:19:35 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 5.199 seconds (process running for 33.06)
2024-09-15 18:19:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 18:19:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: 6iTDNI87QbmdPVBJWeYQaQ
2024-09-15 18:19:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33660 (id: 2147483646 rack: null)
2024-09-15 18:19:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-fb205928-619f-43f4-96b7-6334f5b76d28
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-fb205928-619f-43f4-96b7-6334f5b76d28', protocol='range'}
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-fb205928-619f-43f4-96b7-6334f5b76d28=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-fb205928-619f-43f4-96b7-6334f5b76d28', protocol='range'}
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33660 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:19:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:19:36 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33660]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 18:19:36 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:19:36 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33662]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:19:36 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 18:19:36 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 18:19:36 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:19:36 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:19:36 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726409976794
2024-09-15 18:19:36 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: 6iTDNI87QbmdPVBJWeYQaQ
2024-09-15 18:19:36 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 18:19:37 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 18:19:37 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "db6342d1-9ef7-4cfc-9aca-57c3ce07ea67", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:19:36.731545208Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 18:19:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726409976806, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "db6342d1-9ef7-4cfc-9aca-57c3ce07ea67", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:19:36.731545208Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:19:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:19:44 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 18:19:44 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:44 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:45 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 18:19:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:19:45 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 18:19:48 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:48 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:48 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:48 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 18:19:48 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:49 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:49 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 18:19:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 2147483646 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33660 (id: 2147483646 rack: null) is unavailable or invalid due to cause: null. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 18:19:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:52 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:52 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:52 [cluster-ClusterId{value='66e6ecf486484e7cf735ad2c', description='null'}-localhost:33658] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33658
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:19:52 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:52 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:53 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:53 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:53 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:53 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:54 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:54 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:54 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:54 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:55 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:55 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:55 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:56 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:56 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:56 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:56 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:57 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:57 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:57 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:57 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:58 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:19:58 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:19:58 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:19:58 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:20:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:20:00 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:20:00 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:20:00 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33660) could not be established. Node may not be available.
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 11 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 13 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 44 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 18:20:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 18:20:00 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 18:20:00 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:20:00 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:20:00 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:20:00 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:20:00 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 18:20:34 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 18:20:34 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 18:20:34 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 18:20:35 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 18:20:35 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 18:20:35 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 18:20:35 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 18:20:35 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 18:20:36 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 18:20:36 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: c02bc122149e5a32ec4deb6c2c266cf3c2dc67a3fe27a23e180dd79640b9ae61
2024-09-15 18:20:37 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.430625048S
2024-09-15 18:20:37 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 18:20:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 18:20:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 18:20:37 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 18:20:37 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 3056d6c050a195af487f7462976834210fe57bb46b777c83f25fa92d2b51e7b2
2024-09-15 18:20:38 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.071399082S
2024-09-15 18:20:40 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 18:20:40 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: f918b0cc13973cd12ce407af90040dd83f09dcb285c939db9838f71ec30c6136
2024-09-15 18:20:40 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.427687391S
2024-09-15 18:20:40 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 18:20:40 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: f14fa75385ae9479a568b1550e0f5af43ec9bc92a8370f46fa1ef00f6c35a48f
2024-09-15 18:20:47 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT6.731465125S
2024-09-15 18:20:47 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 18:20:47 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 28457389464f4b86f6ddb7f0a65584428ff2c628b821aca458002b8f3ff6fd26
2024-09-15 18:20:47 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /happy_faraday: Waiting for 60 seconds for URL: http://localhost:33668/subjects (where port 33668 maps to container port 8081)
2024-09-15 18:20:58 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT10.845409775S
2024-09-15 18:20:58 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 18:20:58 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 435749 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 18:20:58 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 18:20:59 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:20:59 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 18:20:59 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 163 ms. Found 1 MongoDB repository interface.
2024-09-15 18:20:59 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:20:59 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 18:20:59 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 18:20:59 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 17 ms. Found 0 Redis repository interfaces.
2024-09-15 18:21:00 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@4f7ae05, com.mongodb.Jep395RecordCodecProvider@1e23ee0e, com.mongodb.KotlinCodecProvider@b144175]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33664], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 18:21:00 [cluster-ClusterId{value='66e6ed4c68caf55973cff933', description='null'}-localhost:33664] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33664, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=38661311, setName='docker-rs', canonicalAddress=3056d6c050a1:27017, hosts=[3056d6c050a1:27017], passives=[], arbiters=[], primary='3056d6c050a1:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e6ed363a184e71db91326d, counter=6}, lastWriteDate=Sun Sep 15 18:20:59 GET 2024, lastUpdateTimeNanos=26231050434985}
2024-09-15 18:21:02 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33666]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 18:21:02 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:21:02 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33668]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:21:02 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33668]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:21:03 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 18:21:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:21:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:21:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410063015
2024-09-15 18:21:03 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 18:21:03 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 4.907 seconds (process running for 30.284)
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: ZFmzJ9ULTQC_pmSNO2Z1gQ
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33666 (id: 2147483646 rack: null)
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-ce4c7110-7079-4fba-b10b-a42dfc184b86
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-ce4c7110-7079-4fba-b10b-a42dfc184b86', protocol='range'}
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-ce4c7110-7079-4fba-b10b-a42dfc184b86=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-ce4c7110-7079-4fba-b10b-a42dfc184b86', protocol='range'}
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33666 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:21:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:21:04 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33666]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 18:21:04 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:21:04 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33668]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:21:04 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 18:21:04 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 18:21:04 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:21:04 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:21:04 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410064250
2024-09-15 18:21:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: ZFmzJ9ULTQC_pmSNO2Z1gQ
2024-09-15 18:21:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 18:21:04 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 18:21:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "1453bf80-9928-4085-9165-63e906c4e43f", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:21:04.183597558Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 18:21:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410064261, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "1453bf80-9928-4085-9165-63e906c4e43f", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:21:04.183597558Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:21:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:21:12 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 18:21:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33666 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:27 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:27 [cluster-ClusterId{value='66e6ed4c68caf55973cff933', description='null'}-localhost:33664] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33664
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:21:27 [cluster-ClusterId{value='66e6ed4c68caf55973cff933', description='null'}-localhost:33664] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33664
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 18:21:28 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:28 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:28 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:28 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:29 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:29 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:29 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:29 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:30 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:30 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:30 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:31 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:31 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:31 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:31 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:32 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:32 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:32 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:32 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:33 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:33 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:33 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:35 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:21:35 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:21:35 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:35 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33666) could not be established. Node may not be available.
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 11 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 13 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 44 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 18:21:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 18:21:36 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 18:21:36 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:21:36 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:21:36 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:21:36 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:21:36 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 18:22:00 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 18:22:00 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 18:22:01 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 18:22:01 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 18:22:02 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 18:22:02 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 18:22:02 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 18:22:02 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 18:22:02 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 18:22:03 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 363ce7e9e5052c39135bacd5ed5f2a004051623d4d3cd5bad7a5c8c234f459a8
2024-09-15 18:22:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.478129688S
2024-09-15 18:22:04 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 18:22:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 18:22:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 18:22:04 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 18:22:04 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: e93e4fa30b240895d42858dfeb8c3206798eb77a1af66572d525af9ca0aee685
2024-09-15 18:22:05 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.167914675S
2024-09-15 18:22:06 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 18:22:06 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 1a3a76c6b498c271d995a28a0312685eb915660af9d19c659f28dc7a8a9bc46d
2024-09-15 18:22:07 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.453301523S
2024-09-15 18:22:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 18:22:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 9ac492b442fb74894651ffa813e058616ab335990d54bd01e467a7505bc0c3fb
2024-09-15 18:22:13 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT6.573580266S
2024-09-15 18:22:13 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 18:22:13 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: b924e93323599af87a02adc2ab90c01a7b1b7b0517e1eade09256e57df7590fb
2024-09-15 18:22:14 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /sleepy_austin: Waiting for 60 seconds for URL: http://localhost:33674/subjects (where port 33674 maps to container port 8081)
2024-09-15 18:22:24 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT10.826173543S
2024-09-15 18:22:25 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 18:22:25 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 437557 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 18:22:25 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 18:22:26 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:22:26 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 18:22:26 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 164 ms. Found 1 MongoDB repository interface.
2024-09-15 18:22:26 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:22:26 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 18:22:26 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 18:22:26 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 12 ms. Found 0 Redis repository interfaces.
2024-09-15 18:22:27 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@12723c5d, com.mongodb.Jep395RecordCodecProvider@56a09a5c, com.mongodb.KotlinCodecProvider@775edae0]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33670], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 18:22:27 [cluster-ClusterId{value='66e6eda3983ca31eaee12511', description='null'}-localhost:33670] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33670, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=32149230, setName='docker-rs', canonicalAddress=e93e4fa30b24:27017, hosts=[e93e4fa30b24:27017], passives=[], arbiters=[], primary='e93e4fa30b24:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e6ed8cb1e24d18ab3d24fb, counter=6}, lastWriteDate=Sun Sep 15 18:22:26 GET 2024, lastUpdateTimeNanos=26317591235010}
2024-09-15 18:22:29 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33672]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 18:22:29 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:22:29 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33674]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:22:29 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33674]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:22:29 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 18:22:29 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:22:29 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:22:29 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410149571
2024-09-15 18:22:29 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 18:22:29 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 4.812 seconds (process running for 29.848)
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: uazhW8AjRueS1RgZvmOhfA
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33672 (id: 2147483646 rack: null)
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-16e11247-a4a3-46b9-97bf-73cb872c3389
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-16e11247-a4a3-46b9-97bf-73cb872c3389', protocol='range'}
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-16e11247-a4a3-46b9-97bf-73cb872c3389=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-16e11247-a4a3-46b9-97bf-73cb872c3389', protocol='range'}
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33672 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:22:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:22:30 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33672]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 18:22:30 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:22:30 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33674]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:22:30 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 18:22:30 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 18:22:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:22:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:22:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410150682
2024-09-15 18:22:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: uazhW8AjRueS1RgZvmOhfA
2024-09-15 18:22:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 18:22:31 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 18:22:31 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410150695, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410150695, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:22:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410150695, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410150695, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:22:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410150695, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410150695, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:22:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:34 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410150695, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e089f4a0-7ff1-4725-b175-9fcdb88a2b82", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:22:30.616818817Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 43 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:34 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 27 due to node 1 being disconnected (elapsed time since creation: 320ms, elapsed time since send: 319ms, request timeout: 30000ms)
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 18:22:34 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=2059112980, epoch=14) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 18:22:34 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 18:22:34 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33672 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:22:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33672) could not be established. Node may not be available.
2024-09-15 18:22:34 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:22:34 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33672) could not be established. Node may not be available.
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33672) could not be established. Node may not be available.
2024-09-15 18:22:35 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:22:35 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33672) could not be established. Node may not be available.
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33672) could not be established. Node may not be available.
2024-09-15 18:22:35 [cluster-ClusterId{value='66e6eda3983ca31eaee12511', description='null'}-localhost:33670] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33670
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:22:35 [cluster-ClusterId{value='66e6eda3983ca31eaee12511', description='null'}-localhost:33670] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33670
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:22:35 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:22:35 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33672) could not be established. Node may not be available.
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 18:22:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 18:22:35 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 18:22:35 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:22:35 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:22:35 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:22:35 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:22:35 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 18:23:44 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 18:23:44 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 18:23:45 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 18:23:45 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 18:23:46 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 18:23:46 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 18:23:46 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 18:23:46 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 18:23:46 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 18:23:47 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: dafb92ed67eeff293646a5839f6714e3bb4d3db1246150d90fc147a2fbfe06cf
2024-09-15 18:23:48 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.676982367S
2024-09-15 18:23:48 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 18:23:48 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 18:23:48 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 18:23:48 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 18:23:48 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: b972b8633be1839b6e90bc90b961bb7d1dc17d5d06a72815705a185c9c25811e
2024-09-15 18:23:49 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.386549223S
2024-09-15 18:23:51 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 18:23:51 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: fe6623585b2a29e5cc571554d7588da03a04ed59f64f97b990c4b9408cb6db39
2024-09-15 18:23:51 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.584237076S
2024-09-15 18:23:51 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 18:23:51 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 80ed600a1042f659bf0bb61f25a4f6532ed47495a01e9d4f1c2eb9f7a35c2476
2024-09-15 18:23:58 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT6.722462082S
2024-09-15 18:23:58 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 18:23:58 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: daa16f0848559963b7238987c672cd55d4372a5e125af006ddda174cff592210
2024-09-15 18:23:58 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /gallant_fermi: Waiting for 60 seconds for URL: http://localhost:33680/subjects (where port 33680 maps to container port 8081)
2024-09-15 18:24:09 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT10.859416325S
2024-09-15 18:24:09 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 18:24:09 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 439374 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 18:24:09 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 18:24:10 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:24:10 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 18:24:11 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 186 ms. Found 1 MongoDB repository interface.
2024-09-15 18:24:11 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:24:11 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 18:24:11 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 18:24:11 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 12 ms. Found 0 Redis repository interfaces.
2024-09-15 18:24:12 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@1e23ee0e, com.mongodb.Jep395RecordCodecProvider@b144175, com.mongodb.KotlinCodecProvider@38923cfe]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33676], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 18:24:12 [cluster-ClusterId{value='66e6ee0cc68aaf6181072d06', description='null'}-localhost:33676] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33676, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=30784663, setName='docker-rs', canonicalAddress=b972b8633be1:27017, hosts=[b972b8633be1:27017], passives=[], arbiters=[], primary='b972b8633be1:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e6edf492e50c7274af437f, counter=6}, lastWriteDate=Sun Sep 15 18:24:10 GET 2024, lastUpdateTimeNanos=26422369042441}
2024-09-15 18:24:14 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33678]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 18:24:14 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:24:14 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33680]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:24:14 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33680]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:24:14 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 18:24:14 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:24:14 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:24:14 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410254589
2024-09-15 18:24:14 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 18:24:14 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 5.25 seconds (process running for 31.112)
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: GWBryb-CQjG8kS6gKBX5Aw
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33678 (id: 2147483646 rack: null)
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-b9e0b984-dc0a-4d3c-8bd9-33f47f6852e2
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-b9e0b984-dc0a-4d3c-8bd9-33f47f6852e2', protocol='range'}
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-b9e0b984-dc0a-4d3c-8bd9-33f47f6852e2=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-b9e0b984-dc0a-4d3c-8bd9-33f47f6852e2', protocol='range'}
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33678 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:24:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:24:15 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33678]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 18:24:15 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:24:15 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33680]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:24:15 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 18:24:15 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 18:24:15 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:24:15 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:24:15 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410255922
2024-09-15 18:24:15 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: GWBryb-CQjG8kS6gKBX5Aw
2024-09-15 18:24:16 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 18:24:16 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 18:24:16 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "e3b045ee-4365-468c-84a6-3750a9a5f368", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:24:15.850773281Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 18:24:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410255934, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e3b045ee-4365-468c-84a6-3750a9a5f368", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:24:15.850773281Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:24:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:24:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 18:24:55 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 11 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 13 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:6379
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 44 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:6379
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 17 due to node 1 being disconnected (elapsed time since creation: 32ms, elapsed time since send: 32ms, request timeout: 30000ms)
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 18:24:56 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33678 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=317854286, epoch=4) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33678) could not be established. Node may not be available.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33678) could not be established. Node may not be available.
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33678) could not be established. Node may not be available.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33678) could not be established. Node may not be available.
2024-09-15 18:24:56 [cluster-ClusterId{value='66e6ee0cc68aaf6181072d06', description='null'}-localhost:33676] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33676
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 18:24:56 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33678) could not be established. Node may not be available.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 18:24:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33678) could not be established. Node may not be available.
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 18:24:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 18:24:57 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 18:24:57 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 18:24:57 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 18:24:57 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 18:24:57 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 18:24:57 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 18:25:21 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 18:25:21 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 18:25:22 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 18:25:22 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 18:25:22 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 18:25:23 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 18:25:23 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 18:25:23 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 18:25:23 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 18:25:24 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: f03acfda5a9dd6210f6f9e8205ed6e3bb5f3cfb8c6fe4c903c540d8498c98f12
2024-09-15 18:25:24 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT1.381215592S
2024-09-15 18:25:24 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 18:25:24 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 18:25:24 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 18:25:24 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 18:25:24 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: d464fe13fdce88c6d2d3f8a14adc9cc153f64a8720d3236a649887525874651f
2024-09-15 18:25:25 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT1.048563381S
2024-09-15 18:25:27 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 18:25:27 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 6b52455361bbc4b5f96f974a321c5649bad4066d422a3aeeefc31433aff8da9a
2024-09-15 18:25:27 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.432093809S
2024-09-15 18:25:27 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 18:25:27 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: d5f2d2fe177923702628112be7d0fb45146217ced082f69aecb1c93036ae8c9b
2024-09-15 18:25:34 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT6.566651194S
2024-09-15 18:25:34 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 18:25:34 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: aea05576d072107f19e945ef77afcdbd069ad29456b5b31bb4302de412eaf9d8
2024-09-15 18:25:34 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /priceless_mendel: Waiting for 60 seconds for URL: http://localhost:33686/subjects (where port 33686 maps to container port 8081)
2024-09-15 18:25:45 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT10.881742426S
2024-09-15 18:25:45 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 18:26:53 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 441167 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 18:26:53 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 18:26:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:26:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 18:26:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 196 ms. Found 1 MongoDB repository interface.
2024-09-15 18:26:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 18:26:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 18:26:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 18:26:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 18 ms. Found 0 Redis repository interfaces.
2024-09-15 18:26:55 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@14ad42, com.mongodb.Jep395RecordCodecProvider@608b906d, com.mongodb.KotlinCodecProvider@173cfb01]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33682], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 18:26:55 [cluster-ClusterId{value='66e6eeafa76ed934521a4357', description='null'}-localhost:33682] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33682, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=36900759, setName='docker-rs', canonicalAddress=d464fe13fdce:27017, hosts=[d464fe13fdce:27017], passives=[], arbiters=[], primary='d464fe13fdce:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e6ee550e75d4ac8b022863, counter=6}, lastWriteDate=Sun Sep 15 18:26:46 GET 2024, lastUpdateTimeNanos=26585999162912}
2024-09-15 18:26:57 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33684]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 18:26:57 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:26:57 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33686]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:26:57 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33686]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:26:58 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 18:26:58 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:26:58 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:26:58 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410418283
2024-09-15 18:26:58 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 18:26:58 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 73.077 seconds (process running for 97.987)
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: 0srYk16ESPGprBBJ5xbWwQ
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33684 (id: 2147483646 rack: null)
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-59ec3da9-4428-4338-89b6-d256d6368009
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-59ec3da9-4428-4338-89b6-d256d6368009', protocol='range'}
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-59ec3da9-4428-4338-89b6-d256d6368009=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-59ec3da9-4428-4338-89b6-d256d6368009', protocol='range'}
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33684 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:26:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:28:13 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33684 (id: 2147483646 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 18:28:13 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:33684 (id: 2147483646 rack: null)
2024-09-15 18:28:13 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483646
2024-09-15 18:28:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33684 (id: 2147483646 rack: null)
2024-09-15 18:28:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33684 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 18:28:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:33684 (id: 2147483646 rack: null)
2024-09-15 18:28:14 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33684]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 18:28:14 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33684 (id: 2147483646 rack: null)
2024-09-15 18:28:14 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33686]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Attempt to heartbeat with Generation{generationId=1, memberId='consumer-customerViewConsumer-1-59ec3da9-4428-4338-89b6-d256d6368009', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Giving away all assigned partitions as lost since generation/memberID has been reset,indicating that consumer is in old state or no longer part of the group
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Lost previously assigned partitions CustomerViewEventTopic-0
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions lost: [CustomerViewEventTopic-0]
2024-09-15 18:28:14 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-27889537-5395-459c-9bbd-77bf35102042
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=3, memberId='consumer-customerViewConsumer-1-27889537-5395-459c-9bbd-77bf35102042', protocol='range'}
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 3: {consumer-customerViewConsumer-1-27889537-5395-459c-9bbd-77bf35102042=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=3, memberId='consumer-customerViewConsumer-1-27889537-5395-459c-9bbd-77bf35102042', protocol='range'}
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 18:28:14 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 18:28:14 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 18:28:14 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 18:28:14 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726410494134
2024-09-15 18:28:14 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: 0srYk16ESPGprBBJ5xbWwQ
2024-09-15 18:28:14 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 18:28:14 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33684 (id: 1 rack: null)], epoch=0}}.
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 18:28:14 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "f0b11027-0b1c-4481-a355-3d3e2828eeed", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:28:14.062859933Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726410494146, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "f0b11027-0b1c-4481-a355-3d3e2828eeed", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T14:28:14.062859933Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 18:28:14 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 20:45:22 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 20:45:22 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 20:45:23 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 20:45:23 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 20:45:23 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 20:45:23 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 20:45:23 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 20:45:23 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 20:45:23 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 20:45:24 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 40dca02c7e67cef3956eb2d6b15fcf51dc858f6d5618387f726e82b0ff0206d1
2024-09-15 20:45:24 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.855853748S
2024-09-15 20:45:24 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 20:45:24 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 20:45:24 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 20:45:24 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 20:45:24 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: ca2bc43536cef31ee9f6c7e08d3b26ce12352d6e171e821e17f1bb26e3b4fad6
2024-09-15 20:45:25 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.854904244S
2024-09-15 20:45:26 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 20:45:26 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 092cc10819a0f154b1920019b99c4e1ec67fbe03aa904ae9550e270991fc1dd7
2024-09-15 20:45:26 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.257127654S
2024-09-15 20:45:26 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 20:45:26 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 994200112a47704ea608844275ac6697a9597d526fb4f3c66fc549ac47dc69e3
2024-09-15 20:45:29 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT2.978647365S
2024-09-15 20:45:29 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 20:45:29 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: aaa65d3dfcb135f557302ebeb762328a6c7616646cbc36612d2c2944ebf6947f
2024-09-15 20:45:29 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /relaxed_kare: Waiting for 60 seconds for URL: http://localhost:33692/subjects (where port 33692 maps to container port 8081)
2024-09-15 20:45:34 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.392310246S
2024-09-15 20:45:34 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 20:45:34 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 444287 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 20:45:34 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 20:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 20:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 77 ms. Found 1 MongoDB repository interface.
2024-09-15 20:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 20:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 20:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 20:45:36 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@efa04b5, com.mongodb.Jep395RecordCodecProvider@a80a896, com.mongodb.KotlinCodecProvider@13c81bc5]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33688], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 20:45:36 [cluster-ClusterId{value='66e70f301efb3a6d50ac621b', description='null'}-localhost:33688] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33688, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=21929959, setName='docker-rs', canonicalAddress=ca2bc43536ce:27017, hosts=[ca2bc43536ce:27017], passives=[], arbiters=[], primary='ca2bc43536ce:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e70f244adc121d567d7074, counter=6}, lastWriteDate=Sun Sep 15 20:45:25 GET 2024, lastUpdateTimeNanos=27053300633420}
2024-09-15 20:52:03 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 20:52:03 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 20:52:04 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 20:52:04 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 20:52:04 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 20:52:04 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 20:52:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 20:52:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 20:52:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 20:52:05 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 2e8c058c67a5241cf375832d87362a4ef25b883f4ce8dc81c38b2a8ec61c5923
2024-09-15 20:52:05 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.730756824S
2024-09-15 20:52:05 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 20:52:05 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 20:52:05 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 20:52:05 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 20:52:05 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 297a86bec7b19333221880fce688c11ad9ac4820c3bd8ff3dee0bba35850a307
2024-09-15 20:52:06 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.773326843S
2024-09-15 20:52:06 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 20:52:06 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 02667e9fc4a0a54386389f551c421a69ebca8d8b90a59eec08b864e696d42f82
2024-09-15 20:52:07 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.243146031S
2024-09-15 20:52:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 20:52:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 4931241cc426889288bc001360bcac83ae87f5478a3f877c49ae7b48333c8fdc
2024-09-15 20:52:10 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT2.983047692S
2024-09-15 20:52:10 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 20:52:10 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 897379c10fa88a8b054e0b3701bc335868c1cb9a699b4142e248beaf969237ef
2024-09-15 20:52:10 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /beautiful_hertz: Waiting for 60 seconds for URL: http://localhost:33698/subjects (where port 33698 maps to container port 8081)
2024-09-15 20:52:15 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.40954334S
2024-09-15 20:52:15 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 20:52:15 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 446772 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 20:52:15 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 20:52:15 [main] [31mWARN [0;39m o.s.w.c.s.GenericWebApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.support.BeanDefinitionOverrideException: Invalid bean definition with name 'connectionFactory' defined in class path resource [com/zhigalko/producer/config/RedisConfig.class]: Cannot register bean definition [Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=redisConfig; factoryMethodName=connectionFactory; initMethodNames=null; destroyMethodNames=[(inferred)]; defined in class path resource [com/zhigalko/producer/config/RedisConfig.class]] for bean 'connectionFactory' since there is already [Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=com.zhigalko.producer.integration.config.KafkaTestConfig; factoryMethodName=connectionFactory; initMethodNames=null; destroyMethodNames=[(inferred)]; defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]] bound.
2024-09-15 20:52:15 [main] [34mINFO [0;39m o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2024-09-15 20:52:15 [main] [1;31mERROR[0;39m o.s.b.d.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

The bean 'connectionFactory', defined in class path resource [com/zhigalko/producer/config/RedisConfig.class], could not be registered. A bean with that name has already been defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class] and overriding is disabled.

Action:

Consider renaming one of the beans or enabling overriding by setting spring.main.allow-bean-definition-overriding=true

2024-09-15 20:52:15 [main] [31mWARN [0;39m o.s.test.context.TestContextManager - Caught exception while allowing TestExecutionListener [org.springframework.test.context.web.ServletTestExecutionListener] to prepare test instance [com.zhigalko.producer.integration.listener.KafkaConsumerIT@234cd86c]
java.lang.IllegalStateException: Failed to load ApplicationContext for [WebMergedContextConfiguration@743c3520 testClass = com.zhigalko.producer.integration.listener.KafkaConsumerIT, locations = [], classes = [com.zhigalko.producer.ProducerApplication], contextInitializerClasses = [], activeProfiles = [], propertySourceDescriptors = [], propertySourceProperties = ["org.springframework.boot.test.context.SpringBootTestContextBootstrapper=true"], contextCustomizers = [[ImportsContextCustomizer@6842c101 key = [com.zhigalko.producer.integration.config.KafkaTestConfig]], org.springframework.boot.test.context.filter.ExcludeFilterContextCustomizer@5286c33a, org.springframework.boot.test.json.DuplicateJsonObjectContextCustomizerFactory$DuplicateJsonObjectContextCustomizer@c5ee75e, org.springframework.boot.test.mock.mockito.MockitoContextCustomizer@0, org.springframework.boot.test.web.client.TestRestTemplateContextCustomizer@47c4ecdc, org.springframework.boot.test.web.reactor.netty.DisableReactorResourceFactoryGlobalResourcesContextCustomizerFactory$DisableReactorResourceFactoryGlobalResourcesContextCustomizerCustomizer@4ba302e0, org.springframework.boot.test.autoconfigure.actuate.observability.ObservabilityContextCustomizerFactory$DisableObservabilityContextCustomizer@1f, org.springframework.boot.test.autoconfigure.properties.PropertyMappingContextCustomizer@0, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverContextCustomizer@3543df7d, org.springframework.test.context.support.DynamicPropertiesContextCustomizer@34c67b84, org.springframework.boot.testcontainers.service.connection.ServiceConnectionContextCustomizer@0, org.springframework.boot.test.context.SpringBootTestAnnotation@9b225ef1], resourceBasePath = "src/main/webapp", contextLoader = org.springframework.boot.test.context.SpringBootContextLoader, parent = null]
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:180)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:130)
	at org.springframework.test.context.web.ServletTestExecutionListener.setUpRequestContextIfNecessary(ServletTestExecutionListener.java:191)
	at org.springframework.test.context.web.ServletTestExecutionListener.prepareTestInstance(ServletTestExecutionListener.java:130)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:260)
	at org.springframework.test.context.junit.jupiter.SpringExtension.postProcessTestInstance(SpringExtension.java:163)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$10(ClassBasedTestDescriptor.java:378)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.executeAndMaskThrowable(ClassBasedTestDescriptor.java:383)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$11(ClassBasedTestDescriptor.java:378)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestInstancePostProcessors(ClassBasedTestDescriptor.java:377)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$instantiateAndPostProcessTestInstance$6(ClassBasedTestDescriptor.java:290)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:289)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:279)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:278)
	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$prepare$0(TestMethodTestDescriptor.java:106)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:105)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:69)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:63)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: org.springframework.beans.factory.support.BeanDefinitionOverrideException: Invalid bean definition with name 'connectionFactory' defined in class path resource [com/zhigalko/producer/config/RedisConfig.class]: Cannot register bean definition [Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=redisConfig; factoryMethodName=connectionFactory; initMethodNames=null; destroyMethodNames=[(inferred)]; defined in class path resource [com/zhigalko/producer/config/RedisConfig.class]] for bean 'connectionFactory' since there is already [Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=com.zhigalko.producer.integration.config.KafkaTestConfig; factoryMethodName=connectionFactory; initMethodNames=null; destroyMethodNames=[(inferred)]; defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]] bound.
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.registerBeanDefinition(DefaultListableBeanFactory.java:1017)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefinitionReader.java:277)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:144)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:120)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:429)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:290)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:349)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:118)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:789)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:607)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1463)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:553)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:225)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:152)
	... 72 common frames omitted
2024-09-15 20:52:55 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 20:52:55 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 20:52:55 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 20:52:55 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 20:52:55 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 20:52:56 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 20:52:56 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 20:52:56 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 20:52:56 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 20:52:56 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: bb5009f0d69bff7f3b8400b1b46c30d15f3794f5ad8a0a1b633d6fef8f9693d1
2024-09-15 20:52:56 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.728254017S
2024-09-15 20:52:56 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 20:52:56 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 20:52:56 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 20:52:56 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 20:52:57 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: ad16d81e327aa991856be154201af95a67ed7e002f16c39e5e1c459acfbec225
2024-09-15 20:52:57 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.847948194S
2024-09-15 20:52:58 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 20:52:58 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 6e5482e0c446fef65db6295c91635700cdde0533addb2914e343d2d5166863ca
2024-09-15 20:52:58 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.26231912S
2024-09-15 20:52:58 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 20:52:58 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 726c96545d51da9a2ef6b7c28488f0c5888cf4c08c512443f085a1de2dd09d3d
2024-09-15 20:53:01 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.050625816S
2024-09-15 20:53:01 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 20:53:01 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: cc0cc60fc2eef7f3570e0f424fc280a4aeb3fb56218b7e5cd172726ef7b78469
2024-09-15 20:53:02 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /practical_hamilton: Waiting for 60 seconds for URL: http://localhost:33704/subjects (where port 33704 maps to container port 8081)
2024-09-15 20:53:07 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.421892826S
2024-09-15 20:53:07 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 20:53:07 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 448346 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 20:53:07 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 20:53:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:53:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 20:53:08 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 74 ms. Found 1 MongoDB repository interface.
2024-09-15 20:53:08 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:53:08 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 20:53:08 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 20:53:08 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 10 ms. Found 0 Redis repository interfaces.
2024-09-15 20:53:08 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@50d05167, com.mongodb.Jep395RecordCodecProvider@1da32baf, com.mongodb.KotlinCodecProvider@4e4af370]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33700], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 20:53:08 [cluster-ClusterId{value='66e710f4c18415314b10e1ee', description='null'}-localhost:33700] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33700, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=22504594, setName='docker-rs', canonicalAddress=ad16d81e327a:27017, hosts=[ad16d81e327a:27017], passives=[], arbiters=[], primary='ad16d81e327a:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e710e98c7a9194d4bcd3a3, counter=6}, lastWriteDate=Sun Sep 15 20:52:58 GET 2024, lastUpdateTimeNanos=27505827227865}
2024-09-15 20:53:26 [main] [31mWARN [0;39m o.s.w.c.s.GenericWebApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'cacheManager' defined in class path resource [org/springframework/boot/autoconfigure/cache/RedisCacheConfiguration.class]: Unsatisfied dependency expressed through method 'cacheManager' parameter 4: No qualifying bean of type 'org.springframework.data.redis.connection.RedisConnectionFactory' available: expected single matching bean but found 2: testConnectionFactory,connectionFactory
2024-09-15 20:53:26 [main] [34mINFO [0;39m o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2024-09-15 20:53:26 [main] [1;31mERROR[0;39m o.s.b.d.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

Parameter 4 of method cacheManager in org.springframework.boot.autoconfigure.cache.RedisCacheConfiguration required a single bean, but 2 were found:
	- testConnectionFactory: defined by method 'testConnectionFactory' in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]
	- connectionFactory: defined by method 'connectionFactory' in class path resource [com/zhigalko/producer/config/RedisConfig.class]

This may be due to missing parameter name information

Action:

Consider marking one of the beans as @Primary, updating the consumer to accept multiple beans, or using @Qualifier to identify the bean that should be consumed

Ensure that your compiler is configured to use the '-parameters' flag.
You may need to update both your build tool settings as well as your IDE.
(See https://github.com/spring-projects/spring-framework/wiki/Upgrading-to-Spring-Framework-6.x#parameter-name-retention)


2024-09-15 20:53:26 [main] [31mWARN [0;39m o.s.test.context.TestContextManager - Caught exception while allowing TestExecutionListener [org.springframework.test.context.web.ServletTestExecutionListener] to prepare test instance [com.zhigalko.producer.integration.listener.KafkaConsumerIT@44f338ec]
java.lang.IllegalStateException: Failed to load ApplicationContext for [WebMergedContextConfiguration@78e4fa1 testClass = com.zhigalko.producer.integration.listener.KafkaConsumerIT, locations = [], classes = [com.zhigalko.producer.ProducerApplication], contextInitializerClasses = [], activeProfiles = [], propertySourceDescriptors = [], propertySourceProperties = ["org.springframework.boot.test.context.SpringBootTestContextBootstrapper=true"], contextCustomizers = [[ImportsContextCustomizer@4516c2ef key = [com.zhigalko.producer.integration.config.KafkaTestConfig]], org.springframework.boot.test.context.filter.ExcludeFilterContextCustomizer@5286c33a, org.springframework.boot.test.json.DuplicateJsonObjectContextCustomizerFactory$DuplicateJsonObjectContextCustomizer@c5ee75e, org.springframework.boot.test.mock.mockito.MockitoContextCustomizer@0, org.springframework.boot.test.web.client.TestRestTemplateContextCustomizer@47c4ecdc, org.springframework.boot.test.web.reactor.netty.DisableReactorResourceFactoryGlobalResourcesContextCustomizerFactory$DisableReactorResourceFactoryGlobalResourcesContextCustomizerCustomizer@4ba302e0, org.springframework.boot.test.autoconfigure.actuate.observability.ObservabilityContextCustomizerFactory$DisableObservabilityContextCustomizer@1f, org.springframework.boot.test.autoconfigure.properties.PropertyMappingContextCustomizer@0, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverContextCustomizer@3543df7d, org.springframework.test.context.support.DynamicPropertiesContextCustomizer@34c67b84, org.springframework.boot.testcontainers.service.connection.ServiceConnectionContextCustomizer@0, org.springframework.boot.test.context.SpringBootTestAnnotation@9b225ef1], resourceBasePath = "src/main/webapp", contextLoader = org.springframework.boot.test.context.SpringBootContextLoader, parent = null]
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:180)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:130)
	at org.springframework.test.context.web.ServletTestExecutionListener.setUpRequestContextIfNecessary(ServletTestExecutionListener.java:191)
	at org.springframework.test.context.web.ServletTestExecutionListener.prepareTestInstance(ServletTestExecutionListener.java:130)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:260)
	at org.springframework.test.context.junit.jupiter.SpringExtension.postProcessTestInstance(SpringExtension.java:163)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$10(ClassBasedTestDescriptor.java:378)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.executeAndMaskThrowable(ClassBasedTestDescriptor.java:383)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$11(ClassBasedTestDescriptor.java:378)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestInstancePostProcessors(ClassBasedTestDescriptor.java:377)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$instantiateAndPostProcessTestInstance$6(ClassBasedTestDescriptor.java:290)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:289)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:279)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:278)
	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$prepare$0(TestMethodTestDescriptor.java:106)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:105)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:69)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:63)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'cacheManager' defined in class path resource [org/springframework/boot/autoconfigure/cache/RedisCacheConfiguration.class]: Unsatisfied dependency expressed through method 'cacheManager' parameter 4: No qualifying bean of type 'org.springframework.data.redis.connection.RedisConnectionFactory' available: expected single matching bean but found 2: testConnectionFactory,connectionFactory
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:542)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:975)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:971)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:625)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1463)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:553)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:225)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:152)
	... 72 common frames omitted
Caused by: org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type 'org.springframework.data.redis.connection.RedisConnectionFactory' available: expected single matching bean but found 2: testConnectionFactory,connectionFactory
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveNotUnique(DependencyDescriptor.java:218)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 96 common frames omitted
2024-09-15 20:56:10 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 20:56:10 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 20:56:11 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 20:56:11 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 20:56:11 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 20:56:11 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 20:56:11 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 20:56:11 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 20:56:11 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 20:56:12 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: b1caf2031320a4dc76d2285a843a9f4fc61d467664a4cbe9699d4b2b25cff9ad
2024-09-15 20:56:12 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.706495357S
2024-09-15 20:56:12 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 20:56:12 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 20:56:12 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 20:56:12 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 20:56:12 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: f4fbc0efa79d39b8c1e9e452e5e7cb9fb150161f96c07b780c40780d8ad01c6d
2024-09-15 20:56:13 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.817201805S
2024-09-15 20:56:13 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 20:56:13 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: e85871abe430257d6ead9ab8e8c926179da7614edaea965fac5172812ee39676
2024-09-15 20:56:14 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.283025538S
2024-09-15 20:56:14 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 20:56:14 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 6d56ebb12fb390a9a6ec81d5530414eaecd3eed53e8e55ee42deedb61adb0ae9
2024-09-15 20:56:17 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.046991124S
2024-09-15 20:56:17 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 20:56:17 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 2e1388309d2558bb83f2b149c56de50692c052a6199162003ac68accaa34920a
2024-09-15 20:56:17 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /flamboyant_cray: Waiting for 60 seconds for URL: http://localhost:33710/subjects (where port 33710 maps to container port 8081)
2024-09-15 20:56:22 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.419978588S
2024-09-15 20:56:22 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 20:56:22 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 450336 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 20:56:22 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 20:56:23 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:56:23 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 20:56:23 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 76 ms. Found 1 MongoDB repository interface.
2024-09-15 20:56:23 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:56:23 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 20:56:23 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 20:56:23 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 20:56:24 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@46b2a11a, com.mongodb.Jep395RecordCodecProvider@55c8fc60, com.mongodb.KotlinCodecProvider@50d05167]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33706], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 20:56:24 [cluster-ClusterId{value='66e711b80b736e7b9c02b2a8', description='null'}-localhost:33706] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33706, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=26804036, setName='docker-rs', canonicalAddress=f4fbc0efa79d:27017, hosts=[f4fbc0efa79d:27017], passives=[], arbiters=[], primary='f4fbc0efa79d:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e711ac384dcd5d62562ff3, counter=6}, lastWriteDate=Sun Sep 15 20:56:13 GET 2024, lastUpdateTimeNanos=27701468468495}
2024-09-15 20:57:51 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 20:57:51 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 20:57:51 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 20:57:51 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 20:57:52 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 20:57:52 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 20:57:52 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 20:57:52 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 20:57:52 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 20:57:52 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 0f1f904b5c9b65590db09b535abcd9b5087abd3ab921e7d2565ce769d45b1172
2024-09-15 20:57:52 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.745195556S
2024-09-15 20:57:52 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 20:57:52 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 20:57:52 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 20:57:52 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 20:57:53 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 5b9b3aa7f07f3a0d1827d9475a15e39839c94500ceb0ee9285ad0ac94f685086
2024-09-15 20:57:53 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.75441771S
2024-09-15 20:57:54 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 20:57:54 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 5213773385bc9d859e115b3d2be8bcfdabb2cdd8878e0c2105e93945fda8ab94
2024-09-15 20:57:54 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.254800472S
2024-09-15 20:57:54 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 20:57:54 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 954d47786cc8b30ff3ffc1cdf9ea12f06f35bda149238d9e6be5ffeed8bdc3ae
2024-09-15 20:57:57 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.097759142S
2024-09-15 20:57:57 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 20:57:57 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 1c21f27acd9934aa8239823567cf530da2ab0fe2c8e84d6a2edc96297837e48c
2024-09-15 20:57:58 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /determined_hopper: Waiting for 60 seconds for URL: http://localhost:33716/subjects (where port 33716 maps to container port 8081)
2024-09-15 20:58:03 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.436318524S
2024-09-15 20:58:03 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 20:58:03 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 452007 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 20:58:03 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 20:58:03 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:58:04 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 20:58:04 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 102 ms. Found 1 MongoDB repository interface.
2024-09-15 20:58:04 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:58:04 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 20:58:04 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 20:58:04 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 20:58:04 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@13c81bc5, com.mongodb.Jep395RecordCodecProvider@5617168c, com.mongodb.KotlinCodecProvider@8167f57]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33712], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 20:58:04 [cluster-ClusterId{value='66e7121cc5783c0c83f13f3a', description='null'}-localhost:33712] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33712, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19124539, setName='docker-rs', canonicalAddress=5b9b3aa7f07f:27017, hosts=[5b9b3aa7f07f:27017], passives=[], arbiters=[], primary='5b9b3aa7f07f:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e712117b406b7d673ab847, counter=6}, lastWriteDate=Sun Sep 15 20:57:54 GET 2024, lastUpdateTimeNanos=27801945625074}
2024-09-15 20:58:27 [main] [31mWARN [0;39m o.s.w.c.s.GenericWebApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'cacheManager' defined in class path resource [org/springframework/boot/autoconfigure/cache/RedisCacheConfiguration.class]: Unsatisfied dependency expressed through method 'cacheManager' parameter 4: No qualifying bean of type 'org.springframework.data.redis.connection.RedisConnectionFactory' available: expected single matching bean but found 2: testConnectionFactory,connectionFactory
2024-09-15 20:58:27 [main] [34mINFO [0;39m o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2024-09-15 20:58:27 [main] [1;31mERROR[0;39m o.s.b.d.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

Parameter 4 of method cacheManager in org.springframework.boot.autoconfigure.cache.RedisCacheConfiguration required a single bean, but 2 were found:
	- testConnectionFactory: defined by method 'testConnectionFactory' in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]
	- connectionFactory: defined by method 'connectionFactory' in class path resource [com/zhigalko/producer/config/RedisConfig.class]

This may be due to missing parameter name information

Action:

Consider marking one of the beans as @Primary, updating the consumer to accept multiple beans, or using @Qualifier to identify the bean that should be consumed

Ensure that your compiler is configured to use the '-parameters' flag.
You may need to update both your build tool settings as well as your IDE.
(See https://github.com/spring-projects/spring-framework/wiki/Upgrading-to-Spring-Framework-6.x#parameter-name-retention)


2024-09-15 20:58:27 [main] [31mWARN [0;39m o.s.test.context.TestContextManager - Caught exception while allowing TestExecutionListener [org.springframework.test.context.web.ServletTestExecutionListener] to prepare test instance [com.zhigalko.producer.integration.listener.KafkaConsumerIT@5a079446]
java.lang.IllegalStateException: Failed to load ApplicationContext for [WebMergedContextConfiguration@3cd89c72 testClass = com.zhigalko.producer.integration.listener.KafkaConsumerIT, locations = [], classes = [com.zhigalko.producer.ProducerApplication], contextInitializerClasses = [], activeProfiles = [], propertySourceDescriptors = [], propertySourceProperties = ["org.springframework.boot.test.context.SpringBootTestContextBootstrapper=true"], contextCustomizers = [[ImportsContextCustomizer@7fb46c10 key = [com.zhigalko.producer.integration.config.KafkaTestConfig]], org.springframework.boot.test.context.filter.ExcludeFilterContextCustomizer@5286c33a, org.springframework.boot.test.json.DuplicateJsonObjectContextCustomizerFactory$DuplicateJsonObjectContextCustomizer@c5ee75e, org.springframework.boot.test.mock.mockito.MockitoContextCustomizer@0, org.springframework.boot.test.web.client.TestRestTemplateContextCustomizer@47c4ecdc, org.springframework.boot.test.web.reactor.netty.DisableReactorResourceFactoryGlobalResourcesContextCustomizerFactory$DisableReactorResourceFactoryGlobalResourcesContextCustomizerCustomizer@4ba302e0, org.springframework.boot.test.autoconfigure.actuate.observability.ObservabilityContextCustomizerFactory$DisableObservabilityContextCustomizer@1f, org.springframework.boot.test.autoconfigure.properties.PropertyMappingContextCustomizer@0, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverContextCustomizer@3543df7d, org.springframework.test.context.support.DynamicPropertiesContextCustomizer@34c67b84, org.springframework.boot.testcontainers.service.connection.ServiceConnectionContextCustomizer@0, org.springframework.boot.test.context.SpringBootTestAnnotation@9b225ef1], resourceBasePath = "src/main/webapp", contextLoader = org.springframework.boot.test.context.SpringBootContextLoader, parent = null]
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:180)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:130)
	at org.springframework.test.context.web.ServletTestExecutionListener.setUpRequestContextIfNecessary(ServletTestExecutionListener.java:191)
	at org.springframework.test.context.web.ServletTestExecutionListener.prepareTestInstance(ServletTestExecutionListener.java:130)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:260)
	at org.springframework.test.context.junit.jupiter.SpringExtension.postProcessTestInstance(SpringExtension.java:163)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$10(ClassBasedTestDescriptor.java:378)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.executeAndMaskThrowable(ClassBasedTestDescriptor.java:383)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$11(ClassBasedTestDescriptor.java:378)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestInstancePostProcessors(ClassBasedTestDescriptor.java:377)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$instantiateAndPostProcessTestInstance$6(ClassBasedTestDescriptor.java:290)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:289)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:279)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:278)
	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$prepare$0(TestMethodTestDescriptor.java:106)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:105)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:69)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:63)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'cacheManager' defined in class path resource [org/springframework/boot/autoconfigure/cache/RedisCacheConfiguration.class]: Unsatisfied dependency expressed through method 'cacheManager' parameter 4: No qualifying bean of type 'org.springframework.data.redis.connection.RedisConnectionFactory' available: expected single matching bean but found 2: testConnectionFactory,connectionFactory
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:542)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:975)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:971)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:625)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1463)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:553)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:225)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:152)
	... 72 common frames omitted
Caused by: org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type 'org.springframework.data.redis.connection.RedisConnectionFactory' available: expected single matching bean but found 2: testConnectionFactory,connectionFactory
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveNotUnique(DependencyDescriptor.java:218)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 96 common frames omitted
2024-09-15 20:59:40 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 20:59:40 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 20:59:41 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 20:59:41 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 20:59:41 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 20:59:41 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 20:59:41 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 20:59:41 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 20:59:41 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 20:59:42 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: e528682d4fd814b7fcdfc1b4c1ac322c07c32105015b586e1c2bd8f3175c1714
2024-09-15 20:59:42 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.739681886S
2024-09-15 20:59:42 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 20:59:42 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 20:59:42 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 20:59:42 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 20:59:42 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 610e657bcdb2eaa536cfbb9fd86ed1197978bcef9f5664abd7eeeb73cdbba7fb
2024-09-15 20:59:43 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.800170326S
2024-09-15 20:59:43 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 20:59:43 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 8699952b0337d4428927bfca1ea4d55415a99a5ecf9f709691cdc256d595b506
2024-09-15 20:59:44 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.284127351S
2024-09-15 20:59:44 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 20:59:44 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 7fbee4b1a8710ae841ce97dd6ba5f693e2b0db97933e566203046e86e600bd49
2024-09-15 20:59:47 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.089101496S
2024-09-15 20:59:47 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 20:59:47 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 9877191242c581a4cfcf48830b8daff6de63103754ceb5d1477fd3c1325c0b61
2024-09-15 20:59:47 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /vibrant_pike: Waiting for 60 seconds for URL: http://localhost:33722/subjects (where port 33722 maps to container port 8081)
2024-09-15 20:59:52 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.392738982S
2024-09-15 20:59:52 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 20:59:52 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 453699 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 20:59:52 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 20:59:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:59:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 20:59:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 67 ms. Found 1 MongoDB repository interface.
2024-09-15 20:59:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 20:59:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 20:59:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 20:59:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 21:01:02 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@767d9b9, com.mongodb.Jep395RecordCodecProvider@593f7d2e, com.mongodb.KotlinCodecProvider@6e8aea7e]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33718], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:01:02 [cluster-ClusterId{value='66e712ceef13a3084d1188c8', description='null'}-localhost:33718] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33718, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=24201097, setName='docker-rs', canonicalAddress=610e657bcdb2:27017, hosts=[610e657bcdb2:27017], passives=[], arbiters=[], primary='610e657bcdb2:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e7127e4b263a7b22d5c447, counter=6}, lastWriteDate=Sun Sep 15 21:00:53 GET 2024, lastUpdateTimeNanos=27979727289274}
2024-09-15 21:01:03 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33720]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:01:03 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:01:03 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33722]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:01:03 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33722]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:01:03 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:01:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:01:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:01:03 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726419663973
2024-09-15 21:01:03 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:01:03 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 71.257 seconds (process running for 83.52)
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: XofwUmtdQ4yEDreEmTjFDw
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33720 (id: 2147483646 rack: null)
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-1739ff46-f410-4185-98a4-da2f91a61928
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-1739ff46-f410-4185-98a4-da2f91a61928', protocol='range'}
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-1739ff46-f410-4185-98a4-da2f91a61928=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-1739ff46-f410-4185-98a4-da2f91a61928', protocol='range'}
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33720 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:01:04 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33720]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:01:04 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:01:04 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33722]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:01:04 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:01:04 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:01:04 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:01:04 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:01:04 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726419664575
2024-09-15 21:01:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: XofwUmtdQ4yEDreEmTjFDw
2024-09-15 21:01:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:01:04 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:01:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "ea5d96b1-4ba8-407f-ab5e-4574005af8ed", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:01:04.542809105Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726419664582, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "ea5d96b1-4ba8-407f-ab5e-4574005af8ed", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:01:04.542809105Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:01:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:01:17 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:01:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:01:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:01:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:01:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 16 due to node 1 being disconnected (elapsed time since creation: 104ms, elapsed time since send: 104ms, request timeout: 30000ms)
2024-09-15 21:01:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:01:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:01:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:01:17 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33720 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:01:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=722256792, epoch=2) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:01:18 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:01:18 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33720) could not be established. Node may not be available.
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33720) could not be established. Node may not be available.
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33720) could not be established. Node may not be available.
2024-09-15 21:01:18 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:01:18 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33720) could not be established. Node may not be available.
2024-09-15 21:01:18 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33719
2024-09-15 21:01:18 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33719]: Connection refused: localhost/127.0.0.1:33719
2024-09-15 21:01:18 [cluster-ClusterId{value='66e712ceef13a3084d1188c8', description='null'}-localhost:33718] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33718
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33720) could not be established. Node may not be available.
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:01:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:01:18 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:01:18 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:01:18 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:01:18 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:01:18 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:01:18 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:11:02 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:11:02 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:11:03 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:11:03 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:11:03 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:11:03 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:11:03 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:11:03 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:11:03 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:11:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 06cddc897a159509c1330427087ee20ee4f2201f002c51996efa00860a1183aa
2024-09-15 21:11:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.763562536S
2024-09-15 21:11:04 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:11:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:11:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:11:04 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:11:04 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 8328eb53fda008ea9350912f98ad9b45bc2096fa2d4c176f95aa1259d76d2663
2024-09-15 21:11:05 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.804320371S
2024-09-15 21:11:05 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:11:05 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 049e3c37fd0b2cb06c723d000a0fbdd1d16a530ab8d7dbc5ae31b764051f38a0
2024-09-15 21:11:06 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.25735396S
2024-09-15 21:11:06 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:11:06 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: dcf251fa2e5047853d489c9bf8afcb6aa4069cb1d1ef4f6f33260bdb3eeea2db
2024-09-15 21:11:09 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT2.995056467S
2024-09-15 21:11:09 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:11:09 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: b5e6c3a93f45c758a546689c2597e79adf239eac33f2e85c76951c83365e21e6
2024-09-15 21:11:09 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /naughty_williamson: Waiting for 60 seconds for URL: http://localhost:33728/subjects (where port 33728 maps to container port 8081)
2024-09-15 21:11:14 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.411827076S
2024-09-15 21:11:14 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:11:14 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 456515 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:11:14 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:11:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:11:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:11:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 79 ms. Found 1 MongoDB repository interface.
2024-09-15 21:11:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:11:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:11:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:11:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 21:11:15 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@42066f0d, com.mongodb.Jep395RecordCodecProvider@687e561b, com.mongodb.KotlinCodecProvider@299786b1]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33724], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:11:15 [cluster-ClusterId{value='66e715333d2ed331894f823e', description='null'}-localhost:33724] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33724, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=16138957, setName='docker-rs', canonicalAddress=8328eb53fda0:27017, hosts=[8328eb53fda0:27017], passives=[], arbiters=[], primary='8328eb53fda0:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71528d9a91cd07d70aa30, counter=6}, lastWriteDate=Sun Sep 15 21:11:05 GET 2024, lastUpdateTimeNanos=28592939846415}
2024-09-15 21:11:16 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33726]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:11:16 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:11:16 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33728]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:11:16 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33728]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:11:16 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:11:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:11:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:11:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420276892
2024-09-15 21:11:16 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:11:16 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.324 seconds (process running for 14.526)
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: iBanex1iRc6QlGbNhXoBYA
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33726 (id: 2147483646 rack: null)
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-568d356b-1697-4222-89bd-816217ceb026
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-568d356b-1697-4222-89bd-816217ceb026', protocol='range'}
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-568d356b-1697-4222-89bd-816217ceb026=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-568d356b-1697-4222-89bd-816217ceb026', protocol='range'}
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33726 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:11:17 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33726]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:11:17 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:11:17 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33728]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:11:17 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:11:17 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:11:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:11:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:11:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420277599
2024-09-15 21:11:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: iBanex1iRc6QlGbNhXoBYA
2024-09-15 21:11:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:11:17 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:11:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "75aa9605-9ee0-4a47-9f10-162fc5b147ab", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:11:17.568078207Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726420277606, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "75aa9605-9ee0-4a47-9f10-162fc5b147ab", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:11:17.568078207Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:11:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:11:21 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:11:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 16 due to node 1 being disconnected (elapsed time since creation: 86ms, elapsed time since send: 86ms, request timeout: 30000ms)
2024-09-15 21:11:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:11:22 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33726 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=943264961, epoch=2) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33726) could not be established. Node may not be available.
2024-09-15 21:11:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:22 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33726) could not be established. Node may not be available.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33726) could not be established. Node may not be available.
2024-09-15 21:11:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:22 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33726) could not be established. Node may not be available.
2024-09-15 21:11:22 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33725
2024-09-15 21:11:22 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33725]: Connection refused: localhost/127.0.0.1:33725
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33726) could not be established. Node may not be available.
2024-09-15 21:11:22 [cluster-ClusterId{value='66e715333d2ed331894f823e', description='null'}-localhost:33724] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33724
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:11:22 [cluster-ClusterId{value='66e715333d2ed331894f823e', description='null'}-localhost:33724] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33724
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:11:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:11:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:11:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:11:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:11:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:11:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:11:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:11:31 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:11:31 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:11:32 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:11:32 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:11:32 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:11:32 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:11:32 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:11:32 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:11:32 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:11:33 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: e62449c150fe3d911aa24cf28a0d03e91169a5f7aaf21b611042f1439e8c7972
2024-09-15 21:11:33 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.731344147S
2024-09-15 21:11:33 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:11:33 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:11:33 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:11:33 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:11:33 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 839a719aa943b7e23cec3c5f7f4416dc650c68dba9b19560be50d3a187e7d2f4
2024-09-15 21:11:34 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.767420403S
2024-09-15 21:11:34 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:11:34 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 8781b6505a96ad6de9784fcfde8cd202f8ad5d86c4732889fea2da70c9346222
2024-09-15 21:11:35 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.231925855S
2024-09-15 21:11:35 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:11:35 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 7285f9bcb9edce29434042b6f7081e177cf3b6508b7bcf8aeda2383177357f3e
2024-09-15 21:11:38 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.07751926S
2024-09-15 21:11:38 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:11:38 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: d8dfff9167a4c9ebaba00a17ee4f66f2377ce722ee9bdc27eebb7c4dd3967519
2024-09-15 21:11:38 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /recursing_mcnulty: Waiting for 60 seconds for URL: http://localhost:33734/subjects (where port 33734 maps to container port 8081)
2024-09-15 21:11:43 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.409238425S
2024-09-15 21:11:43 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:11:43 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 458148 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:11:43 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:11:44 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:11:44 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:11:44 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 77 ms. Found 1 MongoDB repository interface.
2024-09-15 21:11:44 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:11:44 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:11:44 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:11:44 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 7 ms. Found 0 Redis repository interfaces.
2024-09-15 21:11:44 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@b144175, com.mongodb.Jep395RecordCodecProvider@38923cfe, com.mongodb.KotlinCodecProvider@1ac3a6f]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33730], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:11:44 [cluster-ClusterId{value='66e71550bc1cb5192f9c2926', description='null'}-localhost:33730] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33730, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=20965803, setName='docker-rs', canonicalAddress=839a719aa943:27017, hosts=[839a719aa943:27017], passives=[], arbiters=[], primary='839a719aa943:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71545a6bc2c59d7d6cdb1, counter=6}, lastWriteDate=Sun Sep 15 21:11:34 GET 2024, lastUpdateTimeNanos=28622121407188}
2024-09-15 21:11:45 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33732]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:11:45 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:11:46 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33734]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:11:46 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33734]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420306270
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:11:46 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.728 seconds (process running for 14.861)
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: tPz4HefLRGavFnRzq7FPsw
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33732 (id: 2147483646 rack: null)
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-3d43b15f-eeea-4dee-96ac-3279a0d094e8
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-3d43b15f-eeea-4dee-96ac-3279a0d094e8', protocol='range'}
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-3d43b15f-eeea-4dee-96ac-3279a0d094e8=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-3d43b15f-eeea-4dee-96ac-3279a0d094e8', protocol='range'}
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33732 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:11:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33732]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:11:46 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33734]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:11:46 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420306969
2024-09-15 21:11:46 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: tPz4HefLRGavFnRzq7FPsw
2024-09-15 21:11:47 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:11:47 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:11:47 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "eb13817b-47fd-4334-af5c-9af0a29496fd", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:11:46.940891442Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:11:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726420306976, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "eb13817b-47fd-4334-af5c-9af0a29496fd", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:11:46.940891442Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:11:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:11:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:11:51 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:11:51 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:51 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:11:51 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:51 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:11:51 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:11:51 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33732 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:11:51 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:51 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:51 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:51 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:51 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:51 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:51 [cluster-ClusterId{value='66e71550bc1cb5192f9c2926', description='null'}-localhost:33730] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33730
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:11:51 [cluster-ClusterId{value='66e71550bc1cb5192f9c2926', description='null'}-localhost:33730] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33730
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:11:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:52 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:52 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:52 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:52 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:52 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:53 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:53 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:53 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:53 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:54 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:54 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:58 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:11:58 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:11:58 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:58 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33732) could not be established. Node may not be available.
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Seeking to offset 0 for partition CustomerViewEventTopic-0
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(org.apache.kafka.clients.consumer.ConsumerRecord<java.lang.String, com.zhigalko.core.schema.CustomerViewAvroEvent>)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 11 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.translateException(LettuceConnectionFactory.java:1847)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1778)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getNativeConnection(LettuceConnectionFactory.java:1580)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.lambda$getConnection$0(LettuceConnectionFactory.java:1560)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.doInLock(LettuceConnectionFactory.java:1521)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$SharedConnection.getConnection(LettuceConnectionFactory.java:1557)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:1243)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:1049)
	at org.springframework.data.redis.core.RedisConnectionUtils.fetchConnection(RedisConnectionUtils.java:195)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:144)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:105)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:383)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:363)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:97)
	at org.springframework.data.redis.core.DefaultValueOperations.set(DefaultValueOperations.java:253)
	at org.springframework.data.redis.core.ValueOperations.set(ValueOperations.java:75)
	at com.zhigalko.producer.service.impl.CacheServiceImpl.save(CacheServiceImpl.java:23)
	at com.zhigalko.producer.service.impl.CustomerQueryServiceImpl.saveCustomerProjection(CustomerQueryServiceImpl.java:52)
	at com.zhigalko.producer.projector.CustomerProjector.project(CustomerProjector.java:28)
	at com.zhigalko.producer.listener.KafkaConsumer.listenCustomerViewTopic(KafkaConsumer.java:21)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 13 common frames omitted
Caused by: io.lettuce.core.RedisConnectionException: Unable to connect to localhost/<unresolved>:33731
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:78)
	at io.lettuce.core.RedisConnectionException.create(RedisConnectionException.java:56)
	at io.lettuce.core.AbstractRedisClient.getConnection(AbstractRedisClient.java:350)
	at io.lettuce.core.RedisClient.connect(RedisClient.java:215)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.lambda$getConnection$1(StandaloneConnectionProvider.java:112)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.springframework.data.redis.connection.lettuce.StandaloneConnectionProvider.getConnection(StandaloneConnectionProvider.java:112)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory$ExceptionTranslatingConnectionProvider.getConnection(LettuceConnectionFactory.java:1776)
	... 44 common frames omitted
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:33731
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:11:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:11:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:11:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:11:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:11:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:11:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:11:58 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:12:03 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:12:03 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:12:03 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:12:03 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:12:04 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:12:04 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:12:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:12:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:12:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:12:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: caf89d0b760bf247609243b1072d2618b7afd96b4acf74e8c9a6ee0be5f01248
2024-09-15 21:12:05 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.751820772S
2024-09-15 21:12:05 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:12:05 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:12:05 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:12:05 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:12:05 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: eafeee9a0e645279ee2f122e301fe1fc87ddae5011cb870c8fc28ad9acc6897b
2024-09-15 21:12:05 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.793239088S
2024-09-15 21:12:06 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:12:06 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 7380783403e6c4b4445bce1cc9c2ec462b4c8edbaae50e2b0735ffd945689429
2024-09-15 21:12:06 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.266895441S
2024-09-15 21:12:06 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:12:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: f133faad95387418d37758ffde929079f93e15d57f9df9e6439105f90e695d7c
2024-09-15 21:12:10 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.151768076S
2024-09-15 21:12:10 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:12:10 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: c60baf8006bca7112a1df2a203bc6b873d6bafcf07b16ac7863c98c86f5cd311
2024-09-15 21:12:10 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /vigorous_meninsky: Waiting for 60 seconds for URL: http://localhost:33740/subjects (where port 33740 maps to container port 8081)
2024-09-15 21:12:15 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.426733845S
2024-09-15 21:12:15 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:12:15 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 459787 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:12:15 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:12:16 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:12:16 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:12:16 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 82 ms. Found 1 MongoDB repository interface.
2024-09-15 21:12:16 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:12:16 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:12:16 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:12:16 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 21:12:16 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@fee7ca, com.mongodb.Jep395RecordCodecProvider@29c80149, com.mongodb.KotlinCodecProvider@14ad42]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33736], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:12:16 [cluster-ClusterId{value='66e71570a162bc55fe417eb0', description='null'}-localhost:33736] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33736, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=17193322, setName='docker-rs', canonicalAddress=eafeee9a0e64:27017, hosts=[eafeee9a0e64:27017], passives=[], arbiters=[], primary='eafeee9a0e64:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71565b4cebaf2e7402212, counter=6}, lastWriteDate=Sun Sep 15 21:12:06 GET 2024, lastUpdateTimeNanos=28653963794621}
2024-09-15 21:12:17 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33738]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:12:17 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:12:17 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33740]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:12:17 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33740]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420338062
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:12:18 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.516 seconds (process running for 14.907)
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: haVK4IqYRpqUYxoogkBKfA
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33738 (id: 2147483646 rack: null)
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-61ac7c36-3573-4d22-8f17-5b0893742d34
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-61ac7c36-3573-4d22-8f17-5b0893742d34', protocol='range'}
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-61ac7c36-3573-4d22-8f17-5b0893742d34=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-61ac7c36-3573-4d22-8f17-5b0893742d34', protocol='range'}
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33738 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:12:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33738]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:12:18 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33740]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:12:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420338881
2024-09-15 21:12:18 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: haVK4IqYRpqUYxoogkBKfA
2024-09-15 21:12:18 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:12:19 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:12:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "b98d5ea1-b999-4ba7-aef1-5a0804ed4955", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:12:18.850162663Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:12:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726420338994, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "b98d5ea1-b999-4ba7-aef1-5a0804ed4955", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:12:18.850162663Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:12:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:12:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:15:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33738 (id: 2147483646 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 21:15:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:33738 (id: 2147483646 rack: null)
2024-09-15 21:15:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483646
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33738 (id: 2147483646 rack: null)
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33738 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:33738 (id: 2147483646 rack: null)
2024-09-15 21:15:29 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33738 (id: 2147483646 rack: null)
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Offset commit failed on partition CustomerViewEventTopic-0 at offset 1: The coordinator is not aware of this member.
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] OffsetCommit failed with Generation{generationId=1, memberId='consumer-customerViewConsumer-1-61ac7c36-3573-4d22-8f17-5b0893742d34', protocol='range'}: The coordinator is not aware of this member.
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: encountered UNKNOWN_MEMBER_ID from OFFSET_COMMIT response
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: encountered UNKNOWN_MEMBER_ID from OFFSET_COMMIT response
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Giving away all assigned partitions as lost since generation/memberID has been reset,indicating that consumer is in old state or no longer part of the group
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Lost previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions lost: [CustomerViewEventTopic-0]
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-4e9f5ad2-c581-4367-91fb-40f9fcf0c71d
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=3, memberId='consumer-customerViewConsumer-1-4e9f5ad2-c581-4367-91fb-40f9fcf0c71d', protocol='range'}
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 3: {consumer-customerViewConsumer-1-4e9f5ad2-c581-4367-91fb-40f9fcf0c71d=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=3, memberId='consumer-customerViewConsumer-1-4e9f5ad2-c581-4367-91fb-40f9fcf0c71d', protocol='range'}
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33738 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726420338994, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "b98d5ea1-b999-4ba7-aef1-5a0804ed4955", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:12:18.850162663Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:15:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:15:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 27 due to node 1 being disconnected (elapsed time since creation: 45ms, elapsed time since send: 45ms, request timeout: 30000ms)
2024-09-15 21:15:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=277763584, epoch=5) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33738 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:15:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:15:30 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33738) could not be established. Node may not be available.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33738) could not be established. Node may not be available.
2024-09-15 21:15:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:15:30 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33738) could not be established. Node may not be available.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:15:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33738) could not be established. Node may not be available.
2024-09-15 21:15:30 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33737
2024-09-15 21:15:30 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33737]: Connection refused: localhost/127.0.0.1:33737
2024-09-15 21:15:30 [cluster-ClusterId{value='66e71570a162bc55fe417eb0', description='null'}-localhost:33736] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33736
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:15:30 [cluster-ClusterId{value='66e71570a162bc55fe417eb0', description='null'}-localhost:33736] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33736
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:15:31 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:15:31 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33738) could not be established. Node may not be available.
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33738) could not be established. Node may not be available.
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:15:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:15:31 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:15:31 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:15:31 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:15:31 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:15:31 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:15:31 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:21:18 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:21:18 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:21:19 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:21:19 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:21:19 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:21:19 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:21:19 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:21:19 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:21:19 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:21:20 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 8d3bf16efe7230b182818178500cbad1bec36ca886d44776ab8a54ca4e97a831
2024-09-15 21:21:20 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.774231111S
2024-09-15 21:21:20 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:21:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:21:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:21:20 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:21:20 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 81ac4627bf1d54b61635cb3e7379a9fd110df94acff23aff314773c3068f77bc
2024-09-15 21:21:21 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.806541206S
2024-09-15 21:21:21 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:21:21 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: fbde06d1d56b68623021e5b2d4ddb7433bb7738dd9f6e41628d7bc4f965102a5
2024-09-15 21:21:22 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.254696773S
2024-09-15 21:21:22 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:21:22 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 33442a9db24f60abb327740b1dd98f67b1900f8d07ce92df42e14ed41c245be3
2024-09-15 21:21:25 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.016133596S
2024-09-15 21:21:25 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:21:25 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 9917b250bf7b57676d101a27f4b4cbeee5bff31dfe9a6c56de08227a8dbdb19a
2024-09-15 21:21:25 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /elated_euler: Waiting for 60 seconds for URL: http://localhost:33746/subjects (where port 33746 maps to container port 8081)
2024-09-15 21:21:30 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.444683472S
2024-09-15 21:21:30 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:21:30 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 462365 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:21:30 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:21:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:21:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:21:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 66 ms. Found 1 MongoDB repository interface.
2024-09-15 21:21:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:21:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:21:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:21:31 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 7 ms. Found 0 Redis repository interfaces.
2024-09-15 21:21:31 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@7747cc1b, com.mongodb.Jep395RecordCodecProvider@76cf91c9, com.mongodb.KotlinCodecProvider@74bfdd66]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33742], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:21:31 [cluster-ClusterId{value='66e7179b6306e568fd623d64', description='null'}-localhost:33742] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33742, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=14536902, setName='docker-rs', canonicalAddress=81ac4627bf1d:27017, hosts=[81ac4627bf1d:27017], passives=[], arbiters=[], primary='81ac4627bf1d:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e717903e79092804018e11, counter=6}, lastWriteDate=Sun Sep 15 21:21:21 GET 2024, lastUpdateTimeNanos=29208901471918}
2024-09-15 21:21:32 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33744]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:21:32 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:21:32 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33746]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:21:32 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33746]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:21:32 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:21:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:21:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:21:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420892861
2024-09-15 21:21:32 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:21:32 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.205 seconds (process running for 14.283)
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: d7--px9yTlmbcp-ht6J8pA
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33744 (id: 2147483646 rack: null)
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-5d03491c-5686-436e-b63f-604eb21e7540
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-5d03491c-5686-436e-b63f-604eb21e7540', protocol='range'}
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-5d03491c-5686-436e-b63f-604eb21e7540=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-5d03491c-5686-436e-b63f-604eb21e7540', protocol='range'}
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33744 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:21:33 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33744]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:21:33 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:21:33 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33746]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:21:33 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:21:33 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:21:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:21:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:21:33 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420893308
2024-09-15 21:21:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: d7--px9yTlmbcp-ht6J8pA
2024-09-15 21:21:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:21:33 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:21:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "b748f340-3a62-4c10-9363-7755801607e0", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:21:33.280335914Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726420893314, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "b748f340-3a62-4c10-9363-7755801607e0", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:21:33.280335914Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:21:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:21:36 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:21:39 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:21:42 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:21:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:21:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:21:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 42 due to node 1 being disconnected (elapsed time since creation: 212ms, elapsed time since send: 212ms, request timeout: 30000ms)
2024-09-15 21:21:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:21:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:21:45 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:21:45 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33744 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:21:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1185737150, epoch=25) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:21:46 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:21:46 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33744) could not be established. Node may not be available.
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33744) could not be established. Node may not be available.
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33744) could not be established. Node may not be available.
2024-09-15 21:21:46 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33743
2024-09-15 21:21:46 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33743]: Connection refused: localhost/127.0.0.1:33743
2024-09-15 21:21:46 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:21:46 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33744) could not be established. Node may not be available.
2024-09-15 21:21:46 [cluster-ClusterId{value='66e7179b6306e568fd623d64', description='null'}-localhost:33742] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33742
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:21:46 [cluster-ClusterId{value='66e7179b6306e568fd623d64', description='null'}-localhost:33742] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33742
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33744) could not be established. Node may not be available.
2024-09-15 21:21:46 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:21:46 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33744) could not be established. Node may not be available.
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:21:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:21:46 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:21:46 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:21:46 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:21:46 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:21:46 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:21:46 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:22:46 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:22:46 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:22:47 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 4822cbc1ec5f140fe22295ef48fb6c7eecc81c1bdf90e3d29b5ed65f65b67666
2024-09-15 21:22:47 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.709976671S
2024-09-15 21:22:47 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:22:47 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:22:47 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:22:47 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:22:47 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: f592ee4d03fbb978f04f066823127bdade31a3dc875732f5fe1a56e40574391c
2024-09-15 21:22:48 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.775893026S
2024-09-15 21:22:49 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:22:49 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 9ed1df3c751e6f755405adc3fe4fbead33854043360441cc83739d9dfc788914
2024-09-15 21:22:49 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.257512309S
2024-09-15 21:22:49 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:22:49 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 7d89020b50a9cf4b5c7b67532e0060a7dfebb9c09545b984c4cc2300ba39cde7
2024-09-15 21:22:52 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.028156676S
2024-09-15 21:22:52 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:22:52 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: efc6b678b4e5b85130ddffdf9123266d8a797d569062ec254f32219bc88891cf
2024-09-15 21:22:52 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /inspiring_ardinghelli: Waiting for 60 seconds for URL: http://localhost:33752/subjects (where port 33752 maps to container port 8081)
2024-09-15 21:22:57 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.454154492S
2024-09-15 21:22:58 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:22:58 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 464182 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:22:58 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:22:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:22:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:22:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 77 ms. Found 1 MongoDB repository interface.
2024-09-15 21:22:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:22:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:22:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:22:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 16 ms. Found 0 Redis repository interfaces.
2024-09-15 21:22:59 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@fee7ca, com.mongodb.Jep395RecordCodecProvider@29c80149, com.mongodb.KotlinCodecProvider@14ad42]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33748], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:22:59 [cluster-ClusterId{value='66e717f3da47421b0e3ea2fc', description='null'}-localhost:33748] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33748, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=17936786, setName='docker-rs', canonicalAddress=f592ee4d03fb:27017, hosts=[f592ee4d03fb:27017], passives=[], arbiters=[], primary='f592ee4d03fb:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e717e86f8b7cd1143fb1b7, counter=6}, lastWriteDate=Sun Sep 15 21:22:49 GET 2024, lastUpdateTimeNanos=29296459837779}
2024-09-15 21:23:00 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33750]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:23:00 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:23:00 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33752]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:23:00 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33752]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:23:00 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:23:00 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:23:00 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:23:00 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420980681
2024-09-15 21:23:00 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:23:00 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.746 seconds (process running for 14.932)
2024-09-15 21:23:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:23:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: TBnp-6ApS0GJoZpwfKe-ww
2024-09-15 21:23:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33750 (id: 2147483646 rack: null)
2024-09-15 21:23:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:23:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-c3ffddc6-0e26-4fcd-ade3-52bb6b62016d
2024-09-15 21:23:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:23:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-c3ffddc6-0e26-4fcd-ade3-52bb6b62016d', protocol='range'}
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-c3ffddc6-0e26-4fcd-ade3-52bb6b62016d=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-c3ffddc6-0e26-4fcd-ade3-52bb6b62016d', protocol='range'}
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33750 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:23:01 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33750]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:23:01 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:23:01 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33752]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:23:01 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:23:01 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:23:01 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:23:01 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:23:01 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726420981275
2024-09-15 21:23:01 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: TBnp-6ApS0GJoZpwfKe-ww
2024-09-15 21:23:01 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:23:01 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:23:01 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "7f6ec40c-1e6f-4169-88e1-2c321db7232e", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:23:01.248577535Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726420981283, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "7f6ec40c-1e6f-4169-88e1-2c321db7232e", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:23:01.248577535Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:23:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:23:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:23:06 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 23 due to node 1 being disconnected (elapsed time since creation: 293ms, elapsed time since send: 293ms, request timeout: 30000ms)
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:23:19 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33750 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=563669705, epoch=6) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33750) could not be established. Node may not be available.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33750) could not be established. Node may not be available.
2024-09-15 21:23:19 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33749
2024-09-15 21:23:19 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33749]: Connection refused: localhost/127.0.0.1:33749
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33750) could not be established. Node may not be available.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33750) could not be established. Node may not be available.
2024-09-15 21:23:19 [cluster-ClusterId{value='66e717f3da47421b0e3ea2fc', description='null'}-localhost:33748] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33748
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:23:19 [cluster-ClusterId{value='66e717f3da47421b0e3ea2fc', description='null'}-localhost:33748] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33748
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:23:19 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33750) could not be established. Node may not be available.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33750) could not be established. Node may not be available.
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:23:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:23:19 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:23:19 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:23:19 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:23:19 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:23:19 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:23:19 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:24:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:24:20 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:24:21 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: fb6e3d52b7436132f15e37e0d5772fb1235a58d38673dde7791ec6e7c26a9cb7
2024-09-15 21:24:21 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.718137818S
2024-09-15 21:24:21 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:24:21 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:24:21 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:24:21 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:24:21 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 57c64fa3ee3f28ea81d067da79d9efd97e09be14de79d313ba1bf73900807099
2024-09-15 21:24:22 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.77914614S
2024-09-15 21:24:23 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:24:23 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 79d2a89f42e466179ae127f27d9b6181a4510155d1aa756c49de681cc2d89f00
2024-09-15 21:24:23 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.264906894S
2024-09-15 21:24:23 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:24:23 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: d526fb840e525bd4a8578ef04ef113dc8d6d7b9bd06f52d716349db5e59b926f
2024-09-15 21:24:26 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.070240683S
2024-09-15 21:24:26 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:24:26 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 2d90290621c638f08f4a34ee1b0f6947c04eaff56b7cce99bad8d6ea32e475ad
2024-09-15 21:24:26 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /boring_fermat: Waiting for 60 seconds for URL: http://localhost:33758/subjects (where port 33758 maps to container port 8081)
2024-09-15 21:24:31 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.440102876S
2024-09-15 21:24:31 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:24:31 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 465960 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:24:31 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:24:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:24:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:24:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 87 ms. Found 1 MongoDB repository interface.
2024-09-15 21:24:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:24:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:24:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:24:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 6 ms. Found 0 Redis repository interfaces.
2024-09-15 21:24:33 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@173cfb01, com.mongodb.Jep395RecordCodecProvider@7e1762e6, com.mongodb.KotlinCodecProvider@5bccaedb]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33754], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:24:33 [cluster-ClusterId{value='66e718501a0c360337d10a86', description='null'}-localhost:33754] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33754, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=26720536, setName='docker-rs', canonicalAddress=57c64fa3ee3f:27017, hosts=[57c64fa3ee3f:27017], passives=[], arbiters=[], primary='57c64fa3ee3f:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71845025cbeaea30186dd, counter=6}, lastWriteDate=Sun Sep 15 21:24:22 GET 2024, lastUpdateTimeNanos=29390203625508}
2024-09-15 21:24:33 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33756]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:24:34 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33758]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:24:34 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33758]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421074269
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:24:34 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.478 seconds (process running for 14.718)
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: zqyJuytaT9mGxZpvhQQNAA
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33756 (id: 2147483646 rack: null)
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-c6b5eb36-e1d7-4ce3-9257-1ee15fdc5f17
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-c6b5eb36-e1d7-4ce3-9257-1ee15fdc5f17', protocol='range'}
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-c6b5eb36-e1d7-4ce3-9257-1ee15fdc5f17=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-c6b5eb36-e1d7-4ce3-9257-1ee15fdc5f17', protocol='range'}
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33756 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:24:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33756]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:24:34 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33758]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:24:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421074918
2024-09-15 21:24:34 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: zqyJuytaT9mGxZpvhQQNAA
2024-09-15 21:24:35 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:24:35 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:24:35 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "c3da5a80-5b53-4f28-bc1c-7509e288f500", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:24:34.889040559Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:24:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726421074925, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "c3da5a80-5b53-4f28-bc1c-7509e288f500", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:24:34.889040559Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:24:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:24:35 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:24:45 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:25:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:25:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:25:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:25:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:25:22 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:25:22 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 19 due to node 1 being disconnected (elapsed time since creation: 252ms, elapsed time since send: 252ms, request timeout: 30000ms)
2024-09-15 21:25:22 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight METADATA request with correlation id 20 due to node 1 being disconnected (elapsed time since creation: 178ms, elapsed time since send: 178ms, request timeout: 30000ms)
2024-09-15 21:25:22 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:25:22 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1260752130, epoch=2) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:25:22 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33756 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:25:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:25:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33756) could not be established. Node may not be available.
2024-09-15 21:25:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:25:22 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33756) could not be established. Node may not be available.
2024-09-15 21:25:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:25:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33756) could not be established. Node may not be available.
2024-09-15 21:25:22 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:25:22 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33756) could not be established. Node may not be available.
2024-09-15 21:25:23 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33755
2024-09-15 21:25:23 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33755]: Connection refused: localhost/127.0.0.1:33755
2024-09-15 21:25:23 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:25:23 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33756) could not be established. Node may not be available.
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33756) could not be established. Node may not be available.
2024-09-15 21:25:23 [cluster-ClusterId{value='66e718501a0c360337d10a86', description='null'}-localhost:33754] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33754
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:25:23 [cluster-ClusterId{value='66e718501a0c360337d10a86', description='null'}-localhost:33754] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33754
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:25:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:25:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:25:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:25:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:25:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:25:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:25:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:25:34 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:25:34 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:25:35 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 22451b56739b9bb0dfa8b178081d57e3eb0a2c44705845f844034adf4eb816ac
2024-09-15 21:25:35 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.804346539S
2024-09-15 21:25:35 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:25:35 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:25:35 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:25:35 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:25:35 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 0765e21d0c1d5118842a882b942815661b2ed08f4069c7aade647015c1171613
2024-09-15 21:25:36 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.809219267S
2024-09-15 21:25:37 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:25:37 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 7b532ac7f5e0ddcfaf4153e334e5af7e00ec91dcdf5dac6c998efb63c69500ae
2024-09-15 21:25:37 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.260097822S
2024-09-15 21:25:37 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:25:37 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: e2df5dbb2df2fab1006fbabb4358ec125726394c5ec1916b1bd3b96e31708cff
2024-09-15 21:25:40 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.195299206S
2024-09-15 21:25:40 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:25:40 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: ef7c887561756104304e5cba6e0ed6559a7c8312e8955277b76b3423db55484a
2024-09-15 21:25:41 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /thirsty_meninsky: Waiting for 60 seconds for URL: http://localhost:33764/subjects (where port 33764 maps to container port 8081)
2024-09-15 21:25:46 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.417404652S
2024-09-15 21:25:46 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:25:46 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 467724 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:25:46 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:25:46 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:25:46 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:25:46 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 118 ms. Found 1 MongoDB repository interface.
2024-09-15 21:25:47 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:25:47 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:25:47 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:25:47 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 21:25:47 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6866e740, com.mongodb.Jep395RecordCodecProvider@2cd5b19c, com.mongodb.KotlinCodecProvider@7109b603]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33760], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:25:47 [cluster-ClusterId{value='66e7189b9c5f1037852738a1', description='null'}-localhost:33760] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33760, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19926016, setName='docker-rs', canonicalAddress=0765e21d0c1d:27017, hosts=[0765e21d0c1d:27017], passives=[], arbiters=[], primary='0765e21d0c1d:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e7189032423fbb2bae0588, counter=6}, lastWriteDate=Sun Sep 15 21:25:37 GET 2024, lastUpdateTimeNanos=29464633491381}
2024-09-15 21:25:48 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33762]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:25:48 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:25:48 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33764]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:25:48 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33764]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:25:48 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:25:48 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:25:48 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:25:48 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421148723
2024-09-15 21:25:48 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:25:48 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.557 seconds (process running for 15.058)
2024-09-15 21:25:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:25:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: y1fN1fSGT8mdorRZ5yHZMQ
2024-09-15 21:25:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33762 (id: 2147483646 rack: null)
2024-09-15 21:25:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-7613336a-2cb1-4204-9f54-4d970428b31f
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-7613336a-2cb1-4204-9f54-4d970428b31f', protocol='range'}
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-7613336a-2cb1-4204-9f54-4d970428b31f=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-7613336a-2cb1-4204-9f54-4d970428b31f', protocol='range'}
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33762 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:25:49 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33762]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:25:49 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:25:49 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33764]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:25:49 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:25:49 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:25:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:25:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:25:49 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421149332
2024-09-15 21:25:49 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: y1fN1fSGT8mdorRZ5yHZMQ
2024-09-15 21:25:49 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:25:49 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:25:49 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "e06cc375-a060-4557-b60f-689dc405b473", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:25:49.300493511Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726421149339, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e06cc375-a060-4557-b60f-689dc405b473", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:25:49.300493511Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:25:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:26:01 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:26:02 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:26:05 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:26:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:26:05 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:26:05 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:26:05 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:26:05 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 24 due to node 1 being disconnected (elapsed time since creation: 227ms, elapsed time since send: 227ms, request timeout: 30000ms)
2024-09-15 21:26:05 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=603100953, epoch=8) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:26:05 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33762 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:26:06 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:26:06 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33762) could not be established. Node may not be available.
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33762) could not be established. Node may not be available.
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33762) could not be established. Node may not be available.
2024-09-15 21:26:06 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33761
2024-09-15 21:26:06 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33761]: Connection refused: localhost/127.0.0.1:33761
2024-09-15 21:26:06 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:26:06 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33762) could not be established. Node may not be available.
2024-09-15 21:26:06 [cluster-ClusterId{value='66e7189b9c5f1037852738a1', description='null'}-localhost:33760] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33760
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33762) could not be established. Node may not be available.
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:26:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:26:06 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:26:06 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33762) could not be established. Node may not be available.
2024-09-15 21:26:06 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:26:06 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:26:06 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:26:06 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:26:06 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:26:06 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:26:30 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:26:30 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:26:30 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:26:31 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:26:31 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:26:31 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:26:31 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:26:31 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:26:31 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:26:31 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 65fb611b86ec809e6d3c01d2a258ad972a067d4cd4127397beda030096e3a37a
2024-09-15 21:26:32 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.743486583S
2024-09-15 21:26:32 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:26:32 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:26:32 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:26:32 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:26:32 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 890ac74d83a299542023e57a03da877b921b7304039595b4e62b82c783fc7094
2024-09-15 21:26:32 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.773084781S
2024-09-15 21:26:33 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:26:33 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 22bbb0d250896d23d562415d67415d4fba46f367811383abcd77ab1191afeddc
2024-09-15 21:26:33 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.248168986S
2024-09-15 21:26:33 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:26:33 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: e0d924fd22ce427bd1a6dbbd17b476655a1af2229f484e3cc19bde22e6c3ed41
2024-09-15 21:26:37 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.14507577S
2024-09-15 21:26:37 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:26:37 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 2918f5d44e5e87067bd0d3993d526b2187d9ba9101c1d06344514faa478bb288
2024-09-15 21:26:37 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /friendly_einstein: Waiting for 60 seconds for URL: http://localhost:33770/subjects (where port 33770 maps to container port 8081)
2024-09-15 21:26:42 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.427414423S
2024-09-15 21:26:42 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:26:42 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 469507 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:26:42 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:26:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:26:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:26:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 88 ms. Found 1 MongoDB repository interface.
2024-09-15 21:26:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:26:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:26:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:26:43 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 8 ms. Found 0 Redis repository interfaces.
2024-09-15 21:26:43 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@38923cfe, com.mongodb.Jep395RecordCodecProvider@1ac3a6f, com.mongodb.KotlinCodecProvider@fee7ca]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33766], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:26:43 [cluster-ClusterId{value='66e718d3a4bf7a77527136cd', description='null'}-localhost:33766] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33766, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=13538099, setName='docker-rs', canonicalAddress=890ac74d83a2:27017, hosts=[890ac74d83a2:27017], passives=[], arbiters=[], primary='890ac74d83a2:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e718c8e6b480a962de8c57, counter=6}, lastWriteDate=Sun Sep 15 21:26:33 GET 2024, lastUpdateTimeNanos=29521036406858}
2024-09-15 21:26:44 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33768]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:26:44 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:26:44 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33770]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:26:44 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33770]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:26:44 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:26:44 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:26:44 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:26:44 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421204989
2024-09-15 21:26:44 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:26:45 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.46 seconds (process running for 14.727)
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: iODa766bSAqczYixQNwdDQ
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33768 (id: 2147483646 rack: null)
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-25c9fc04-9db2-4d50-adad-6867943bbd23
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-25c9fc04-9db2-4d50-adad-6867943bbd23', protocol='range'}
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-25c9fc04-9db2-4d50-adad-6867943bbd23=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-25c9fc04-9db2-4d50-adad-6867943bbd23', protocol='range'}
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33768 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:26:45 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33768]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:26:45 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:26:45 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33770]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:26:45 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:26:45 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:26:45 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:26:45 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:26:45 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421205648
2024-09-15 21:26:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: iODa766bSAqczYixQNwdDQ
2024-09-15 21:26:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:26:45 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:26:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "41c7ccb8-b7ff-4746-920a-d9832800d8b9", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:26:45.614773338Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726421205655, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "41c7ccb8-b7ff-4746-920a-d9832800d8b9", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:26:45.614773338Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:26:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:26:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:26:58 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:27:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:27:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 26 due to node 1 being disconnected (elapsed time since creation: 253ms, elapsed time since send: 253ms, request timeout: 30000ms)
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:27:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:27:09 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:27:09 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33768 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1337035457, epoch=9) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33768) could not be established. Node may not be available.
2024-09-15 21:27:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:27:09 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33768) could not be established. Node may not be available.
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:27:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33768) could not be established. Node may not be available.
2024-09-15 21:27:09 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33767
2024-09-15 21:27:09 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33767]: Connection refused: localhost/127.0.0.1:33767
2024-09-15 21:27:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:27:09 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33768) could not be established. Node may not be available.
2024-09-15 21:27:09 [cluster-ClusterId{value='66e718d3a4bf7a77527136cd', description='null'}-localhost:33766] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33766
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:27:09 [cluster-ClusterId{value='66e718d3a4bf7a77527136cd', description='null'}-localhost:33766] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33766
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:27:10 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:27:10 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33768) could not be established. Node may not be available.
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:27:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:27:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:27:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:27:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:27:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:27:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:27:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:27:11 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:27:11 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:27:12 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:27:12 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:27:12 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:27:12 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:27:12 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:27:12 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:27:12 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:27:13 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 3d0e052318e6cb4ed545371f3a0d8b1807ac4cbc2e77a9e53bc71691cb38a152
2024-09-15 21:27:13 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.768872886S
2024-09-15 21:27:13 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:27:13 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:27:13 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:27:13 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:27:13 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 2414e201a8e5d8abf235d832c58e7e4b95bf6a0a884fa5dcb98968f0b9a961b3
2024-09-15 21:27:14 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.803708154S
2024-09-15 21:27:15 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:27:15 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: a31125b10a4ad2e92b74bff879003d64ba6d8e355daae2c5cda874c376412cd2
2024-09-15 21:27:15 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.242823289S
2024-09-15 21:27:15 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:27:15 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: abc7054e9f81031d7916e5fa7b7c2bad5e958d9b57011aea4b155635f3e4b71c
2024-09-15 21:27:18 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.200945605S
2024-09-15 21:27:18 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:27:18 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: e1cc155297ddf4d7bf3fa5ca95e34888c7c7ffa580ac3065427a23ac5cf41ecc
2024-09-15 21:27:18 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /dreamy_golick: Waiting for 60 seconds for URL: http://localhost:33776/subjects (where port 33776 maps to container port 8081)
2024-09-15 21:27:23 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.424167177S
2024-09-15 21:27:24 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:27:24 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 471152 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:27:24 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:27:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:27:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:27:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 90 ms. Found 1 MongoDB repository interface.
2024-09-15 21:27:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:27:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:27:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:27:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 7 ms. Found 0 Redis repository interfaces.
2024-09-15 21:27:25 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@79273a4f, com.mongodb.Jep395RecordCodecProvider@4e26987b, com.mongodb.KotlinCodecProvider@50bb1c1f]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33772], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:27:25 [cluster-ClusterId{value='66e718fd4f8b64570a48d869', description='null'}-localhost:33772] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33772, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19630656, setName='docker-rs', canonicalAddress=2414e201a8e5:27017, hosts=[2414e201a8e5:27017], passives=[], arbiters=[], primary='2414e201a8e5:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e718f10676a7b7a02a9bf9, counter=6}, lastWriteDate=Sun Sep 15 21:27:15 GET 2024, lastUpdateTimeNanos=29562394392088}
2024-09-15 21:27:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33774]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:27:26 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:27:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33776]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:27:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33776]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:27:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:27:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:27:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:27:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421246398
2024-09-15 21:27:26 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:27:26 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.465 seconds (process running for 14.909)
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: ssB7lX50QFipKjb2ljhHmw
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33774 (id: 2147483646 rack: null)
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-d8182895-ae58-42c5-8558-35402dd7f977
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-d8182895-ae58-42c5-8558-35402dd7f977', protocol='range'}
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-d8182895-ae58-42c5-8558-35402dd7f977=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-d8182895-ae58-42c5-8558-35402dd7f977', protocol='range'}
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33774 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:27:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:27:27 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33774]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:27:27 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:27:27 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33776]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:27:27 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:27:27 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:27:27 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:27:27 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:27:27 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421247087
2024-09-15 21:27:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: ssB7lX50QFipKjb2ljhHmw
2024-09-15 21:27:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:27:27 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:27:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "9118ad28-076a-49cf-b9cb-beea50ee2a2b", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:27:27.049625636Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:27:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726421247094, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "9118ad28-076a-49cf-b9cb-beea50ee2a2b", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:27:27.049625636Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:27:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:27:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:27:34 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:28:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:28:32 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:28:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:28:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 19 due to node 1 being disconnected (elapsed time since creation: 225ms, elapsed time since send: 225ms, request timeout: 30000ms)
2024-09-15 21:28:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:28:32 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:28:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:28:32 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33774 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:28:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=691957982, epoch=3) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:28:32 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:28:32 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33774) could not be established. Node may not be available.
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33774) could not be established. Node may not be available.
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33774) could not be established. Node may not be available.
2024-09-15 21:28:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:28:33 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33774) could not be established. Node may not be available.
2024-09-15 21:28:33 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33773
2024-09-15 21:28:33 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33773]: Connection refused: localhost/127.0.0.1:33773
2024-09-15 21:28:33 [cluster-ClusterId{value='66e718fd4f8b64570a48d869', description='null'}-localhost:33772] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33772
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:28:33 [cluster-ClusterId{value='66e718fd4f8b64570a48d869', description='null'}-localhost:33772] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33772
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33774) could not be established. Node may not be available.
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:28:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:28:33 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33774) could not be established. Node may not be available.
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:28:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:28:33 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:28:33 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:28:33 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:28:33 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:28:33 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:28:33 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:28:42 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:28:42 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:28:43 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: c93128b1054124e3f486e0c05be7cac3c0cc0223bcbb7efbc54b8f538a1b25fe
2024-09-15 21:28:43 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.75127399S
2024-09-15 21:28:43 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:28:43 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:28:43 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:28:43 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:28:43 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 77ba72505d16fcd7165cd810aaf4358c1ec52632af3bb2710c0ac028b5510635
2024-09-15 21:28:44 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.814510168S
2024-09-15 21:28:45 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:28:45 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 066f18eec36cc5c3e20e933c134137857ac188a1482770b3c3d3ea0c92bf0bee
2024-09-15 21:28:45 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.278728323S
2024-09-15 21:28:45 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:28:45 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 5f3216a9f9f9c65179d8639ff883bd9a927ec07193797945343396afd0d66096
2024-09-15 21:28:48 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.200258509S
2024-09-15 21:28:48 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:28:48 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 39a9ca46b09e63098de4296c4a96f3eadd9d70538373d019761a98cd6d3f5cc3
2024-09-15 21:28:48 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /reverent_carver: Waiting for 60 seconds for URL: http://localhost:33782/subjects (where port 33782 maps to container port 8081)
2024-09-15 21:28:54 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.427697942S
2024-09-15 21:28:54 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:28:54 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 472915 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:28:54 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:28:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:28:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:28:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 78 ms. Found 1 MongoDB repository interface.
2024-09-15 21:28:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:28:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:28:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:28:54 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 6 ms. Found 0 Redis repository interfaces.
2024-09-15 21:28:55 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5602e540, com.mongodb.Jep395RecordCodecProvider@11f9b95a, com.mongodb.KotlinCodecProvider@42066f0d]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33778], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:28:55 [cluster-ClusterId{value='66e7195744c3d167c4ffbe2b', description='null'}-localhost:33778] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33778, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=18353036, setName='docker-rs', canonicalAddress=77ba72505d16:27017, hosts=[77ba72505d16:27017], passives=[], arbiters=[], primary='77ba72505d16:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e7194b7abb81569a1102b8, counter=6}, lastWriteDate=Sun Sep 15 21:28:45 GET 2024, lastUpdateTimeNanos=29652473349700}
2024-09-15 21:28:56 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33780]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:28:56 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:28:56 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33782]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:28:56 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33782]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:28:56 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:28:56 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:28:56 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:28:56 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421336496
2024-09-15 21:28:56 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:28:56 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.46 seconds (process running for 14.918)
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: LdzF7v-eSdOo24C7A-zqVw
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33780 (id: 2147483646 rack: null)
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-08935638-8132-405c-9810-4d819df5a0df
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-08935638-8132-405c-9810-4d819df5a0df', protocol='range'}
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-08935638-8132-405c-9810-4d819df5a0df=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-08935638-8132-405c-9810-4d819df5a0df', protocol='range'}
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33780 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:28:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:28:57 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33780]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:28:57 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:28:57 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33782]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:28:57 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:28:57 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:28:57 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:28:57 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:28:57 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421337113
2024-09-15 21:28:57 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: LdzF7v-eSdOo24C7A-zqVw
2024-09-15 21:28:57 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:28:57 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:28:57 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "162aa3ea-3b8b-4b4e-9f39-77296141f0da", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:28:57.083464764Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:28:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726421337121, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "162aa3ea-3b8b-4b4e-9f39-77296141f0da", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:28:57.083464764Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:28:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:28:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:29:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33780 (id: 2147483646 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 21:29:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:33780 (id: 2147483646 rack: null)
2024-09-15 21:29:49 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483646
2024-09-15 21:30:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:30:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33780 (id: 2147483646 rack: null)
2024-09-15 21:30:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33780 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 21:30:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:33780 (id: 2147483646 rack: null)
2024-09-15 21:30:06 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:30:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33780 (id: 2147483646 rack: null)
2024-09-15 21:31:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33780 (id: 2147483646 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response. isDisconnected: false. Rediscovery will be attempted.
2024-09-15 21:31:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Requesting disconnect from last known coordinator localhost:33780 (id: 2147483646 rack: null)
2024-09-15 21:31:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Client requested disconnect from node 2147483646
2024-09-15 21:31:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight HEARTBEAT request with correlation id 17 due to node 2147483646 being disconnected (elapsed time since creation: 51682ms, elapsed time since send: 23710ms, request timeout: 30000ms)
2024-09-15 21:31:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight HEARTBEAT request with correlation id 18 due to node 2147483646 being disconnected (elapsed time since creation: 23710ms, elapsed time since send: 1ms, request timeout: 30000ms)
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Disconnecting from node 1 due to request timeout.
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FIND_COORDINATOR request with correlation id 19 due to node 1 being disconnected (elapsed time since creation: 4ms, elapsed time since send: 51688ms, request timeout: 30000ms)
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [1;31mERROR[0;39m o.s.k.l.KafkaMessageListenerContainer - Consumer exception
java.lang.IllegalStateException: This error handler cannot process 'org.apache.kafka.common.errors.TimeoutException's; no record information is available
	at org.springframework.kafka.listener.DefaultErrorHandler.handleOtherException(DefaultErrorHandler.java:198)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.handleConsumerException(KafkaMessageListenerContainer.java:1925)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1348)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before successfully committing offsets {CustomerViewEventTopic-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}}
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:31:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:31:10 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:31:10 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:31:10 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:31:10 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:31:10 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:31:11 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:31:11 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:31:11 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:31:11 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:31:11 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 8edb4fd7792d73165193d2b4738cb505374bce8bedbd7a7ef6e904e9803d8899
2024-09-15 21:31:11 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.823679801S
2024-09-15 21:31:11 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:31:11 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:31:11 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:31:11 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:31:12 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 80754124e93aa22d67a0f30046cb62cdd990cd1dce632f355b2a397eb8e56109
2024-09-15 21:31:12 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.756542569S
2024-09-15 21:31:13 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:31:13 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 016d6416495429f857777bf225e0f17e19544115c2dce99384d585aebc44aac0
2024-09-15 21:31:13 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.361095749S
2024-09-15 21:31:13 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:31:13 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: e52d5b483ad4e498fb96b8299d7ba87f6df37578755364d160add12634d7688c
2024-09-15 21:31:17 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.525644777S
2024-09-15 21:31:17 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:31:17 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: b57481380b2fe0c1794d4a3990787e048779e95631296132b3eae7222536da4c
2024-09-15 21:31:17 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /tender_kirch: Waiting for 60 seconds for URL: http://localhost:33788/subjects (where port 33788 maps to container port 8081)
2024-09-15 21:31:23 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT6.449874744S
2024-09-15 21:31:24 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:31:24 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 478901 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:31:24 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:31:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:31:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:31:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 84 ms. Found 1 MongoDB repository interface.
2024-09-15 21:31:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:31:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:31:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:31:24 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 14 ms. Found 0 Redis repository interfaces.
2024-09-15 21:31:25 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@14ad42, com.mongodb.Jep395RecordCodecProvider@608b906d, com.mongodb.KotlinCodecProvider@173cfb01]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33784], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:31:25 [cluster-ClusterId{value='66e719ed77d709568cfe6e45', description='null'}-localhost:33784] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33784, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=16992443, setName='docker-rs', canonicalAddress=80754124e93a:27017, hosts=[80754124e93a:27017], passives=[], arbiters=[], primary='80754124e93a:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e719e00e740d325dea92ac, counter=6}, lastWriteDate=Sun Sep 15 21:31:13 GET 2024, lastUpdateTimeNanos=29802317444861}
2024-09-15 21:31:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33786]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:31:26 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:31:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33788]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:31:26 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33788]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:31:26 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:31:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:31:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:31:26 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421486346
2024-09-15 21:31:26 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:31:26 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.513 seconds (process running for 16.707)
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: BDzAmTSHR8-19N0pMuqT_g
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33786 (id: 2147483646 rack: null)
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-c50ba821-5aa9-48bb-9480-a05b8d56620b
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-c50ba821-5aa9-48bb-9480-a05b8d56620b', protocol='range'}
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-c50ba821-5aa9-48bb-9480-a05b8d56620b=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-c50ba821-5aa9-48bb-9480-a05b8d56620b', protocol='range'}
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33786 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:31:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:31:27 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33786]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:31:27 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:31:27 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33788]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:31:27 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:31:27 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:31:27 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:31:27 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:31:27 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726421487036
2024-09-15 21:31:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: BDzAmTSHR8-19N0pMuqT_g
2024-09-15 21:31:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:31:27 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:31:27 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "491dc047-9da0-48b6-8f69-5ae1baeb7fe5", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:31:26.999402833Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:31:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726421487042, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "491dc047-9da0-48b6-8f69-5ae1baeb7fe5", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:31:26.999402833Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:31:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:31:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:31:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:31:30 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 21 due to node 1 being disconnected (elapsed time since creation: 238ms, elapsed time since send: 238ms, request timeout: 30000ms)
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:31:38 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:31:38 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33786 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1494000380, epoch=6) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33786) could not be established. Node may not be available.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33786) could not be established. Node may not be available.
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33786) could not be established. Node may not be available.
2024-09-15 21:31:38 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33785
2024-09-15 21:31:38 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33785]: Connection refused: localhost/127.0.0.1:33785
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33786) could not be established. Node may not be available.
2024-09-15 21:31:38 [cluster-ClusterId{value='66e719ed77d709568cfe6e45', description='null'}-localhost:33784] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33784
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:31:38 [cluster-ClusterId{value='66e719ed77d709568cfe6e45', description='null'}-localhost:33784] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33784
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:31:38 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33786) could not be established. Node may not be available.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33786) could not be established. Node may not be available.
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:31:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:31:38 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:31:38 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:31:38 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:31:38 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:31:38 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:31:38 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:41:02 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:41:02 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:41:03 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 37c34e55fb409653df7688494d7d87f2b1d64300dff794b6c511c2fddaf50a02
2024-09-15 21:41:03 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.770096429S
2024-09-15 21:41:03 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:41:03 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:41:03 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:41:03 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:41:03 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 2bab95f4c0b3f966d1bff17a42284aed09ea7645f347db48de1685368852b46b
2024-09-15 21:41:04 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.796818787S
2024-09-15 21:41:05 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:41:05 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: fc85bece271a6b7c4355ff1edab9f916702b8ab0fa5afb72abbb025dd4fb5e7f
2024-09-15 21:41:05 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.27154701S
2024-09-15 21:41:05 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:41:05 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: ed027263762086769edb2d3e03e6536a49962cf6e597af976aced1d47fd603db
2024-09-15 21:41:08 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.149326261S
2024-09-15 21:41:08 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:41:08 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 5b287fafaeeb712dce01cee97b5f4a0b6331a8f02b228cd2e0136d8943094bda
2024-09-15 21:41:08 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /distracted_goldstine: Waiting for 60 seconds for URL: http://localhost:33794/subjects (where port 33794 maps to container port 8081)
2024-09-15 21:41:14 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT6.441849443S
2024-09-15 21:41:15 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:41:15 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 501005 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:41:15 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:41:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:41:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:41:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 67 ms. Found 1 MongoDB repository interface.
2024-09-15 21:41:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:41:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:41:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:41:15 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 21:41:16 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@52290e63, com.mongodb.Jep395RecordCodecProvider@6c2dd88b, com.mongodb.KotlinCodecProvider@49d979c4]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33790], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:41:16 [cluster-ClusterId{value='66e71c3c8374551d928651bf', description='null'}-localhost:33790] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33790, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=12701430, setName='docker-rs', canonicalAddress=2bab95f4c0b3:27017, hosts=[2bab95f4c0b3:27017], passives=[], arbiters=[], primary='2bab95f4c0b3:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71c2f6adc9da4caceeb5b, counter=6}, lastWriteDate=Sun Sep 15 21:41:05 GET 2024, lastUpdateTimeNanos=30393247248142}
2024-09-15 21:41:16 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33792]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:41:16 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:41:16 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33794]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:41:16 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33794]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422077086
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:41:17 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.094 seconds (process running for 15.5)
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: 7RYEiAyBSUelTZLVtw9HzQ
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33792 (id: 2147483646 rack: null)
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-5385f190-a58d-4e1c-9933-9da8cf48d349
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-5385f190-a58d-4e1c-9933-9da8cf48d349', protocol='range'}
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-5385f190-a58d-4e1c-9933-9da8cf48d349=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-5385f190-a58d-4e1c-9933-9da8cf48d349', protocol='range'}
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33792 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33792]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:41:17 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33794]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:41:17 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422077639
2024-09-15 21:41:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: 7RYEiAyBSUelTZLVtw9HzQ
2024-09-15 21:41:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:41:17 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:41:17 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "e45f9cfe-2675-472b-9f82-0cbae37b1a2d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:41:17.613241848Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:41:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422077646, serialized key size = -1, serialized value size = 117, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e45f9cfe-2675-472b-9f82-0cbae37b1a2d", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:41:17.613241848Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:41:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:41:38 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:41:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 17 due to node 1 being disconnected (elapsed time since creation: 140ms, elapsed time since send: 140ms, request timeout: 30000ms)
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:41:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:41:38 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33792 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1881475001, epoch=2) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33792) could not be established. Node may not be available.
2024-09-15 21:41:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:41:38 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33792) could not be established. Node may not be available.
2024-09-15 21:41:38 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:41:38 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33792) could not be established. Node may not be available.
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:41:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33792) could not be established. Node may not be available.
2024-09-15 21:41:38 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33791
2024-09-15 21:41:38 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33791]: Connection refused: localhost/127.0.0.1:33791
2024-09-15 21:41:38 [cluster-ClusterId{value='66e71c3c8374551d928651bf', description='null'}-localhost:33790] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33790
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:41:39 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:41:39 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33792) could not be established. Node may not be available.
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33792) could not be established. Node may not be available.
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:41:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:41:39 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:41:39 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:41:39 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:41:39 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:41:39 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:41:39 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:45:54 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:45:54 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:45:54 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:45:54 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:45:54 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:45:55 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:45:55 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:45:55 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:45:55 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:45:55 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: cd1497c4d37b5769c753c3090ef8c3f06ba95146058b8e3379d2fffc5c205391
2024-09-15 21:45:55 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.810748458S
2024-09-15 21:45:55 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:45:55 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:45:55 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:45:55 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:45:56 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 60f535ac9e84b45e1d070e687ab4fdb6d3bb2662b7586cb04d3afb666eb5ca00
2024-09-15 21:45:56 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.778741398S
2024-09-15 21:45:57 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:45:57 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: ed734a6d0a46d3cd93432de1e5ad9b1b63c011674d784a15355768a751d76ff5
2024-09-15 21:45:57 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.294100524S
2024-09-15 21:45:57 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:45:57 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 5c7b6dd061388251e8092b3b7e0d05b4aac92f2bb071760963e297316057f639
2024-09-15 21:46:01 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.487017054S
2024-09-15 21:46:01 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:46:01 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: ea33a78749bb93b1d4206fc88ff7838a504fc7f8da66830aec2e70e3fa1b4756
2024-09-15 21:46:01 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /compassionate_mirzakhani: Waiting for 60 seconds for URL: http://localhost:33800/subjects (where port 33800 maps to container port 8081)
2024-09-15 21:46:06 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.4613872S
2024-09-15 21:46:06 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:46:06 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 503206 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:46:06 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:46:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:46:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:46:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 60 ms. Found 1 MongoDB repository interface.
2024-09-15 21:46:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:46:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:46:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:46:07 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 21:46:07 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@4a41caed, com.mongodb.Jep395RecordCodecProvider@6b4fd7d, com.mongodb.KotlinCodecProvider@7404aff2]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33796], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:46:07 [cluster-ClusterId{value='66e71d5f22ac647f71ee983d', description='null'}-localhost:33796] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33796, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=16160947, setName='docker-rs', canonicalAddress=60f535ac9e84:27017, hosts=[60f535ac9e84:27017], passives=[], arbiters=[], primary='60f535ac9e84:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71d54485290f4a7b1f79d, counter=6}, lastWriteDate=Sun Sep 15 21:45:57 GET 2024, lastUpdateTimeNanos=30684862049854}
2024-09-15 21:46:08 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33798]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:46:08 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:46:08 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33800]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:46:08 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33800]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:46:08 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:46:08 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:46:08 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:46:08 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422368543
2024-09-15 21:46:08 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:46:08 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 1.789 seconds (process running for 14.545)
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: TLxKiCUVRWqP2I3IA3Ovlw
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33798 (id: 2147483646 rack: null)
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-4439a313-d837-4933-9998-25dffcafc9a0
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-4439a313-d837-4933-9998-25dffcafc9a0', protocol='range'}
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-4439a313-d837-4933-9998-25dffcafc9a0=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-4439a313-d837-4933-9998-25dffcafc9a0', protocol='range'}
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33798 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:46:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:46:08 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:46:09 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:46:09 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33798]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:46:09 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:46:09 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33800]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:46:09 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:46:09 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:46:09 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:46:09 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:46:09 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422369270
2024-09-15 21:46:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: TLxKiCUVRWqP2I3IA3Ovlw
2024-09-15 21:46:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:46:09 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:46:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "d644c4a2-2ae0-4f9f-bbdd-92d6b15f215c", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:46:09.245770706Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:46:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422369275, serialized key size = -1, serialized value size = 116, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "d644c4a2-2ae0-4f9f-bbdd-92d6b15f215c", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:46:09.245770706Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:46:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:46:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Tom
2024-09-15 21:46:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Tom, address=New York]
2024-09-15 21:46:12 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:46:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:46:12 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:46:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 22 due to node 1 being disconnected (elapsed time since creation: 258ms, elapsed time since send: 258ms, request timeout: 30000ms)
2024-09-15 21:46:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:46:12 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:46:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:46:12 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33798 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:46:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1009542128, epoch=8) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:46:12 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:46:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:46:12 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33798) could not be established. Node may not be available.
2024-09-15 21:46:12 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33798) could not be established. Node may not be available.
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33798) could not be established. Node may not be available.
2024-09-15 21:46:13 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:46:13 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33798) could not be established. Node may not be available.
2024-09-15 21:46:13 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33797
2024-09-15 21:46:13 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33797]: Connection refused: localhost/127.0.0.1:33797
2024-09-15 21:46:13 [cluster-ClusterId{value='66e71d5f22ac647f71ee983d', description='null'}-localhost:33796] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33796
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:46:13 [cluster-ClusterId{value='66e71d5f22ac647f71ee983d', description='null'}-localhost:33796] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33796
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:46:13 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:46:13 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33798) could not be established. Node may not be available.
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:46:13 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:46:13 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:46:13 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33798) could not be established. Node may not be available.
2024-09-15 21:46:13 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:46:13 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:46:13 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:46:13 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:46:13 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:46:13 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:46:20 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:46:20 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:46:20 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:46:20 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:46:20 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:46:20 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:46:20 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:46:21 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:46:21 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:46:21 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 2b82ea1c7ddcfbae91deac497bcc80067d54e7eeeb2bfd7107c82a48c5a429dd
2024-09-15 21:46:21 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.786914379S
2024-09-15 21:46:21 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:46:21 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:46:21 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:46:21 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:46:21 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 76a9e54c84f4a4ec0c93bbddbbba9745ca61a816969948adea0ea5fbe360828a
2024-09-15 21:46:22 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.785394758S
2024-09-15 21:46:23 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:46:23 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 70b420c1af64396b591e47d0eb685d1305acc3d34b71ac989fc55e8ad48c7a5c
2024-09-15 21:46:23 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.244052575S
2024-09-15 21:46:23 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:46:23 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 4d3ea7d4dcca5baafd3e0fa7ee17445f20384849857e23dba706ab89df1e7777
2024-09-15 21:46:26 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.146001809S
2024-09-15 21:46:26 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:46:26 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 6c46af8febb79a935f44999cd37f8d9ede88dee0148edf5a1f57ee8267535ae6
2024-09-15 21:46:27 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /competent_gates: Waiting for 60 seconds for URL: http://localhost:33806/subjects (where port 33806 maps to container port 8081)
2024-09-15 21:46:32 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.419366153S
2024-09-15 21:46:32 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:46:32 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 504841 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:46:32 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:46:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:46:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:46:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 82 ms. Found 1 MongoDB repository interface.
2024-09-15 21:46:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:46:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:46:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:46:32 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 12 ms. Found 0 Redis repository interfaces.
2024-09-15 21:46:33 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@67784537, com.mongodb.Jep395RecordCodecProvider@17ec5e2a, com.mongodb.KotlinCodecProvider@52290e63]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33802], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:46:33 [cluster-ClusterId{value='66e71d79809ec32483cd3bf5', description='null'}-localhost:33802] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33802, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=23785894, setName='docker-rs', canonicalAddress=76a9e54c84f4:27017, hosts=[76a9e54c84f4:27017], passives=[], arbiters=[], primary='76a9e54c84f4:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71d6e36f39ce53a9ef581, counter=6}, lastWriteDate=Sun Sep 15 21:46:23 GET 2024, lastUpdateTimeNanos=30710548029661}
2024-09-15 21:46:34 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33804]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:46:34 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:46:34 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33806]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:46:34 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33806]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:46:34 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:46:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:46:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:46:34 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422394520
2024-09-15 21:46:34 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:46:34 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.324 seconds (process running for 14.74)
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: Xt4LyoLERFm4AW3D4c3wwQ
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33804 (id: 2147483646 rack: null)
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-1f86ee2f-4b06-4f0b-81ca-fa03e7808f68
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-1f86ee2f-4b06-4f0b-81ca-fa03e7808f68', protocol='range'}
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-1f86ee2f-4b06-4f0b-81ca-fa03e7808f68=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-1f86ee2f-4b06-4f0b-81ca-fa03e7808f68', protocol='range'}
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33804 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:46:35 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:46:45 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:46:45 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33804]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:46:45 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:46:45 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33806]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:46:45 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:46:45 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:46:45 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:46:45 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:46:45 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422405268
2024-09-15 21:46:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: Xt4LyoLERFm4AW3D4c3wwQ
2024-09-15 21:46:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:46:45 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:46:45 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "240a3f57-3042-496b-8b63-1d45c3b9b8c0", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:46:45.235866427Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:46:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422405273, serialized key size = -1, serialized value size = 116, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "240a3f57-3042-496b-8b63-1d45c3b9b8c0", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:46:45.235866427Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:46:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:46:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Tom
2024-09-15 21:46:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Tom, address=New York]
2024-09-15 21:46:49 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:47:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 26 due to node 1 being disconnected (elapsed time since creation: 260ms, elapsed time since send: 260ms, request timeout: 30000ms)
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:47:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:47:09 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33804 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=278400677, epoch=9) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:47:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:47:09 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33804) could not be established. Node may not be available.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33804) could not be established. Node may not be available.
2024-09-15 21:47:09 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33803
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33804) could not be established. Node may not be available.
2024-09-15 21:47:09 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33803]: Connection refused: localhost/127.0.0.1:33803
2024-09-15 21:47:09 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:47:09 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33804) could not be established. Node may not be available.
2024-09-15 21:47:09 [cluster-ClusterId{value='66e71d79809ec32483cd3bf5', description='null'}-localhost:33802] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33802
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:47:09 [cluster-ClusterId{value='66e71d79809ec32483cd3bf5', description='null'}-localhost:33802] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33802
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:47:09 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33804) could not be established. Node may not be available.
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:47:10 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:47:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:47:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:47:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:47:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:47:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:47:10 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:48:04 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:48:04 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:48:05 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 04d19e69661b3dff48c9c04edf5ee56affc331ceba8a19a2ec137793207cc122
2024-09-15 21:48:05 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.724843852S
2024-09-15 21:48:05 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:48:05 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:48:05 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:48:05 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:48:05 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 4d44992997ae3beb2beffe4d0d055e30d2957e1e989ee8ee345b8ab563c220a1
2024-09-15 21:48:06 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.774694024S
2024-09-15 21:48:07 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:48:07 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: f45e642e03be4a3eb34ace90cc16379d72349aa214c791f428da8d1c1ddd9233
2024-09-15 21:48:07 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.2689658S
2024-09-15 21:48:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:48:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 63b085a505d4f752bd7af9e38009fca9027fc1410d06680f55f9d84875cd3564
2024-09-15 21:48:10 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.202496483S
2024-09-15 21:48:10 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:48:10 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 3c236b79a158f990fb91fce036b5a85e2de20fe828fd544ba079992b95de2940
2024-09-15 21:48:10 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /hopeful_perlman: Waiting for 60 seconds for URL: http://localhost:33812/subjects (where port 33812 maps to container port 8081)
2024-09-15 21:48:17 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT6.471901439S
2024-09-15 21:48:17 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:48:17 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 506681 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:48:17 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:48:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:48:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:48:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 64 ms. Found 1 MongoDB repository interface.
2024-09-15 21:48:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:48:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:48:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:48:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 21:48:18 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6b4fd7d, com.mongodb.Jep395RecordCodecProvider@7404aff2, com.mongodb.KotlinCodecProvider@12723c5d]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33808], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:48:18 [cluster-ClusterId{value='66e71de26ebeec7b93b6647b', description='null'}-localhost:33808] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33808, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=15284667, setName='docker-rs', canonicalAddress=4d44992997ae:27017, hosts=[4d44992997ae:27017], passives=[], arbiters=[], primary='4d44992997ae:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71dd5982adfbd6548d186, counter=6}, lastWriteDate=Sun Sep 15 21:48:07 GET 2024, lastUpdateTimeNanos=30815230531910}
2024-09-15 21:48:18 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33810]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:48:18 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:48:18 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33812]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:48:18 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33812]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:48:18 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:48:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:48:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:48:18 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422498932
2024-09-15 21:48:18 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:48:18 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 1.909 seconds (process running for 15.179)
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: 3pdouh2RRmas81kDDlIeSA
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33810 (id: 2147483646 rack: null)
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-01d38650-d9b4-4bb1-b863-b9ad1e6967bd
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-01d38650-d9b4-4bb1-b863-b9ad1e6967bd', protocol='range'}
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-01d38650-d9b4-4bb1-b863-b9ad1e6967bd=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-01d38650-d9b4-4bb1-b863-b9ad1e6967bd', protocol='range'}
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33810 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:48:19 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:48:19 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:48:19 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33810]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:48:19 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:48:19 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33812]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:48:19 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:48:19 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:48:19 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:48:19 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:48:19 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422499699
2024-09-15 21:48:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: 3pdouh2RRmas81kDDlIeSA
2024-09-15 21:48:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:48:19 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:48:19 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "f2d76d1c-958b-4924-bdfb-0a88eee316f1", "name": "Alex", "address": "London", "timestamp": "2024-09-15T17:48:19.673508363Z", "eventType": "UpdateCustomerAddressViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422499704, serialized key size = -1, serialized value size = 118, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "f2d76d1c-958b-4924-bdfb-0a88eee316f1", "name": "Alex", "address": "London", "timestamp": "2024-09-15T17:48:19.673508363Z", "eventType": "UpdateCustomerAddressViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:48:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=London]
2024-09-15 21:48:22 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 22 due to node 1 being disconnected (elapsed time since creation: 225ms, elapsed time since send: 225ms, request timeout: 30000ms)
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:48:23 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:48:23 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33810 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1744989781, epoch=8) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33810) could not be established. Node may not be available.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33810) could not be established. Node may not be available.
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33810) could not be established. Node may not be available.
2024-09-15 21:48:23 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33809
2024-09-15 21:48:23 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33809]: Connection refused: localhost/127.0.0.1:33809
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33810) could not be established. Node may not be available.
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:48:23 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33810) could not be established. Node may not be available.
2024-09-15 21:48:23 [cluster-ClusterId{value='66e71de26ebeec7b93b6647b', description='null'}-localhost:33808] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33808
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:48:23 [cluster-ClusterId{value='66e71de26ebeec7b93b6647b', description='null'}-localhost:33808] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33808
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33810) could not be established. Node may not be available.
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:48:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:48:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:48:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:48:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:48:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:48:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:48:23 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:48:27 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:48:27 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:48:28 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 84a7caf38cda49858b9241f3016d7e80401d615312e55c8a06e84d7f02798b0e
2024-09-15 21:48:28 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.776740225S
2024-09-15 21:48:28 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:48:28 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:48:28 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:48:28 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:48:28 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: c66a8733bc546098fdae817c7af025770a66d8b3fbcd62442e59004acfd02903
2024-09-15 21:48:29 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.789789083S
2024-09-15 21:48:30 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:48:30 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: c3ab95ab402ec7c13c1ef721ed2283d08e5b96240f59afc07c33e5b6dd44efd2
2024-09-15 21:48:30 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.261476583S
2024-09-15 21:48:30 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:48:30 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 3aada0c14bacac513fce4a274bc2243a7f398a88cc566eb7cd7a7b39c9bed3a2
2024-09-15 21:48:33 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.182886682S
2024-09-15 21:48:33 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:48:33 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: ceecfa998752a2d8023bc8dc21ae785f997a218dc69a4c0ec11290b2ccfefe3f
2024-09-15 21:48:33 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /eloquent_hypatia: Waiting for 60 seconds for URL: http://localhost:33818/subjects (where port 33818 maps to container port 8081)
2024-09-15 21:48:39 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.435136001S
2024-09-15 21:48:39 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:48:39 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 508268 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:48:39 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:48:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:48:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:48:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 79 ms. Found 1 MongoDB repository interface.
2024-09-15 21:48:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:48:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:48:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:48:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 21:48:40 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6c2dd88b, com.mongodb.Jep395RecordCodecProvider@49d979c4, com.mongodb.KotlinCodecProvider@3cb173db]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33814], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:48:40 [cluster-ClusterId{value='66e71df8d67d7d21cb97931a', description='null'}-localhost:33814] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33814, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=18957651, setName='docker-rs', canonicalAddress=c66a8733bc54:27017, hosts=[c66a8733bc54:27017], passives=[], arbiters=[], primary='c66a8733bc54:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71dec24a670d7fb773b24, counter=6}, lastWriteDate=Sun Sep 15 21:48:30 GET 2024, lastUpdateTimeNanos=30837382075272}
2024-09-15 21:48:41 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33816]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:48:41 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:48:41 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33818]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:48:41 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33818]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:48:41 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:48:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:48:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:48:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422521338
2024-09-15 21:48:41 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:48:41 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.259 seconds (process running for 14.746)
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: ASfpyk4sSzqCKqA94Y8N0Q
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33816 (id: 2147483646 rack: null)
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-dfe7a0d3-3b7f-419e-a2c2-ac56e29d0b85
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-dfe7a0d3-3b7f-419e-a2c2-ac56e29d0b85', protocol='range'}
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-dfe7a0d3-3b7f-419e-a2c2-ac56e29d0b85=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-dfe7a0d3-3b7f-419e-a2c2-ac56e29d0b85', protocol='range'}
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33816 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:48:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:48:41 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:48:42 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:48:42 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33816]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:48:42 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:48:42 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33818]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:48:42 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:48:42 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:48:42 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:48:42 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:48:42 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422522327
2024-09-15 21:48:42 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: ASfpyk4sSzqCKqA94Y8N0Q
2024-09-15 21:48:42 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:48:42 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:48:42 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "eabac0a4-6bf6-4b99-a099-1b9bdadb0f43", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:48:42.297407529Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:48:42 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422522334, serialized key size = -1, serialized value size = 116, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "eabac0a4-6bf6-4b99-a099-1b9bdadb0f43", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:48:42.297407529Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:48:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:48:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Tom
2024-09-15 21:48:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Tom, address=New York]
2024-09-15 21:48:50 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:49:07 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 25 due to node 1 being disconnected (elapsed time since creation: 288ms, elapsed time since send: 288ms, request timeout: 30000ms)
2024-09-15 21:49:07 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:49:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:49:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:49:07 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33816 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:49:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=750602370, epoch=8) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:49:08 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:08 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33816) could not be established. Node may not be available.
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33816) could not be established. Node may not be available.
2024-09-15 21:49:08 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:08 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33816) could not be established. Node may not be available.
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33816) could not be established. Node may not be available.
2024-09-15 21:49:08 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33815
2024-09-15 21:49:08 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33815]: Connection refused: localhost/127.0.0.1:33815
2024-09-15 21:49:08 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:08 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33816) could not be established. Node may not be available.
2024-09-15 21:49:08 [cluster-ClusterId{value='66e71df8d67d7d21cb97931a', description='null'}-localhost:33814] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33814
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:49:08 [cluster-ClusterId{value='66e71df8d67d7d21cb97931a', description='null'}-localhost:33814] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33814
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33816) could not be established. Node may not be available.
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:49:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:49:08 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:49:08 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:49:08 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:49:08 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:49:08 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:49:08 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:49:15 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:49:15 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:49:15 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:49:15 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:49:16 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:49:16 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:49:16 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:49:16 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:49:16 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:49:16 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: bad9a22bbb3263a72bf8b178cd712f16736e1e02b276b70a18d5befce36f0494
2024-09-15 21:49:17 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.802949814S
2024-09-15 21:49:17 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:49:17 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:49:17 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:49:17 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:49:17 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 27dea71f0370f346747778e6dfc4f93bdd133f660ac8d5c4bfbf154a26bce198
2024-09-15 21:49:17 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.777828885S
2024-09-15 21:49:18 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:49:18 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 9a7bf3a33270b686dda532fdf7c9bf257c402f72e91008b2e53e182918102dbc
2024-09-15 21:49:18 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.278392966S
2024-09-15 21:49:18 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:49:19 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 37ba1f0204cd02b00078fd27716df41025279596eda34fc72de8ae039643558b
2024-09-15 21:49:22 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.259409918S
2024-09-15 21:49:22 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:49:22 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: f17df4a8c317bf879bc63ebc651cd80d7faaad9f5ee5984a1b488cabe2db8cba
2024-09-15 21:49:22 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /elated_jones: Waiting for 60 seconds for URL: http://localhost:33824/subjects (where port 33824 maps to container port 8081)
2024-09-15 21:49:27 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.436343887S
2024-09-15 21:49:27 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:49:27 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 509920 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:49:27 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:49:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:49:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:49:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 65 ms. Found 1 MongoDB repository interface.
2024-09-15 21:49:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:49:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:49:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:49:28 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 21:49:28 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@26a9c6df, com.mongodb.Jep395RecordCodecProvider@2ce24a1a, com.mongodb.KotlinCodecProvider@26bce60d]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33820], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:49:28 [cluster-ClusterId{value='66e71e283b94f70219d5233f', description='null'}-localhost:33820] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33820, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=13388235, setName='docker-rs', canonicalAddress=27dea71f0370:27017, hosts=[27dea71f0370:27017], passives=[], arbiters=[], primary='27dea71f0370:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71e1d1d4ff62237db7a94, counter=6}, lastWriteDate=Sun Sep 15 21:49:18 GET 2024, lastUpdateTimeNanos=30885817214602}
2024-09-15 21:49:29 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33822]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:49:29 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:49:29 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33824]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:49:29 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33824]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:49:29 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:49:29 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:49:29 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:49:29 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422569544
2024-09-15 21:49:29 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:49:29 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 1.854 seconds (process running for 14.391)
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: B0m801wuSXWjx-Ouz_Gcbg
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33822 (id: 2147483646 rack: null)
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-8685d4ae-922a-4f6b-bbc3-a5e4f8e8e9cf
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-8685d4ae-922a-4f6b-bbc3-a5e4f8e8e9cf', protocol='range'}
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-8685d4ae-922a-4f6b-bbc3-a5e4f8e8e9cf=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-8685d4ae-922a-4f6b-bbc3-a5e4f8e8e9cf', protocol='range'}
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33822 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:49:29 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:49:30 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:49:30 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:49:30 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33822]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:49:30 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:49:30 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33824]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:49:30 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:49:30 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:49:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:49:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:49:30 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422570313
2024-09-15 21:49:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: B0m801wuSXWjx-Ouz_Gcbg
2024-09-15 21:49:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:49:30 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:49:30 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "23362c95-f2f0-4f38-9ca6-347cbf79d917", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:49:30.285327261Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:49:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422570320, serialized key size = -1, serialized value size = 116, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "23362c95-f2f0-4f38-9ca6-347cbf79d917", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:49:30.285327261Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:49:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:49:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Tom
2024-09-15 21:49:30 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Tom, address=New York]
2024-09-15 21:49:33 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:49:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 22 due to node 1 being disconnected (elapsed time since creation: 237ms, elapsed time since send: 237ms, request timeout: 30000ms)
2024-09-15 21:49:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:49:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:49:33 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:49:33 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33822 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:49:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1851890658, epoch=8) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:49:33 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:33 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33822) could not be established. Node may not be available.
2024-09-15 21:49:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33822) could not be established. Node may not be available.
2024-09-15 21:49:34 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33821
2024-09-15 21:49:34 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33821]: Connection refused: localhost/127.0.0.1:33821
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33822) could not be established. Node may not be available.
2024-09-15 21:49:34 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:34 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33822) could not be established. Node may not be available.
2024-09-15 21:49:34 [cluster-ClusterId{value='66e71e283b94f70219d5233f', description='null'}-localhost:33820] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33820
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:49:34 [cluster-ClusterId{value='66e71e283b94f70219d5233f', description='null'}-localhost:33820] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33820
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33822) could not be established. Node may not be available.
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:49:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:49:34 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:49:34 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33822) could not be established. Node may not be available.
2024-09-15 21:49:34 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:49:34 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:49:34 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:49:34 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:49:34 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:49:34 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:49:39 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:49:39 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:49:40 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: c5df7ff664f56c9b360c30be2ce9883cf335243207ae43979cefb2b4c4dcccf1
2024-09-15 21:49:40 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.755927627S
2024-09-15 21:49:40 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:49:40 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:49:40 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:49:40 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:49:40 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 1e184575e740e408ca5134ebd5a42b726f4aee38ca94f27ff280b6881c976822
2024-09-15 21:49:41 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.812228743S
2024-09-15 21:49:42 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:49:42 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: fa5ec5ab6509297e32ce72e42e5748903520cf28c04e627370baccc563f2f6a9
2024-09-15 21:49:42 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.256598105S
2024-09-15 21:49:42 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:49:42 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 9c2b8ad5be0e33203e692d5af54e6365f7be1bf64efa1c15329b0e63a4320557
2024-09-15 21:49:45 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.279438417S
2024-09-15 21:49:45 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:49:45 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 035ad619bc20b303553938d3481a1a5fd7dd5f15d420023ff0ef13a52956e32b
2024-09-15 21:49:45 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /funny_shannon: Waiting for 60 seconds for URL: http://localhost:33830/subjects (where port 33830 maps to container port 8081)
2024-09-15 21:49:51 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.44738462S
2024-09-15 21:49:51 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:49:51 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 511500 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:49:51 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:49:51 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:49:51 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:49:52 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 92 ms. Found 1 MongoDB repository interface.
2024-09-15 21:49:52 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:49:52 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:49:52 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:49:52 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 21:49:52 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6802d023, com.mongodb.Jep395RecordCodecProvider@e895e3e, com.mongodb.KotlinCodecProvider@77476fcf]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33826], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:49:52 [cluster-ClusterId{value='66e71e4048db53014fbd74a6', description='null'}-localhost:33826] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33826, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=16001277, setName='docker-rs', canonicalAddress=1e184575e740:27017, hosts=[1e184575e740:27017], passives=[], arbiters=[], primary='1e184575e740:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71e348b94ed9196b035c8, counter=6}, lastWriteDate=Sun Sep 15 21:49:42 GET 2024, lastUpdateTimeNanos=30909653023754}
2024-09-15 21:49:53 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33828]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:49:53 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:49:53 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33830]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:49:53 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33830]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:49:53 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:49:53 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:49:53 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:49:53 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422593480
2024-09-15 21:49:53 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:49:53 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.337 seconds (process running for 14.784)
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: fBp0WD1JQsCdtZmBs7gOiA
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33828 (id: 2147483646 rack: null)
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-b96911b0-0f24-4436-a7a9-2d8a671853be
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-b96911b0-0f24-4436-a7a9-2d8a671853be', protocol='range'}
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-b96911b0-0f24-4436-a7a9-2d8a671853be=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-b96911b0-0f24-4436-a7a9-2d8a671853be', protocol='range'}
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33828 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:49:53 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:49:54 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:49:54 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33828]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:49:54 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:49:54 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33830]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:49:54 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:49:54 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:49:54 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:49:54 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:49:54 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422594301
2024-09-15 21:49:54 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: fBp0WD1JQsCdtZmBs7gOiA
2024-09-15 21:49:54 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:49:54 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:49:54 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "28956edb-4ea5-4f6a-8bb6-bfbdc7ec2b27", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:49:54.261403459Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:49:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422594308, serialized key size = -1, serialized value size = 116, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "28956edb-4ea5-4f6a-8bb6-bfbdc7ec2b27", "name": "Tom", "address": "New York", "timestamp": "2024-09-15T17:49:54.261403459Z", "eventType": "UpdateCustomerNameViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:49:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:49:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Tom
2024-09-15 21:49:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Tom, address=New York]
2024-09-15 21:49:57 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:49:57 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:49:57 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:49:57 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:49:57 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "5d8a8088-68a2-4e56-a509-8fccb672ef4f", "name": "Alex", "address": "London", "timestamp": "2024-09-15T17:49:57.588970917Z", "eventType": "UpdateCustomerAddressViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:49:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 1, CreateTime = 1726422597589, serialized key size = -1, serialized value size = 118, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "5d8a8088-68a2-4e56-a509-8fccb672ef4f", "name": "Alex", "address": "London", "timestamp": "2024-09-15T17:49:57.588970917Z", "eventType": "UpdateCustomerAddressViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:49:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:49:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:49:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=London]
2024-09-15 21:50:00 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:50:00 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:50:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "929936f1-a1f6-4ba1-aa4c-10ac68c33b7a", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:50:00.607901918Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:50:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 2, CreateTime = 1726422600607, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "929936f1-a1f6-4ba1-aa4c-10ac68c33b7a", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:50:00.607901918Z", "eventType": "CreateCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:50:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 21:50:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:50:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:50:03 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:50:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:50:03 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:50:03 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:50:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 39 due to node 1 being disconnected (elapsed time since creation: 263ms, elapsed time since send: 263ms, request timeout: 30000ms)
2024-09-15 21:50:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:50:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:50:03 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33828 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:50:03 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1184862563, epoch=21) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33828) could not be established. Node may not be available.
2024-09-15 21:50:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:50:04 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33828) could not be established. Node may not be available.
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33828) could not be established. Node may not be available.
2024-09-15 21:50:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:50:04 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33828) could not be established. Node may not be available.
2024-09-15 21:50:04 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33827
2024-09-15 21:50:04 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33827]: Connection refused: localhost/127.0.0.1:33827
2024-09-15 21:50:04 [cluster-ClusterId{value='66e71e4048db53014fbd74a6', description='null'}-localhost:33826] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33826
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33828) could not be established. Node may not be available.
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:50:04 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:50:04 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:50:04 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33828) could not be established. Node may not be available.
2024-09-15 21:50:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:50:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:50:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:50:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:50:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:50:04 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:53:01 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:53:01 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:53:02 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 5118f67f3ddb4d0d24296472ef61c102a3f70f3099bc75df6dd422219889e231
2024-09-15 21:53:02 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.76004855S
2024-09-15 21:53:02 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:53:02 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:53:02 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:53:02 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:53:02 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: acd6bccb064518114e5dfa4f4956486d0c7e0a6bbbf34dd42d53ac9be4e7fc4f
2024-09-15 21:53:03 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.855586659S
2024-09-15 21:53:04 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:53:04 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: fc5fed62655c447e79bf0d4602746252b53664d84ded54d3b05108c7092c7c7d
2024-09-15 21:53:04 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.27037633S
2024-09-15 21:53:04 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:53:04 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: 817f6494834d56d19d7f9df8951d51f240a2195490f22f3a41a61092a8a7a7e3
2024-09-15 21:53:07 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.155674881S
2024-09-15 21:53:07 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:53:07 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: d6302a3b900497b3db4396936beb6dbc1703ebd01e0b42bc5f4a545b351644c7
2024-09-15 21:53:08 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /distracted_beaver: Waiting for 60 seconds for URL: http://localhost:33836/subjects (where port 33836 maps to container port 8081)
2024-09-15 21:53:13 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.416863295S
2024-09-15 21:53:13 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:53:13 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 513445 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:53:13 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:53:13 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:53:13 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:53:13 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 76 ms. Found 1 MongoDB repository interface.
2024-09-15 21:53:13 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:53:13 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:53:13 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:53:13 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 12 ms. Found 0 Redis repository interfaces.
2024-09-15 21:53:14 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@66944c7c, com.mongodb.Jep395RecordCodecProvider@14993306, com.mongodb.KotlinCodecProvider@73ae82da]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33832], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:53:14 [cluster-ClusterId{value='66e71f0a982fb47f0ef0f89e', description='null'}-localhost:33832] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33832, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19421618, setName='docker-rs', canonicalAddress=acd6bccb0645:27017, hosts=[acd6bccb0645:27017], passives=[], arbiters=[], primary='acd6bccb0645:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71eff8b8380da9036dd92, counter=6}, lastWriteDate=Sun Sep 15 21:53:04 GET 2024, lastUpdateTimeNanos=31111625865444}
2024-09-15 21:53:15 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33834]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:53:15 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:53:15 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33836]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:53:15 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33836]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:53:15 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:53:15 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:53:15 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:53:15 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422795524
2024-09-15 21:53:15 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:53:15 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.294 seconds (process running for 14.842)
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: K0ukqyLeSIW9QZTE55EUEQ
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33834 (id: 2147483646 rack: null)
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-e0377b70-1324-48ed-a263-a56a2c7c76d1
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-e0377b70-1324-48ed-a263-a56a2c7c76d1', protocol='range'}
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-e0377b70-1324-48ed-a263-a56a2c7c76d1=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-e0377b70-1324-48ed-a263-a56a2c7c76d1', protocol='range'}
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33834 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:53:15 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:53:16 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:53:16 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:53:16 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33834]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:53:16 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:53:16 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33836]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:53:16 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:53:16 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:53:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:53:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:53:16 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422796439
2024-09-15 21:53:16 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: K0ukqyLeSIW9QZTE55EUEQ
2024-09-15 21:53:16 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:53:16 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:53:16 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "863cc5a0-5374-49ad-8c3d-b3c5af9e7e26", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:53:16.407902296Z", "eventType": "DeleteCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:53:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422796446, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "863cc5a0-5374-49ad-8c3d-b3c5af9e7e26", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:53:16.407902296Z", "eventType": "DeleteCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:53:20 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:53:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was removed: id - 1
2024-09-15 21:53:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was removed from cache: id - 1
2024-09-15 21:53:23 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Deleted customer projection for: 1
2024-09-15 21:53:26 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:53:26 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 25 due to node 1 being disconnected (elapsed time since creation: 225ms, elapsed time since send: 225ms, request timeout: 30000ms)
2024-09-15 21:53:26 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:53:26 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33834 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1125427562, epoch=9) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33834) could not be established. Node may not be available.
2024-09-15 21:53:26 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:53:26 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33834) could not be established. Node may not be available.
2024-09-15 21:53:26 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:53:26 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33834) could not be established. Node may not be available.
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:53:26 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33834) could not be established. Node may not be available.
2024-09-15 21:53:26 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33833
2024-09-15 21:53:26 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33833]: Connection refused: localhost/127.0.0.1:33833
2024-09-15 21:53:26 [cluster-ClusterId{value='66e71f0a982fb47f0ef0f89e', description='null'}-localhost:33832] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33832
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:53:26 [cluster-ClusterId{value='66e71f0a982fb47f0ef0f89e', description='null'}-localhost:33832] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33832
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33834) could not be established. Node may not be available.
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:53:27 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:53:27 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:53:27 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:53:27 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:53:27 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:53:27 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 21:53:36 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 21:53:36 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 21:53:36 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.listener.KafkaConsumerIT]: KafkaConsumerIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 21:53:37 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.listener.KafkaConsumerIT
2024-09-15 21:53:37 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 21:53:37 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 21:53:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 21:53:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 21:53:37 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 21:53:37 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: d84ee051e248162de593b09f04a463be0f88977bf567f867a87657918ae3fa54
2024-09-15 21:53:38 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.750020524S
2024-09-15 21:53:38 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 21:53:38 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 21:53:38 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 21:53:38 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 21:53:38 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: 966d9e5f401df24ea249a0dec7253f5c849d78cadf4ab45d743198297eb07b66
2024-09-15 21:53:38 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.763507575S
2024-09-15 21:53:39 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 21:53:39 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 6f4b9c8717ca1b8dc36b7d9fcbfb0633ed20a35a0c320ece841573577e2ce449
2024-09-15 21:53:40 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.262377847S
2024-09-15 21:53:40 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Creating container for image: confluentinc/cp-kafka:latest
2024-09-15 21:53:40 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest is starting: cca2713ba05a2103efe5569c486f3102d85085d126b24c057581e0689b557f40
2024-09-15 21:53:43 [main] [34mINFO [0;39m tc.confluentinc/cp-kafka:latest - Container confluentinc/cp-kafka:latest started in PT3.328120569S
2024-09-15 21:53:43 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Creating container for image: confluentinc/cp-schema-registry:latest
2024-09-15 21:53:43 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest is starting: 139e43677068b290c53745afd580a442833844aceb19204002e9d0bba7445d89
2024-09-15 21:53:43 [main] [34mINFO [0;39m o.t.c.wait.strategy.HttpWaitStrategy - /inspiring_engelbart: Waiting for 60 seconds for URL: http://localhost:33842/subjects (where port 33842 maps to container port 8081)
2024-09-15 21:53:48 [main] [34mINFO [0;39m t.confluentinc/cp-schema-registry:latest - Container confluentinc/cp-schema-registry:latest started in PT5.421911149S
2024-09-15 21:53:48 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 21:53:49 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Starting KafkaConsumerIT using Java 17.0.8 with PID 515096 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 21:53:49 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 21:53:49 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:53:49 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 21:53:49 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 71 ms. Found 1 MongoDB repository interface.
2024-09-15 21:53:49 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 21:53:49 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 21:53:49 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 21:53:49 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 21:53:50 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@14ad42, com.mongodb.Jep395RecordCodecProvider@608b906d, com.mongodb.KotlinCodecProvider@173cfb01]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33838], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 21:53:50 [cluster-ClusterId{value='66e71f2da118466c83c0a915', description='null'}-localhost:33838] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33838, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=17640828, setName='docker-rs', canonicalAddress=966d9e5f401d:27017, hosts=[966d9e5f401d:27017], passives=[], arbiters=[], primary='966d9e5f401d:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e71f22635bc20701e0e01c, counter=6}, lastWriteDate=Sun Sep 15 21:53:39 GET 2024, lastUpdateTimeNanos=31147166663034}
2024-09-15 21:53:50 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:33840]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 21:53:50 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:53:50 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33842]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:53:50 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33842]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:53:51 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 21:53:51 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:53:51 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:53:51 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422831132
2024-09-15 21:53:51 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 21:53:51 [main] [34mINFO [0;39m c.z.p.integration.KafkaConsumerIT - Started KafkaConsumerIT in 2.354 seconds (process running for 14.868)
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error while fetching metadata with correlation id 2 : {CustomerViewEventTopic=LEADER_NOT_AVAILABLE}
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cluster ID: O6mM4CPsS4iSgLojHt5oeQ
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Discovered group coordinator localhost:33840 (id: 2147483646 rack: null)
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: need to re-join with the given member-id: consumer-customerViewConsumer-1-fb22851b-6fc4-4e9d-91b5-02824f6c6b2c
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] (Re-)joining group
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-fb22851b-6fc4-4e9d-91b5-02824f6c6b2c', protocol='range'}
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Finished assignment for group at generation 1: {consumer-customerViewConsumer-1-fb22851b-6fc4-4e9d-91b5-02824f6c6b2c=Assignment(partitions=[CustomerViewEventTopic-0])}
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-customerViewConsumer-1-fb22851b-6fc4-4e9d-91b5-02824f6c6b2c', protocol='range'}
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Notifying assignor about the new Assignment(partitions=[CustomerViewEventTopic-0])
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Adding newly assigned partitions: CustomerViewEventTopic-0
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Found no committed offset for partition CustomerViewEventTopic-0
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting offset for partition CustomerViewEventTopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:33840 (id: 1 rack: null)], epoch=0}}.
2024-09-15 21:53:51 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions assigned: [CustomerViewEventTopic-0]
2024-09-15 21:53:51 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was stored: id - 1, name - Alex
2024-09-15 21:53:51 [main] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Saved in cached value: CustomerProjection[id=1, name=Alex, address=New York]
2024-09-15 21:53:52 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://localhost:33840]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-service-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2024-09-15 21:53:52 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 21:53:52 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:33842]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 21:53:52 [main] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Instantiated an idempotent producer.
2024-09-15 21:53:52 [main] [34mINFO [0;39m o.a.k.c.producer.ProducerConfig - These configurations '[schema.registry.url]' were supplied but are not used yet.
2024-09-15 21:53:52 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 21:53:52 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 21:53:52 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726422832065
2024-09-15 21:53:52 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m org.apache.kafka.clients.Metadata - [Producer clientId=producer-service-producer-1] Cluster ID: O6mM4CPsS4iSgLojHt5oeQ
2024-09-15 21:53:52 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-service-producer-1] ProducerId set to 0 with epoch 0
2024-09-15 21:53:52 [main] [34mINFO [0;39m c.z.core.service.KafkaProducer - Sending kafka message on topic CustomerViewEventTopic
2024-09-15 21:53:52 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m c.z.core.service.KafkaProducer - Kafka message successfully sent on topic CustomerViewEventTopic and value {"id": "e3dd2f63-c9ac-4f85-b19c-3ad8d9852761", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:53:52.036361028Z", "eventType": "DeleteCustomerViewEvent", "aggregateId": 1, "version": 1}
2024-09-15 21:53:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.producer.listener.KafkaConsumer - Received event: ConsumerRecord(topic = CustomerViewEventTopic, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1726422832072, serialized key size = -1, serialized value size = 113, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id": "e3dd2f63-c9ac-4f85-b19c-3ad8d9852761", "name": "Alex", "address": "New York", "timestamp": "2024-09-15T17:53:52.036361028Z", "eventType": "DeleteCustomerViewEvent", "aggregateId": 1, "version": 1})
2024-09-15 21:53:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was removed: id - 1
2024-09-15 21:53:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.s.i.CustomerQueryServiceImpl - Customer projection was removed from cache: id - 1
2024-09-15 21:53:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Deleted customer projection for: 1
2024-09-15 21:53:55 [awaitility-thread] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node -1 disconnected.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Cancelled in-flight FETCH request with correlation id 20 due to node 1 being disconnected (elapsed time since creation: 277ms, elapsed time since send: 277ms, request timeout: 30000ms)
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 2147483646 disconnected.
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Cancelled in-flight METADATA request with correlation id 7 due to node 1 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)
2024-09-15 21:54:00 [kafka-coordinator-heartbeat-thread | customerViewConsumer] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Group coordinator localhost:33840 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.clients.FetchSessionHandler - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Error sending fetch request (sessionId=1779238376, epoch=5) to node 1:
org.apache.kafka.common.errors.DisconnectException: null
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33840) could not be established. Node may not be available.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33840) could not be established. Node may not be available.
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33840) could not be established. Node may not be available.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33840) could not be established. Node may not be available.
2024-09-15 21:54:00 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33839
2024-09-15 21:54:00 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33839]: Connection refused: localhost/127.0.0.1:33839
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Node 1 disconnected.
2024-09-15 21:54:00 [kafka-producer-network-thread | producer-service-producer-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-service-producer-1] Connection to node 1 (localhost/127.0.0.1:33840) could not be established. Node may not be available.
2024-09-15 21:54:00 [cluster-ClusterId{value='66e71f2da118466c83c0a915', description='null'}-localhost:33838] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33838
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 21:54:00 [cluster-ClusterId{value='66e71f2da118466c83c0a915', description='null'}-localhost:33838] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33838
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Revoke previously assigned partitions CustomerViewEventTopic-0
2024-09-15 21:54:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: partitions revoked: [CustomerViewEventTopic-0]
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node 1 disconnected.
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node 1 (localhost/127.0.0.1:33840) could not be established. Node may not be available.
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 21:54:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 21:54:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-service-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2024-09-15 21:54:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 21:54:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 21:54:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 21:54:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 21:54:01 [SpringApplicationShutdownHook] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-service-producer-1 unregistered
2024-09-15 22:09:48 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:11:02 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:11:05 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:12:09 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:12:34 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:13:05 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:13:09 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:14:26 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:17:26 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Deleted customer projection for: 1
2024-09-15 22:17:49 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Deleted customer projection for: 1
2024-09-15 22:18:11 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Deleted customer projection for: 1
2024-09-15 22:19:06 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Deleted customer projection for: 1
2024-09-15 22:19:15 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:19:16 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:19:16 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Deleted customer projection for: 1
2024-09-15 22:19:16 [main] [34mINFO [0;39m c.z.p.projector.CustomerProjector - Updated customer projection for: 1
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.repository.CustomerRepositoryIT]: CustomerRepositoryIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.repository.CustomerRepositoryIT
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:33:53 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:33:53 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:33:54 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: b6014101ec6c57b7da5c26b29e567c21bf42aacc72fae9346b5313130f3e37fd
2024-09-15 22:33:54 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.76362649S
2024-09-15 22:33:54 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:33:54 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:33:54 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:33:54 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 22:33:54 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: b2e03f0f6390a1ee6997710ca6e4810f528c1aebd7c2db64516ce0cb14341b50
2024-09-15 22:33:55 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.678427359S
2024-09-15 22:33:56 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:33:56 [main] [34mINFO [0;39m c.z.p.i.r.CustomerRepositoryIT - Starting CustomerRepositoryIT using Java 17.0.8 with PID 550750 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:33:56 [main] [34mINFO [0;39m c.z.p.i.r.CustomerRepositoryIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:33:56 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:33:56 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:33:56 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 65 ms. Found 1 MongoDB repository interface.
2024-09-15 22:33:56 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:33:56 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:33:56 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:33:56 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 22:33:57 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@32e7b78d, com.mongodb.Jep395RecordCodecProvider@4583b617, com.mongodb.KotlinCodecProvider@22a63740]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33854], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:33:57 [cluster-ClusterId{value='66e72895aaa6a0277f7a565f', description='null'}-localhost:33854] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33854, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=12375925, setName='docker-rs', canonicalAddress=b2e03f0f6390:27017, hosts=[b2e03f0f6390:27017], passives=[], arbiters=[], primary='b2e03f0f6390:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e7289215ffbf9603af116d, counter=6}, lastWriteDate=Sun Sep 15 22:33:56 GET 2024, lastUpdateTimeNanos=33554344882235}
2024-09-15 22:33:58 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:33:58 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:33:58 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:33:58 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:33:58 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:33:58 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:33:58 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:33:58 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726425238602
2024-09-15 22:33:58 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:33:58 [main] [34mINFO [0;39m c.z.p.i.r.CustomerRepositoryIT - Started CustomerRepositoryIT in 2.474 seconds (process running for 5.765)
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:33:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:33:58 [main] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 22:33:59 [cluster-ClusterId{value='66e72895aaa6a0277f7a565f', description='null'}-localhost:33854] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33854
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 22:33:59 [cluster-ClusterId{value='66e72895aaa6a0277f7a565f', description='null'}-localhost:33854] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33854
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:33:59 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 22:36:13 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.repository.CustomerRepositoryIT]: CustomerRepositoryIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:36:13 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.repository.CustomerRepositoryIT
2024-09-15 22:36:14 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:36:14 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:36:14 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:36:14 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:36:14 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:36:14 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:36:14 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:36:14 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: a93096994b7a6033e592450d74462d4a865ed5175f65195c3dda4d7980e24380
2024-09-15 22:36:15 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.768756189S
2024-09-15 22:36:15 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:36:15 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:36:15 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:36:15 [main] [34mINFO [0;39m tc.mongo:latest - Creating container for image: mongo:latest
2024-09-15 22:36:15 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest is starting: a2758e854a970268c8da2aaa6e67821f113b0bf0fc8b03a823d4fc7f9a9ba02d
2024-09-15 22:36:15 [main] [34mINFO [0;39m tc.mongo:latest - Container mongo:latest started in PT0.713654232S
2024-09-15 22:36:16 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:36:16 [main] [34mINFO [0;39m c.z.p.i.r.CustomerRepositoryIT - Starting CustomerRepositoryIT using Java 17.0.8 with PID 553002 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:36:16 [main] [34mINFO [0;39m c.z.p.i.r.CustomerRepositoryIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:36:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:36:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:36:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 69 ms. Found 1 MongoDB repository interface.
2024-09-15 22:36:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:36:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:36:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:36:17 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 22:36:17 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5cfa2ac5, com.mongodb.Jep395RecordCodecProvider@4cc89246, com.mongodb.KotlinCodecProvider@413eaf5d]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:33860], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:36:17 [cluster-ClusterId{value='66e729219ea9fa10ed2f0a91', description='null'}-localhost:33860] [34mINFO [0;39m org.mongodb.driver.cluster - Monitor thread successfully connected to server with description ServerDescription{address=localhost:33860, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=15012847, setName='docker-rs', canonicalAddress=a2758e854a97:27017, hosts=[a2758e854a97:27017], passives=[], arbiters=[], primary='a2758e854a97:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, topologyVersion=TopologyVersion{processId=66e7291f30f7745a0cf50200, counter=6}, lastWriteDate=Sun Sep 15 22:36:16 GET 2024, lastUpdateTimeNanos=33694874031640}
2024-09-15 22:36:18 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:36:18 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:36:18 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:36:18 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:36:19 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:36:19 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:36:19 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:36:19 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726425379008
2024-09-15 22:36:19 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:36:19 [main] [34mINFO [0;39m c.z.p.i.r.CustomerRepositoryIT - Started CustomerRepositoryIT in 2.417 seconds (process running for 5.718)
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:36:19 [main] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:36:19 [main] [34mINFO [0;39m o.s.d.m.core.convert.QueryMapper - Could not map 'Customer.id'; Maybe a fragment in 'Long' is considered a simple type; Mapper continues with id
2024-09-15 22:36:19 [cluster-ClusterId{value='66e729219ea9fa10ed2f0a91', description='null'}-localhost:33860] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33860
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:178)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:381)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:221)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
2024-09-15 22:36:19 [cluster-ClusterId{value='66e729219ea9fa10ed2f0a91', description='null'}-localhost:33860] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:33860
com.mongodb.MongoSocketReadException: Exception receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:583)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:428)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:354)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:92)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:48)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:130)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:203)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.SocketException: Connection reset
	at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)
	at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)
	at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)
	at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:176)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:196)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:716)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:580)
	... 10 common frames omitted
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:36:19 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.service.CacheServiceIT]: CacheServiceIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.service.CacheServiceIT
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:43:36 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:43:36 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:43:37 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: a29b442486eab7dec091a6d47b5b4405895d25945d7b579ae4ee563c526da44e
2024-09-15 22:43:37 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.757760936S
2024-09-15 22:43:37 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:43:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:43:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:43:37 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 22:43:37 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: ce3659bab34fdb873154140d888e211b2afc80fdeb8574ba3fb4cf7b8b283e92
2024-09-15 22:43:37 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.23539068S
2024-09-15 22:43:38 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:43:38 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Starting CacheServiceIT using Java 17.0.8 with PID 554542 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:43:38 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 75 ms. Found 1 MongoDB repository interface.
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 6 ms. Found 0 Redis repository interfaces.
2024-09-15 22:43:38 [main] [31mWARN [0;39m o.s.w.c.s.GenericWebApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customerCommandController' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/controller/CustomerCommandController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'customerServiceImpl' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/service/impl/CustomerServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'commandHandlerDispatcher' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/CommandHandlerDispatcher.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
2024-09-15 22:43:38 [main] [34mINFO [0;39m o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2024-09-15 22:43:38 [main] [1;31mERROR[0;39m o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customerCommandController' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/controller/CustomerCommandController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'customerServiceImpl' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/service/impl/CustomerServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'commandHandlerDispatcher' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/CommandHandlerDispatcher.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:975)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:971)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:625)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1463)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:553)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:225)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:152)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:130)
	at org.springframework.test.context.web.ServletTestExecutionListener.setUpRequestContextIfNecessary(ServletTestExecutionListener.java:191)
	at org.springframework.test.context.web.ServletTestExecutionListener.prepareTestInstance(ServletTestExecutionListener.java:130)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:260)
	at org.springframework.test.context.junit.jupiter.SpringExtension.postProcessTestInstance(SpringExtension.java:163)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$10(ClassBasedTestDescriptor.java:378)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.executeAndMaskThrowable(ClassBasedTestDescriptor.java:383)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$11(ClassBasedTestDescriptor.java:378)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestInstancePostProcessors(ClassBasedTestDescriptor.java:377)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$instantiateAndPostProcessTestInstance$6(ClassBasedTestDescriptor.java:290)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:289)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:279)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:278)
	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$prepare$0(TestMethodTestDescriptor.java:106)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:105)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:69)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:63)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customerServiceImpl' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/service/impl/CustomerServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'commandHandlerDispatcher' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/CommandHandlerDispatcher.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 96 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'commandHandlerDispatcher' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/CommandHandlerDispatcher.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 110 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1689)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1653)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeanCollection(DefaultListableBeanFactory.java:1543)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1511)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1392)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 124 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 142 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:542)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 156 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:648)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:636)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 170 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:644)
	... 184 common frames omitted
Caused by: java.lang.IllegalStateException: Mapped port can only be obtained after the container is started
	at org.testcontainers.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:512)
	at org.testcontainers.containers.ContainerState.getMappedPort(ContainerState.java:161)
	at java.base/java.util.Optional.map(Optional.java:260)
	at org.testcontainers.containers.ContainerState.getFirstMappedPort(ContainerState.java:143)
	at com.zhigalko.producer.integration.config.KafkaTestConfig.producerFactory(KafkaTestConfig.java:43)
	at com.zhigalko.producer.integration.config.KafkaTestConfig$$SpringCGLIB$$0.CGLIB$producerFactory$0(<generated>)
	at com.zhigalko.producer.integration.config.KafkaTestConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:348)
	at com.zhigalko.producer.integration.config.KafkaTestConfig$$SpringCGLIB$$0.producerFactory(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:146)
	... 185 common frames omitted
2024-09-15 22:43:38 [main] [31mWARN [0;39m o.s.test.context.TestContextManager - Caught exception while allowing TestExecutionListener [org.springframework.test.context.web.ServletTestExecutionListener] to prepare test instance [com.zhigalko.producer.integration.service.CacheServiceIT@3129299f]
java.lang.IllegalStateException: Failed to load ApplicationContext for [WebMergedContextConfiguration@17befef0 testClass = com.zhigalko.producer.integration.service.CacheServiceIT, locations = [], classes = [com.zhigalko.producer.ProducerApplication], contextInitializerClasses = [], activeProfiles = [], propertySourceDescriptors = [], propertySourceProperties = ["org.springframework.boot.test.context.SpringBootTestContextBootstrapper=true"], contextCustomizers = [[ImportsContextCustomizer@5072e638 key = [com.zhigalko.producer.integration.config.KafkaTestConfig]], org.springframework.boot.test.context.filter.ExcludeFilterContextCustomizer@616fe72b, org.springframework.boot.test.json.DuplicateJsonObjectContextCustomizerFactory$DuplicateJsonObjectContextCustomizer@293a5f75, org.springframework.boot.test.mock.mockito.MockitoContextCustomizer@0, org.springframework.boot.test.web.client.TestRestTemplateContextCustomizer@c35172e, org.springframework.boot.test.web.reactor.netty.DisableReactorResourceFactoryGlobalResourcesContextCustomizerFactory$DisableReactorResourceFactoryGlobalResourcesContextCustomizerCustomizer@35e5d0e5, org.springframework.boot.test.autoconfigure.actuate.observability.ObservabilityContextCustomizerFactory$DisableObservabilityContextCustomizer@1f, org.springframework.boot.test.autoconfigure.properties.PropertyMappingContextCustomizer@0, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverContextCustomizer@aafcffa, org.springframework.test.context.support.DynamicPropertiesContextCustomizer@92269f74, org.springframework.boot.testcontainers.service.connection.ServiceConnectionContextCustomizer@0, org.springframework.boot.test.context.SpringBootTestAnnotation@31f2b307], resourceBasePath = "src/main/webapp", contextLoader = org.springframework.boot.test.context.SpringBootContextLoader, parent = null]
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:180)
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:130)
	at org.springframework.test.context.web.ServletTestExecutionListener.setUpRequestContextIfNecessary(ServletTestExecutionListener.java:191)
	at org.springframework.test.context.web.ServletTestExecutionListener.prepareTestInstance(ServletTestExecutionListener.java:130)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:260)
	at org.springframework.test.context.junit.jupiter.SpringExtension.postProcessTestInstance(SpringExtension.java:163)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$10(ClassBasedTestDescriptor.java:378)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.executeAndMaskThrowable(ClassBasedTestDescriptor.java:383)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$11(ClassBasedTestDescriptor.java:378)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestInstancePostProcessors(ClassBasedTestDescriptor.java:377)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$instantiateAndPostProcessTestInstance$6(ClassBasedTestDescriptor.java:290)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:289)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:279)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:278)
	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$prepare$0(TestMethodTestDescriptor.java:106)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:105)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:69)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:63)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customerCommandController' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/controller/CustomerCommandController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'customerServiceImpl' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/service/impl/CustomerServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'commandHandlerDispatcher' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/CommandHandlerDispatcher.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:975)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:971)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:625)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.test.context.SpringBootContextLoader.lambda$loadContext$3(SpringBootContextLoader.java:137)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:58)
	at org.springframework.util.function.ThrowingSupplier.get(ThrowingSupplier.java:46)
	at org.springframework.boot.SpringApplication.withHook(SpringApplication.java:1463)
	at org.springframework.boot.test.context.SpringBootContextLoader$ContextLoaderHook.run(SpringBootContextLoader.java:553)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:137)
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:108)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:225)
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:152)
	... 72 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customerServiceImpl' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/service/impl/CustomerServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'commandHandlerDispatcher' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/CommandHandlerDispatcher.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 96 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'commandHandlerDispatcher' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/CommandHandlerDispatcher.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 110 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'createCustomerCommandHandler' defined in file [/home/anduser/dev/assessment/assessment-m2/producer/target/classes/com/zhigalko/producer/handler/impl/CreateCustomerCommandHandler.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1689)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1653)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeanCollection(DefaultListableBeanFactory.java:1543)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveMultipleBeans(DefaultListableBeanFactory.java:1511)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1392)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 124 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'com.zhigalko.core.service.KafkaProducer': Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 142 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'testKafkaTemplate' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Unsatisfied dependency expressed through method 'testKafkaTemplate' parameter 0: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:542)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 156 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'producerFactory' defined in class path resource [com/zhigalko/producer/integration/config/KafkaTestConfig.class]: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:648)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:636)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 170 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.kafka.core.ProducerFactory]: Factory method 'producerFactory' threw exception with message: Mapped port can only be obtained after the container is started
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:644)
	... 184 common frames omitted
Caused by: java.lang.IllegalStateException: Mapped port can only be obtained after the container is started
	at org.testcontainers.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:512)
	at org.testcontainers.containers.ContainerState.getMappedPort(ContainerState.java:161)
	at java.base/java.util.Optional.map(Optional.java:260)
	at org.testcontainers.containers.ContainerState.getFirstMappedPort(ContainerState.java:143)
	at com.zhigalko.producer.integration.config.KafkaTestConfig.producerFactory(KafkaTestConfig.java:43)
	at com.zhigalko.producer.integration.config.KafkaTestConfig$$SpringCGLIB$$0.CGLIB$producerFactory$0(<generated>)
	at com.zhigalko.producer.integration.config.KafkaTestConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:348)
	at com.zhigalko.producer.integration.config.KafkaTestConfig$$SpringCGLIB$$0.producerFactory(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:146)
	... 185 common frames omitted
2024-09-15 22:45:32 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.service.CacheServiceIT]: CacheServiceIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:45:33 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.service.CacheServiceIT
2024-09-15 22:45:33 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:45:33 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:45:33 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:45:33 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:45:33 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:45:33 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:45:33 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:45:33 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 38408b5d9bba1b3483c229c1b9df703fb16f8d270fac0a58e92ef43aadff7ace
2024-09-15 22:45:34 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.772827457S
2024-09-15 22:45:34 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:45:34 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:45:34 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:45:34 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 22:45:34 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: d6275aa2d7c31ede990b0fb8db3faf06099df7802a8cb1890c0e5ec19ce5791c
2024-09-15 22:45:34 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.244821464S
2024-09-15 22:45:34 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:45:34 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Starting CacheServiceIT using Java 17.0.8 with PID 555298 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:45:34 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 77 ms. Found 1 MongoDB repository interface.
2024-09-15 22:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:45:35 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 22:45:35 [cluster-ClusterId{value='66e72b4fa76be15b66b88eda', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 22:45:35 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@3400d6fa, com.mongodb.Jep395RecordCodecProvider@30a62a5b, com.mongodb.KotlinCodecProvider@41a8f0d8]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:45:37 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:45:37 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:45:37 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:45:37 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:45:37 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:45:37 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:45:37 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:45:37 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726425937413
2024-09-15 22:45:37 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:45:37 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Started CacheServiceIT in 2.974 seconds (process running for 5.212)
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:45:37 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:45:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:45:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:45:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:45:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:45:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:45:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:45:43 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33864
2024-09-15 22:45:43 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33864]: Connection refused: localhost/127.0.0.1:33864
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:45:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.service.CacheServiceIT]: CacheServiceIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.service.CacheServiceIT
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:46:51 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:46:51 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:46:52 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 27d0cc97d0028f84fc6881693573213ce016e148507625c709380de8c843c6e3
2024-09-15 22:46:52 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.80077418S
2024-09-15 22:46:52 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:46:52 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:46:52 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:46:52 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 22:46:52 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: fb1d9717d8c1d78264411adeb01753b13b58869109fca7fb1667638c5d1d4796
2024-09-15 22:46:52 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.245978076S
2024-09-15 22:46:52 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:46:52 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Starting CacheServiceIT using Java 17.0.8 with PID 555977 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:46:52 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:46:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:46:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:46:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 71 ms. Found 1 MongoDB repository interface.
2024-09-15 22:46:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:46:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:46:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:46:53 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 Redis repository interfaces.
2024-09-15 22:46:53 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@23708f14, com.mongodb.Jep395RecordCodecProvider@3dfc59c5, com.mongodb.KotlinCodecProvider@2cff5aa3]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:46:53 [cluster-ClusterId{value='66e72b9de0d72109f4920b75', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 22:46:54 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:46:54 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:46:54 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:46:54 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:46:55 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:46:55 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:46:55 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:46:55 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726426015103
2024-09-15 22:46:55 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:46:55 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Started CacheServiceIT in 2.347 seconds (process running for 4.446)
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:46:55 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33866
2024-09-15 22:46:55 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33866]: Connection refused: localhost/127.0.0.1:33866
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:46:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 22:49:36 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:49:36 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:49:37 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.service.CacheServiceIT]: CacheServiceIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:49:37 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.service.CacheServiceIT
2024-09-15 22:49:37 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:49:37 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:49:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:49:37 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:49:37 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:49:38 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: bd72dbc110a9308796be3178f2a032dc12f23e7943491706365fb936f5838593
2024-09-15 22:49:38 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.786894846S
2024-09-15 22:49:38 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:49:38 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:49:38 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:49:38 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 22:49:38 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 0fe8eb997a9809527f7f3bac31a1d407ca1d980ab3bc2253779cab721da1be7d
2024-09-15 22:49:38 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.230573603S
2024-09-15 22:49:38 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:49:38 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Starting CacheServiceIT using Java 17.0.8 with PID 556856 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:49:38 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:49:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:49:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:49:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 77 ms. Found 1 MongoDB repository interface.
2024-09-15 22:49:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:49:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:49:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:49:39 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 8 ms. Found 0 Redis repository interfaces.
2024-09-15 22:49:39 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@395eb363, com.mongodb.Jep395RecordCodecProvider@1e194966, com.mongodb.KotlinCodecProvider@160546b1]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:49:39 [cluster-ClusterId{value='66e72c43aadbb826aa42f174', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 22:49:40 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:49:40 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:49:40 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:49:40 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:49:41 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:49:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:49:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:49:41 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726426181060
2024-09-15 22:49:41 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:49:41 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Started CacheServiceIT in 2.478 seconds (process running for 4.508)
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:49:41 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33868
2024-09-15 22:49:41 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33868]: Connection refused: localhost/127.0.0.1:33868
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:49:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.service.CacheServiceIT]: CacheServiceIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.service.CacheServiceIT
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:51:16 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:51:16 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:51:17 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: d9e4b6c72bc4bbed19f525a07b3ea7eb53e85caea667c82cd55ca8d0a24e97a1
2024-09-15 22:51:17 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.729992699S
2024-09-15 22:51:17 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:51:17 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:51:17 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:51:17 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 22:51:17 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 096e574a57e31353e8a124ac45dc38e720ecb6ba35486eb3c5d50252a7ec998e
2024-09-15 22:51:17 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.215423367S
2024-09-15 22:51:18 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:51:18 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Starting CacheServiceIT using Java 17.0.8 with PID 557643 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:51:18 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:51:18 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:51:18 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:51:18 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 82 ms. Found 1 MongoDB repository interface.
2024-09-15 22:51:18 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:51:18 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:51:18 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:51:18 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 22:51:19 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@37364634, com.mongodb.Jep395RecordCodecProvider@3dec79f8, com.mongodb.KotlinCodecProvider@5bf7f15f]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:51:19 [cluster-ClusterId{value='66e72ca7e459e32605cd8bbc', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 22:51:20 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:51:20 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:51:20 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:51:20 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:51:20 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:51:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:51:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:51:20 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726426280453
2024-09-15 22:51:20 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:51:20 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Started CacheServiceIT in 2.535 seconds (process running for 4.577)
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:20 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:21 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33870
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:21 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33870]: Connection refused: localhost/127.0.0.1:33870
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:51:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 22:51:27 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:51:27 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:51:27 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.service.CacheServiceIT]: CacheServiceIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:51:27 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.service.CacheServiceIT
2024-09-15 22:51:27 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:51:28 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:51:28 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:51:28 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:51:28 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:51:28 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: 9979e60b6a14c8dbd621f85891403bdaef0784679df1715edfa364b7ab23b113
2024-09-15 22:51:28 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.761141906S
2024-09-15 22:51:28 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:51:28 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:51:28 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:51:28 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 22:51:29 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 111062e1ce866afb61d3c788dcafeee802d79f7a60084986485b19607f46ba72
2024-09-15 22:51:29 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.256194179S
2024-09-15 22:51:29 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:51:29 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Starting CacheServiceIT using Java 17.0.8 with PID 558205 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:51:29 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:51:30 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:51:30 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:51:30 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 80 ms. Found 1 MongoDB repository interface.
2024-09-15 22:51:30 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:51:30 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:51:30 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:51:30 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 22:51:30 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@54a6ef6, com.mongodb.Jep395RecordCodecProvider@5c740c5a, com.mongodb.KotlinCodecProvider@7fd2a67a]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:51:30 [cluster-ClusterId{value='66e72cb2b53e61280b1dc665', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 22:51:31 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:51:31 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:51:31 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:51:31 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:51:32 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:51:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:51:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:51:32 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726426292000
2024-09-15 22:51:32 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:51:32 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Started CacheServiceIT in 2.787 seconds (process running for 5.02)
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:32 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:51:49 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33872
2024-09-15 22:51:49 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33872]: Connection refused: localhost/127.0.0.1:33872
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:51:49 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
2024-09-15 22:51:56 [main] [34mINFO [0;39m o.s.t.c.s.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [com.zhigalko.producer.integration.service.CacheServiceIT]: CacheServiceIT does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2024-09-15 22:51:56 [main] [34mINFO [0;39m o.s.b.t.c.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration com.zhigalko.producer.ProducerApplication for test class com.zhigalko.producer.integration.service.CacheServiceIT
2024-09-15 22:51:56 [main] [34mINFO [0;39m o.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2024-09-15 22:51:56 [main] [34mINFO [0;39m o.t.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2024-09-15 22:51:57 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2024-09-15 22:51:57 [main] [34mINFO [0;39m o.t.d.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2024-09-15 22:51:57 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Docker host IP address is localhost
2024-09-15 22:51:57 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 26.1.0
  API Version: 1.45
  Operating System: Ubuntu 22.04.4 LTS
  Total Memory: 15676 MB
2024-09-15 22:51:57 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Creating container for image: testcontainers/ryuk:0.7.0
2024-09-15 22:51:57 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 is starting: f23af44d48997af2967bbc78963342613cac9d75b5da33debdd252e2a446eef2
2024-09-15 22:51:57 [main] [34mINFO [0;39m tc.testcontainers/ryuk:0.7.0 - Container testcontainers/ryuk:0.7.0 started in PT0.747179206S
2024-09-15 22:51:57 [main] [34mINFO [0;39m o.t.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2024-09-15 22:51:57 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - Checking the system...
2024-09-15 22:51:57 [main] [34mINFO [0;39m o.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2024-09-15 22:51:57 [main] [34mINFO [0;39m tc.redis:latest - Creating container for image: redis:latest
2024-09-15 22:51:58 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest is starting: 4e9a0727dba1d7dfd3c3bf91542f8ef6a7b0db9ea88e7d2e214edfec78cbcd05
2024-09-15 22:51:58 [main] [34mINFO [0;39m tc.redis:latest - Container redis:latest started in PT0.246350674S
2024-09-15 22:51:58 [background-preinit] [34mINFO [0;39m o.h.validator.internal.util.Version - HV000001: Hibernate Validator 8.0.1.Final
2024-09-15 22:51:58 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Starting CacheServiceIT using Java 17.0.8 with PID 558796 (started by anduser in /home/anduser/dev/assessment/assessment-m2/producer)
2024-09-15 22:51:58 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - No active profile set, falling back to 1 default profile: "default"
2024-09-15 22:51:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:51:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2024-09-15 22:51:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 70 ms. Found 1 MongoDB repository interface.
2024-09-15 22:51:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2024-09-15 22:51:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2024-09-15 22:51:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.zhigalko.producer.repository.CustomerRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2024-09-15 22:51:58 [main] [34mINFO [0;39m o.s.d.r.c.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 5 ms. Found 0 Redis repository interfaces.
2024-09-15 22:51:59 [main] [34mINFO [0;39m org.mongodb.driver.client - MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "5.0.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "6.8.0-40-generic"}, "platform": "Java/Amazon.com Inc./17.0.8+7-LTS"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='admin', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@270b8c2a, com.mongodb.Jep395RecordCodecProvider@7b9c2387, com.mongodb.KotlinCodecProvider@12e2f5ab]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null}
2024-09-15 22:51:59 [cluster-ClusterId{value='66e72ccfc47c065b9d96fd2b', description='null'}-localhost:27017] [34mINFO [0;39m org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket
	at com.mongodb.internal.connection.SocketStream.lambda$open$0(SocketStream.java:86)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:86)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:201)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:193)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:153)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.Net.pollConnect(Native Method)
	at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
	at java.base/java.net.Socket.connect(Socket.java:633)
	at com.mongodb.internal.connection.SocketStreamHelper.initialize(SocketStreamHelper.java:76)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:105)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:80)
	... 4 common frames omitted
2024-09-15 22:52:00 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [http://localhost:19092, http://localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-customerViewConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = customerViewConsumer
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

2024-09-15 22:52:00 [main] [34mINFO [0;39m o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2024-09-15 22:52:00 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:52:00 [main] [34mINFO [0;39m i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.key.type = null
	specific.avro.reader = true
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2024-09-15 22:52:00 [main] [34mINFO [0;39m o.a.k.c.consumer.ConsumerConfig - These configurations '[schema.registry.url, specific.avro.reader]' were supplied but are not used yet.
2024-09-15 22:52:00 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2024-09-15 22:52:00 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2024-09-15 22:52:00 [main] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1726426320488
2024-09-15 22:52:00 [main] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Subscribed to topic(s): CustomerViewEventTopic
2024-09-15 22:52:00 [main] [34mINFO [0;39m c.z.p.i.service.CacheServiceIT - Started CacheServiceIT in 2.262 seconds (process running for 4.343)
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:52:00 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -2 disconnected.
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -2 (localhost/127.0.0.1:29092) could not be established. Node may not be available.
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:29092 (id: -2 rack: null) disconnected
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Node -1 disconnected.
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Connection to node -1 (localhost/127.0.0.1:19092) could not be established. Node may not be available.
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [31mWARN [0;39m o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Bootstrap broker localhost:19092 (id: -1 rack: null) disconnected
2024-09-15 22:52:01 [lettuce-eventExecutorLoop-1-1] [34mINFO [0;39m i.l.core.protocol.ConnectionWatchdog - Reconnecting, last destination was localhost/127.0.0.1:33874
2024-09-15 22:52:01 [lettuce-nioEventLoop-6-2] [31mWARN [0;39m i.l.core.protocol.ConnectionWatchdog - Cannot reconnect to [localhost/<unresolved>:33874]: Connection refused: localhost/127.0.0.1:33874
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Unsubscribed all topics or patterns and assigned partitions
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-customerViewConsumer-1, groupId=customerViewConsumer] Request joining group due to: consumer pro-actively leaving the group
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-customerViewConsumer-1 unregistered
2024-09-15 22:52:01 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [34mINFO [0;39m o.s.k.l.KafkaMessageListenerContainer - customerViewConsumer: Consumer stopped
